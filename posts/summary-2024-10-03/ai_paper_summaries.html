<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.56">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Shwetank Kumar">
<meta name="dcterms.date" content="2024-09-30">

<title>üåô AI Afterhours: Top AI Papers for Sep 30 - Oct 02, 2024 ‚Äì Shwetank Kumar</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-WYRPY1S3G7"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-WYRPY1S3G7', { 'anonymize_ip': true});
</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Shwetank Kumar</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../blog.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../favorite-eggheads.html"> 
<span class="menu-text">Eggheadery</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/shwetank-kumar"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/shwetankumar"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/shwetankkumar"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../index.xml"> <i class="bi bi-rss" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-page">
      <h1 class="title">üåô AI Afterhours: Top AI Papers for Sep 30 - Oct 02, 2024</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">Multimodal Learning</div>
                <div class="quarto-category">Imitation Learning</div>
                <div class="quarto-category">MIMO</div>
                <div class="quarto-category">Natural Language Processing</div>
                <div class="quarto-category">Multimodal Generation</div>
                <div class="quarto-category">Knowledge Distillation</div>
                <div class="quarto-category">3D Computer Vision</div>
                <div class="quarto-category">Large Language Models</div>
                <div class="quarto-category">Multimodal Models</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta column-page">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Shwetank Kumar </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">September 30, 2024</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block column-page" id="quarto-document-content">





<p>Welcome to this week‚Äôs AI Afterhours! Your weekly digest of most upvoted papers in AI. Below is gist of the results, how they got them, and why you should care. With that, let‚Äôs dive into the most exciting AI research from September 30 to October 02, 2024.</p>
<p><strong>‚ÄúEmu3‚Äù demonstrates that next-token prediction is all you need</strong> for achieving state-of-the-art performance in multimodal generation and perception tasks. By tokenizing images, text, and videos into a discrete space and training a single transformer from scratch, Emu3 outperforms established task-specific models. It achieves a zero-shot FID score of 0.43 and a CLIP-T score of 0.67 on various benchmarks. In video generation, it produces high-fidelity results with a PSNR of 22.69 and SSIM of 0.690. This approach suggests that next-token prediction could be a powerful paradigm for developing versatile multimodal models, potentially revolutionizing multimedia AI applications.</p>
<p><a href="https://arxiv.org/pdf/2409.18869v1">arXiv:2409.18869v1</a> üëç47</p>
<p><strong>‚ÄúMolmo and PixMo‚Äù introduce open weights and open data for state-of-the-art multimodal models</strong>, addressing the challenge of building advanced vision-language models (VLMs) without relying on proprietary systems. The Molmo family outperforms other open-weight models on academic benchmarks, with the 72B model ranking second in human preference evaluations. Notably, it achieves 88.7% low-level accuracy and 69.0% high-level accuracy on the AndroidControl benchmark. This work provides a foundation for developing more transparent and open AI systems, potentially democratizing access to powerful multimodal models.</p>
<p><a href="https://arxiv.org/pdf/2409.17146v1">arXiv:2409.17146v1</a> üëç41</p>
<p><strong>‚ÄúProgramming Every Example‚Äù (PROX) lifts pre-training data quality like experts at scale</strong>, improving large language model (LLM) performance while reducing computing power. The PROX framework uses language models to refine pre-training data, resulting in over 2% improvement on various downstream benchmarks. Models trained on PROX-curated tokens yield significant improvements with 20√ó fewer tokens. The additional computational overhead is equivalent to training an extra 12B tokens on TLM-S and 5B tokens on TLM-M. This approach could be a game-changer for LLM development, making training more efficient and potentially reducing the environmental impact of AI research.</p>
<p><a href="https://arxiv.org/pdf/2409.17115v1">arXiv:2409.17115v1</a> üëç36</p>
<p><strong>The ‚ÄúLaw of the Weakest Link‚Äù study reveals crucial insights into cross capabilities of Large Language Models</strong> (LLMs). Using the CrossEval benchmark, comprising 1,400 prompts and 8,400 human ratings, the research shows that LLMs‚Äô cross-capability performance is significantly constrained by their weakest component. Out of 58 cross-capability scores from 17 models, 38 are lower than all individual capabilities. This finding has profound implications for LLM development, suggesting that focusing on improving weaker capabilities could lead to significant gains in cross-capability tasks, potentially revolutionizing how we approach AI model enhancement.</p>
<p><a href="https://arxiv.org/pdf/2409.19951v2">arXiv:2409.19951v2</a> üëç34</p>
<p><strong>‚ÄúMIO‚Äù introduces a foundation model on multimodal tokens</strong>, integrating text, image, video, and speech modalities. Trained using a four-stage process, MIO demonstrates competitive performance compared to previous dual-modal and modality-specific baselines. It shows a 10% improvement in image captioning and 15% in visual question answering. MIO also exhibits advanced capabilities like interleaved video-text generation and chain-of-visual-thought reasoning. While it has some limitations with OCR-related images and video generation, MIO represents a significant step towards more versatile and capable AI systems that can seamlessly work across multiple modalities.</p>
<p><a href="https://arxiv.org/pdf/2409.17692v1">arXiv:2409.17692v1</a> üëç32</p>
<p><strong>‚ÄúHelloBench‚Äù provides a comprehensive evaluation of long text generation capabilities in Large Language Models</strong> (LLMs). The study reveals that current LLMs struggle with generating text longer than 4000 words, often producing degraded quality for longer outputs. The proposed HelloEval method achieves a correlation of around 30 with human evaluation, highlighting the limitations of LLM-as-a-Judge approaches. The research identifies several error modes in long text generation, including repetition and meaningless content. These findings underscore the need for improved long-form text generation in LLMs, which could significantly impact applications like content creation and summarization.</p>
<p><a href="https://arxiv.org/pdf/2409.16191v1">arXiv:2409.16191v1</a> üëç28</p>
<p><strong>‚ÄúMaskLLM‚Äù introduces learnable semi-structured sparsity for Large Language Models</strong>, addressing the challenge of reducing computational overhead while maintaining performance. Using Gumbel Softmax sampling, MaskLLM achieves superior results compared to state-of-the-art methods, with a Wikitext perplexity of 6.72 on LLaMA-2 7B, outperforming SparseGPT‚Äôs 10.42. The method effectively scales to large datasets and accelerates training through transfer learning with pre-computed masks. This approach could significantly enhance the efficiency of LLMs, making them more practical for real-world AI applications where computational resources are a constraint.</p>
<p><a href="https://arxiv.org/pdf/2409.17481v1">arXiv:2409.17481v1</a> üëç25</p>
<p><strong>‚ÄúRATIONALYST‚Äù pre-trains process-supervision for improving reasoning</strong> in Large Language Models (LLMs). By training on implicit rationales extracted from unlabeled data, RATIONALYST improves reasoning accuracy by an average of 3.9% across seven representative benchmarks. It outperforms other verifiers, including GPT-4, and demonstrates that implicit supervision surpasses explicit supervision, with a 2.6% improvement on ECQA and 4.0% on GSM8K. This approach could significantly enhance LLMs‚Äô reasoning capabilities, potentially leading to more reliable AI systems for complex decision-making tasks.</p>
<p><a href="https://arxiv.org/pdf/2410.01044v1">arXiv:2410.01044v1</a> üëç23</p>
<p><strong>‚ÄúRACER‚Äù introduces rich language-guided failure recovery policies for imitation learning</strong> in robotic manipulation. By using a vision-language model as a supervisor and a language-conditioned visuomotor policy as an actor, RACER achieves an average success rate of 70.2% on 18 RLbench tasks, outperforming the state-of-the-art RVT by 7.3%. The use of rich instructions improves performance by 2% compared to simple instructions. This approach could significantly enhance the robustness and adaptability of robotic systems, particularly in complex and dynamic environments where failure recovery is crucial.</p>
<p><a href="https://arxiv.org/pdf/2409.14674v1">arXiv:2409.14674v1</a> üëç22</p>
<p><strong>‚ÄúPHI-S‚Äù introduces distribution balancing for label-free multi-teacher distillation</strong>, addressing the challenge of balancing teacher distributions in knowledge distillation. Using a Hadamard matrix to standardize teacher outputs, PHI-S outperforms other normalization methods, achieving mean squared errors of 4.7200, 4.9010, 0.8865, and 8.3330 for DFN CLIP, SigLIP, DINOv2, and SAM, respectively. The PHI-S-RADIO-B and PHI-S-RADIO-L models reach classification accuracies of 73.16 and 80.45 on ImageNet-1K. This method could significantly improve multi-teacher knowledge distillation, benefiting various applications in computer vision and beyond.</p>
<p><a href="https://arxiv.org/pdf/2410.01680v1">arXiv:2410.01680v1</a> üëç21</p>
<p><strong>‚ÄúLLaVA-3D‚Äù offers a simple yet effective pathway to empowering Large Multimodal Models (LMMs) with 3D-awareness</strong>. By introducing the 3D Patch representation, which augments 2D patch-wise features with 3D positional embeddings, LLaVA-3D achieves state-of-the-art performance on various 3D tasks. It reaches 91.7% accuracy on ScanQA, 79.21% CIDEr on Scan2Cap, and 54.1% accuracy on ScanRefer. Notably, it converges 3.5√ó faster than existing 3D LMMs. This approach could significantly advance the development of generalist models capable of handling both 2D and 3D tasks, potentially revolutionizing fields like robotics and augmented reality.</p>
<p><a href="https://arxiv.org/pdf/2409.18125v1">arXiv:2409.18125v1</a> üëç21</p>
<p><strong>‚ÄúMIMO‚Äù enables controllable character video synthesis with spatial decomposed modeling</strong>, addressing the challenge of generating realistic videos with controllable attributes. Using a spatial decomposed diffusion model, MIMO achieves 95% accuracy in reconstructing input videos, with 90% of generated characters showing high similarity to input images. It can generate videos with novel 3D motions at 85% accuracy and interactive scenes at 90% accuracy. This technology could have far-reaching implications for entertainment, education, and advertising, offering new ways to create personalized and interactive video content.</p>
<p><a href="https://arxiv.org/pdf/2409.16160v1">arXiv:2409.16160v1</a> üëç21</p>
<p>And that‚Äôs a wrap! See you next week!</p>



</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "Óßã";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/shwetank-kumar\.github\.io\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://utteranc.es/client.js" repo="shwetank-kumar/blog-comments" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->




</body></html>