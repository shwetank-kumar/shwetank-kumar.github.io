<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Shwetank Kumar">
<meta name="dcterms.date" content="2024-09-06">

<title>The Great Data Famine: Why the AI that Ate the Web Is Still Starving – Shwetank Kumar</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-WYRPY1S3G7"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-WYRPY1S3G7', { 'anonymize_ip': true});
</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Shwetank Kumar</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../blog.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../favorite-eggheads.html"> 
<span class="menu-text">Eggheadery</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/shwetank-kumar"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/shwetankumar"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/shwetankkumar"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../index.xml"> <i class="bi bi-rss" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default">
  <div class="quarto-title-banner">
    <div class="quarto-title column-body">
      <h1 class="title">The Great Data Famine: Why the AI that Ate the Web Is Still Starving</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">genai</div>
                <div class="quarto-category">strategy</div>
                <div class="quarto-category">investment</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Shwetank Kumar </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">September 6, 2024</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-layout-custom page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
<!-- main -->





<p>If you’re a software engineer or data scientist who’s been losing sleep over the Twitter-fueled hysteria that “English is the new coding language,” I’ve got news for you: put down the panic button and step away from the job boards. I am here to tell you that despite what the Twitterverse might have you believe, AI isn’t about to steal your job or create Skynet. And if you’re an exec or investor who thinks otherwise, I’ve got a Nigerian prince who’d love to chat.</p>
<p>After a weekend of wrestling with the latest and supposedly greatest codegen tools, I’ve come to a startling conclusion: our AI overlords aren’t quite ready to steal your job or usher in the age of Skynet. In fact, they’re struggling with tasks that would make a junior dev roll their eyes.</p>
<p>Let me paint you a picture: There I was, surrounded by caffeine and a stack of AI whitepapers, trying to coax these silicon savants into solving some basic coding problems. It became painfully clear that to tackle even the most fundamental challenges, we’re in desperate need of a quantum leap in LLM complexity.</p>
<p>What we really need is for these models to channel their inner project manager – breaking down problems into bite-sized chunks, crafting plans, and methodically conquering each incremental hurdle. But instead of this sophisticated problem-solving, our current crop of AI tools often resemble a caffeinated squirrel trying to solve a Rubik’s cube – lots of frantic activity, not a lot of progress.</p>
<section id="the-transformer-plateau-when-bigger-isnt-always-better" class="level2">
<h2 data-anchor-id="the-transformer-plateau-when-bigger-isnt-always-better">The Transformer Plateau: When Bigger Isn’t Always Better</h2>
<p>Remember when we thought the solution to every AI challenge was simply to make it bigger? It was like Silicon Valley’s version of “supersize me,” but instead of fries, we were supersizing parameters and datasets. Well, folks, that all-you-can-eat buffet of data and compute has reached its limit.</p>
<p>Since the “Attention is all you need” paper dropped and gave us the Transformer architecture, we’ve been playing a game of “who can build the biggest model?” It’s been like a digital arms race, with each new model flexing more parameters. And for a while, it worked! We fed these hungry, hungry hippos more data, cranked up the parameter count, and watched the benchmarks climb.</p>
<p>But now, we’ve hit a wall. A big, data-shaped wall that’s putting a damper on our “bigger is better” party. We’re now facing a trifecta of challenges that even the most optimistic VC can’t hand-wave away:</p>
<ol type="1">
<li><p><strong>Data Scarcity:</strong> We’re scraping the bottom of the digital barrel, and it’s not pretty. Turns out, the internet isn’t infinite after all.</p></li>
<li><p><strong>Power Consumption:</strong> Our models are energy gluttons that could outshine Las Vegas. At this rate, we’ll need a small nuclear reactor for each training run.</p></li>
<li><p><strong>System Complexity:</strong> We’re playing high-stakes Jenga with GPUs, hoping that one wobbly card doesn’t bring down the entire expensive house of silicon (aka cluster).</p></li>
</ol>
<p>Unless we see some major breakthroughs in model architecture or how these Large Language Models (LLMs) learn, we’re stuck. Future AI products will be reheating the same capabilities we have today, just with fancier marketing. It’s like we’ve trained our AI to be a really eloquent parrot, but now we need it to write a novel.</p>
<p>Lets explore why our current AI models are perpetually data-starved, uncover the fundamental limitations of our learning approaches (backprop, SGD …), and maybe even find a path forward that doesn’t involve sacrificing our firstborns to the GPU gods. Welcome to the frontlines of the AI data crisis, in the land of diminishing returns – where the future is bright, but the training datasets are running on empty.</p>
</section>
<section id="beyond-chinchilla-llama-3.1-and-the-quest-for-more-data" class="level2">
<h2 data-anchor-id="beyond-chinchilla-llama-3.1-and-the-quest-for-more-data">Beyond Chinchilla: Llama 3.1 and the Quest for More Data</h2>
<p>Imagine an AI as a toddler with an endless appetite for knowledge. Now, picture that toddler devouring the entire internet and still asking for seconds. That’s our current predicament with Large Language Models (LLMs). These digital gluttons are slow learners with an insatiable hunger for data, and folks, we’re running out of internet to feed them. Who would’ve thought “we’ve run out of internet” would be a legitimate concern in 2024?</p>
<section id="exhibit-a-metas-llama-3.1---the-data-devourer" class="level3">
<h3 data-anchor-id="exhibit-a-metas-llama-3.1---the-data-devourer">Exhibit A: Meta’s Llama 3.1 - The Data Devourer</h3>
<p>Let’s dive into a real-world example that’ll make your head spin: Meta’s latest 8B parameter Llama 3.1 model. This digital beast was fed a whopping 15 trillion tokens during training. For those keeping score at home, that’s essentially the entire publicly available internet. The Llama paper [1] claims it’s all public data, but they’re keeping the exact dataset under wraps. The closest we’ve got to peeking behind the curtain is the “Fine Web Dataset” [2] on Hugging Face, tipping the scales at a hefty 44TB.</p>
</section>
<section id="breaking-the-rules-when-more-is-more" class="level3">
<h3 data-anchor-id="breaking-the-rules-when-more-is-more">Breaking the Rules: When More is… More?</h3>
<p>Now, if you’ve been following the AI literature like it’s the new Netflix, you might be thinking, “Wait a minute, isn’t that overkill?” And you’d be right - sort of. The Chinchilla paper [3], our previous guidebook for “compute optimal” training, would suggest that an 8B parameter model only needs about 160B tokens. Llama 3.1 ate roughly 100 times that amount!</p>
<p>But here’s the kicker: it worked. Meta’s decision to massively overindulge their model led to continued performance improvements. This reveals two mind-bending facts:</p>
<ol type="1">
<li>Many existing models are actually undernourished by comparison.</li>
<li>High-quality data is the new oil in the AI gold rush.</li>
</ol>
<p>Even after this data feast, Llama 3.1 hadn’t reached what we’d classically call convergence. It was still improving, like a bottomless pit of potential [4]. This, combined with Microsoft’s Tiny Stories paper [5], is forcing us to rethink the data requirements for training these models.</p>
</section>
<section id="the-bigger-they-are-the-hungrier-they-get" class="level3">
<h3 data-anchor-id="the-bigger-they-are-the-hungrier-they-get">The Bigger They Are, The Hungrier They Get</h3>
<p>And now consider the data requirements for 405B parameter version of Llama 3.1. It should ideally be trained on proportionally more data - we’re talking “several internets” worth. But guess what? It was trained on the same 15T tokens as its smaller sibling. If that dataset was barely enough for the 8B model, it’s like trying to feed a blue whale with a goldfish bowl for the 405B version.</p>
</section>
<section id="a-silver-lining-in-the-ai-cloud" class="level3">
<h3 data-anchor-id="a-silver-lining-in-the-ai-cloud">A Silver Lining in the AI Cloud</h3>
<p>Before you start stockpiling hard drives and building your own internet, there’s a glimmer of hope. For enterprises sitting on a goldmine of non-public data (that you’re legally allowed to use, of course), you’re in luck. This is your chance to fine-tune these models for specialized tasks and potentially push their performance beyond what’s publicly possible. And for the efficiency enthusiasts out there, there’s still plenty of room to explore knowledge distillation - teaching smaller models to mimic their bigger, data-guzzling cousins.</p>
</section>
<section id="the-tldr-version" class="level3">
<h3 data-anchor-id="the-tldr-version">The TL;DR Version</h3>
<ol type="1">
<li>Llama 3.1’s training reveals that our “well-trained” models might actually be underfed data-wise.</li>
<li>We’ve essentially run out of high-quality public data to train these ever-growing models.</li>
<li>The next frontier? Leveraging private data to push these models even further.</li>
</ol>
<p>Welcome to the era of data scarcity in AI - where the models are hungry, the internet is finite, and every byte counts!</p>
</section>
</section>
<section id="synthetic-data-ais-infinite-mirror-of-confusion" class="level2">
<h2 data-anchor-id="synthetic-data-ais-infinite-mirror-of-confusion">Synthetic Data: AI’s Infinite Mirror of Confusion</h2>
<p>When faced with data scarcity, researchers and engineers came up with a brilliant idea: Why not leverage AI models to generate their own training data? This concept, far from being a desperate measure, is actually a legitimate and innovative approach to addressing the data hunger of large language models which offers many advantages like unparalleled scalability, solution for privacy preservation in training data, and customization for specialized tasks. However, it also comes with its own set of challenges.</p>
<section id="model-collapse-when-ai-goes-on-a-bland-diet" class="level3">
<h3 data-anchor-id="model-collapse-when-ai-goes-on-a-bland-diet">Model Collapse: When AI Goes on a Bland Diet</h3>
<p>This self-cannibalization of data leads to what’s ominously known as ‘model collapse’. Don’t let the fancy term scare you - it’s simply what happens when your AI goes on a diet of nothing but its own increasingly bland word salad.</p>
<p>Here’s how it works: the model (or its bigger cousin) generates tokens based on probability distributions so it favors tokens closer to the mean (the “average” outputs) providing fewer examples of tokens out in the wings of the distribution. After a few cycles of generating and training on synthetic data, you lose all the diverse content from the original dataset. Result? Your models over generations lose the brilliance and versatility that would come from diversity and start generating singular, monochromatic data which doesn’t capture the real world anymore.</p>
<p>Its like making a photocopy of a photocopy - each generation gets blurrier and weirder until you end up with something that looks like it came from a glitchy parallel universe where creativity went to die.</p>
</section>
<section id="bias-on-steroids" class="level3">
<h3 data-anchor-id="bias-on-steroids">Bias on Steroids</h3>
<p>The opposite side of the same coin is that any small biases in the initial dataset get amplified out of propostion. So now the slight bias in your dataset is dialed to 11 on a scale of 10 and it thinks the entire world population consists of cat-loving, pizza-eating coders who never see the sun. Diversity? That’s just a myth, like inbox zero or bug-free code.</p>
</section>
<section id="quality-control-nightmare" class="level3">
<h3 data-anchor-id="quality-control-nightmare">Quality Control Nightmare</h3>
<p>Validating synthetic data is like fact-checking a politician’s promises - a Sisyphusean task (i.e.&nbsp;can’t be done, easily anyway). It’s a guessing game where the grand prize is “maybe your AI won’t embarrass itself in public.” And good luck keeping it current - by the time you’ve generated your synthetic data, the real world has moved on, leaving your AI stuck in last season’s trends like a digital fashion faux pas.</p>
</section>
<section id="cybersecurity-swiss-cheese" class="level3">
<h3 data-anchor-id="cybersecurity-swiss-cheese">Cybersecurity Swiss Cheese</h3>
<p>Just when you thought it couldn’t get worse, enter the hackers. Synthetic data is like a new chew toy for cybercriminals. They’re gleefully exploring all the ways they can mess with your data generation process, turning your AI into their personal puppet show.</p>
</section>
<section id="silver-linings-for-synthetic-data" class="level3">
<h3 data-anchor-id="silver-linings-for-synthetic-data">Silver Linings for Synthetic Data</h3>
<p>But wait! Don’t despair just yet. One person’s data dilemma is another’s research opportunity. Here are some tantalizing questions for the brave AI researchers of tomorrow:</p>
<ul>
<li>Can we develop smarter sampling strategies to generate synthetic data from the neglected “wings” of the probability distribution?</li>
<li>What’s the perfect cocktail of real and synthetic data? Is there a golden ratio, or does it depend on the task?</li>
<li>How can we build Fort Knox-level security around our synthetic data generation process?</li>
</ul>
</section>
<section id="the-tldr-version-1" class="level3">
<h3 data-anchor-id="the-tldr-version-1">The TL;DR Version</h3>
<ol type="1">
<li>Synthetic data is a promising solution to data scarcity, offering scalability, privacy, and customization.</li>
<li>However, it comes with significant challenges: model collapse, bias amplification, quality control issues, and security risks.</li>
<li>The future of synthetic data lies in developing better generation strategies, finding optimal real-synthetic data ratios, and creating robust security frameworks.</li>
</ol>
<p>Until we crack these problems, synthetic data is the AI world’s equivalent of combating climate change by painting everything green. It looks fantastic in PowerPoint presentations, but step outside, and you’ll find a world of plastic trees where your AI thinks photosynthesis is just the latest Instagram filter.</p>
</section>
</section>
<section id="deep-networks-are-slow-learners" class="level2">
<h2 data-anchor-id="deep-networks-are-slow-learners">Deep Networks are Slow Learners</h2>
<p>The underlying problem that is a root cause of all our AI woes is that neural networks and other architectures using a combination of back propagation and stochastic gradient descent (let’s call these SGD &amp; Progeny Pvt. Ltd.&nbsp;to include other algorithms like Adam, Ada etc.) are slow learners - absorbing knowledge at the speed of continental drift. They’re the Pangaea of machine learning, slowly but inexorably shuffling bits of information around until, eons later, you might just have a functional model. Let’s look at how these work.</p>
<section id="the-sgd-conundrum-navigating-ikea-blindfolded" class="level3">
<h3 data-anchor-id="the-sgd-conundrum-navigating-ikea-blindfolded">The SGD Conundrum: Navigating IKEA Blindfolded</h3>
<p>Stochastic Gradient Descent (SGD) and its variants are the cornerstone of modern machine learning optimization techniques. However, their effectiveness is limited by their inherent randomness. It’s basically like trying to find your way out of an IKEA blindfolded by randomly stumbling around, making small adjustments to your trajectory based on very limited local information. You might eventually find the exit, but you’ll bump into a lot of BILLY bookcases along the way.</p>
</section>
<section id="escape-from-local-minima-the-comfortable-rut" class="level3">
<h3 data-anchor-id="escape-from-local-minima-the-comfortable-rut">Escape from Local Minima: The Comfortable Rut</h3>
<p>Neural networks often get trapped in suboptimal solutions, or “local minima,” during training. To overcome this, we need to expose our models to diverse, high-quality data repeatedly. However, the scale at which this needs to happen is staggering - often requiring millions of iterations. Of course, there are heuristics that can help you along in the process, but coming up with the right set of parameters for training and escaping each minima requires a lot of experimentation with hyperparameters.</p>
</section>
<section id="the-hyperparameter-labyrinth-cracking-the-infinite-safe" class="level3">
<h3 data-anchor-id="the-hyperparameter-labyrinth-cracking-the-infinite-safe">The Hyperparameter Labyrinth: Cracking the Infinite Safe</h3>
<p>Hyperparameter tuning in machine learning is akin to trying to crack a safe with an infinite number of dials. Each parameter - learning rate, batch size, network architecture, etc. - can dramatically affect model performance, yet their interactions are often unpredictable and non-linear. It’s not uncommon for researchers to spend more time tuning hyperparameters than actually training models. This process is often more art than science, relying heavily on intuition, experience, and, frankly, a fair bit of luck.</p>
<p>Automated hyperparameter optimization techniques exist, and while helpful, often require significant computational resources and can still miss optimal configurations due to the vast search space. Moreover, hyperparameters that work well for one dataset or task might fail spectacularly on another, making it challenging to develop generalizable best practices.</p>
<p>The complexity of hyperparameter tuning also raises questions about the robustness and interpretability of our models. If slight tweaks to these parameters can lead to drastically different results, how can we trust the stability and reliability of our AI systems?</p>
</section>
<section id="the-curse-of-memorization-ais-the-office-obsession" class="level3">
<h3 data-anchor-id="the-curse-of-memorization-ais-the-office-obsession">The Curse of Memorization: AI’s “The Office” Obsession</h3>
<p>These models are turning into the Rain Man of useless information. Great if you need to count toothpicks, not so great for actual intelligence. As these models grow larger, they’re not getting smarter – they’re just getting better at regurgitating what they’ve seen before. It’s like that friend who can recite every line from “The Office” but can’t hold a conversation about anything else.</p>
</section>
<section id="the-silver-lining-future-directions" class="level3">
<h3 data-anchor-id="the-silver-lining-future-directions">The Silver Lining: Future Directions</h3>
<p>Data scarcity, which is a direct result of these fundamental limitations, is pushing us towards an inflection point. We can’t just keep making models bigger and feeding them more data. We need to fundamentally rethink how these models learn and generalize. It’s like we’ve been trying to build a skyscraper by just piling up more and more bricks. Now we need to stop and think about architecture, efficiency, and maybe invest in an elevator or two.</p>
<p>Some promising directions include:</p>
<ul>
<li>Algorithms that rely on second-order moments, though they require more memory to store gradients.</li>
<li>Combining these techniques with simplifying assumptions about the nature of the matrix [7], allowing us to store sparser versions.</li>
<li>Coupling these approaches with newer execution engines in GPUs for sparse matrices.</li>
<li>Exploring hardware-software co-design as a powerful research direction.</li>
</ul>
</section>
<section id="the-tldr-version-2" class="level3">
<h3 data-anchor-id="the-tldr-version-2">The TL;DR Version</h3>
<ol type="1">
<li>Deep networks learn slowly due to limitations in optimization techniques like SGD.</li>
<li>Key challenges include navigating complex loss landscapes, escaping local minima, tuning hyperparameters, and avoiding mere memorization.</li>
<li>We need to rethink our approach to model architecture and learning processes.</li>
<li>Promising directions include advanced optimization algorithms, sparse matrix techniques, and hardware-software co-design.</li>
</ol>
<p>Remember, in the world of AI, we’re not just teaching machines to learn – we’re learning how to teach. And right now, we’re realizing we might need to go back to teacher school ourselves.</p>
</section>
</section>
<section id="conclusion-the-tip-of-the-ai-iceberg" class="level2">
<h2 data-anchor-id="conclusion-the-tip-of-the-ai-iceberg">Conclusion: The Tip of the AI Iceberg</h2>
<p>As we’ve explored in this article, the challenges facing AI development are numerous and complex. We’ve only scratched the surface of the data scarcity issue, and there are still two major hurdles we haven’t yet discussed: power consumption and system complexity.</p>
<p>The energy requirements for training and running these increasingly large AI models are staggering. As we push the boundaries of model size and complexity, we’re also pushing the limits of our computational infrastructure. The power consumption of these models isn’t just a technical issue—it’s an environmental and economic concern that the AI community will need to address.</p>
<p>System complexity is another critical challenge. As our AI systems grow more sophisticated, managing and optimizing them becomes increasingly difficult. We’re rapidly approaching a point where the complexity of these systems may outpace our ability to understand and control them effectively.</p>
<p>Down the road I intend to delve deeper into these issues, exploring the implications of AI’s growing energy appetite and the challenges posed by increasingly complex systems. In the meanwhile if there are things in this space that you would like to learn about - DMs are always open!</p>
<ol type="1">
<li><a href="https://arxiv.org/pdf/2407.21783">The Llama 3 Herd of Models</a></li>
<li><a href="https://huggingface.co/datasets/HuggingFaceFW/fineweb">Fineweb dataset</a></li>
<li><a href="https://arxiv.org/abs/2203.15556">Training Compute-Optimal Large Language Models</a></li>
<li><a href="https://x.com/karpathy/status/1781028605709234613?lang=en"><span class="citation" data-cites="Karpathy">@Karpathy</span></a></li>
<li><a href="https://arxiv.org/abs/2305.07759">TinyStories: How Small Can Language Models Be and Still Speak Coherent English?</a></li>
<li><a href="https://arxiv.org/abs/2404.01413">Is Model Collapse Inevitable? Breaking the Curse of Recursion by Accumulating Real and Synthetic Data</a></li>
<li><a href="https://arxiv.org/pdf/1802.09568">Shampoo: Preconditioned Stochastic Tensor Optimization</a></li>
</ol>


</section>


<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/shwetank-kumar\.github\.io\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://utteranc.es/client.js" repo="shwetank-kumar/blog-comments" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->




</body></html>