<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.56">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Shwetank Kumar">
<meta name="dcterms.date" content="2024-09-16">

<title>Less Magic, More Math: Why Inference Scaling is the New Black – Shwetank Kumar</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-WYRPY1S3G7"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-WYRPY1S3G7', { 'anonymize_ip': true});
</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Shwetank Kumar</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../blog.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../favorite-eggheads.html"> 
<span class="menu-text">Eggheadery</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/shwetank-kumar"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/shwetankumar"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/shwetankkumar"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../index.xml"> <i class="bi bi-rss" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default">
  <div class="quarto-title-banner">
    <div class="quarto-title column-body">
      <h1 class="title">Less Magic, More Math: Why Inference Scaling is the New Black</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">genai</div>
                <div class="quarto-category">inference</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Shwetank Kumar </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">September 16, 2024</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-layout-custom page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
<!-- main -->





<p>Ah, the sweet smell of progress in the morning! OpenAI has just dropped their shiny new o1-mini and o1 models, and the internet is abuzz with hot takes faster than you can say “AGI winter is coming.” You’ve probably seen the YouTube videos: wide-eyed tech bros marveling at the models’ ability to solve differential equations or explain quantum mechanics to their goldfish.</p>
<p>But let’s be real: you’re not here for another “OMG, AI can [insert mundane task]!” video. No, you, dear reader, are a person of substance. You crave the meaty, first-principles concepts that will help you navigate the choppy waters of AI progress without drowning in a sea of hype. You’re tired of every Tom, Dick, and Elon barging into the AI field like a bull in a china shop, asking everyone to consider their groundbreaking idea of a <a href="https://en.wikipedia.org/wiki/Spherical_cow">spherically symmetric cow in n-dimensional Hilbert space.</a></p>
<p>That’s why we’re diving into inference scaling today. It’s a crucial concept that’s driving the impressive performance of these new models, and understanding it is key to grasping the current state of AI technology.</p>
<p>Now, if you’re here to debate whether we’re on the cusp of AGI or if these models are secretly plotting to overthrow humanity and replace us with more efficient toasters, I’m going to have to ask you to see yourself out. There are plenty of Twitter threads and subreddits where you can indulge in that particular flavor of speculation. We’re here for the nitty-gritty, the nuts and bolts, the “how does this actually work?” of it all.</p>
<p>So, strap in, silence your phone (unless you’re reading this on it, in which case, carry on), and let’s embark on a journey into the heart of inference scaling. It’s time to learn why OpenAI is betting big on this approach, and why you should care. Welcome to the bleeding edge of AI, where the cows are spherical, the Hilbert spaces are infinite, and the potential for groundbreaking insights is limitless.</p>
<blockquote class="blockquote">
<p><strong>Disclaimer:</strong> Before we dive in, I just want to be super clear: what follows is my best understanding of inference scaling based on publicly available information. As for what’s really going on inside OpenAI’s secret volcano lair? Well, your guess is as good as mine. They’re about as forthcoming with their methods as a cat is with its tax returns.</p>
</blockquote>
<blockquote class="blockquote">
<p><strong>Navigation Prompt:</strong> If you are familiar with different LLM layers, how they work and what roles they accomplish you can skip this section otherwise please read on.</p>
</blockquote>
<section id="the-assembly-line-of-sentences-how-language-models-work-infer" class="level1">
<h1>The Assembly Line of Sentences: How Language Models Work (Infer)</h1>
<p>Now that we’ve cleared the air of AGI speculation and spherical bovines, let’s delve into the intricacies underpinning inference scaling. Note that this is not only about throwing more FLOPs at the problem. To understand how inference scaling works we need to first understand how basic inference works.</p>
<p>Imagine, for a moment, that you’re running a factory. Not just any factory, mind you, but one that produces bespoke, artisanal sentences on demand. Your raw materials? Vectors of floating-point numbers. Your end product? Everything from Shakespearean sonnets to Python code.</p>
<p>Welcome to the world of Large Language Model inferencing. It’s a bit like running a just-in-time manufacturing operation, except instead of assembling cars, you’re assembling words. And let me tell you, the logistics are <em>fascinating</em>.</p>
<p>Let’s break down this process, shall we? It all starts with a prompt. Think of this as the order form for your sentence factory. “I need a limerick about database optimization, stat!” your customer (who is suspiciously often yourself) demands. Your factory springs into action:</p>
<ol type="1">
<li><p><strong>The Tokenizer</strong>: This is your receiving department. It breaks down the incoming order into bite-sized pieces the rest of the factory can work with. “Database” might become “data” and “base”, while “optimization” could be “optim” and “ization”. It’s like those old “FRAGILE: HANDLE WITH CARE” stamps, except here it’s “LANGUAGE: HANDLE WITH VECTORS”. The tokenizer layer is the gatekeeper of language models, transforming raw text into a format the model can understand. Think of it as a translator that converts human-readable text into a sequence of numbers (tokens) that the AI can process.</p>
<pre><code>                          Tokenize(text) = [token_1, token_2, ..., token_n]</code></pre>
<p>where <span class="math inline">\(token_i = vocabulary_{index}(subword_i)\)</span>. Algorithmically, the process works in 3 sub-steps:</p>
<ul>
<li><strong>Text Segmentation:</strong> The input text is broken down into subwords, words, or characters, depending on the tokenization strategy.</li>
<li><strong>Vocabulary Lookup:</strong> Each subword is mapped to a unique integer index in the model’s vocabulary.</li>
<li><strong>Token Generation:</strong> The sequence of these indices forms the final token sequence.</li>
</ul></li>
<li><p><strong>The Embedding Layer</strong>: This is where the magic of turning words into numbers happens. It’s as if each word is run through a very complex, very mathy Instagram filter. “Data” doesn’t just mean “data” anymore; it’s now a list of 768 floating-point numbers that somehow capture the essence of “data-ness”. Imagine the embedding layer as a massive, multidimensional dictionary or lookup table. Here’s is its structure:</p>
<ul>
<li><p><strong>Rows:</strong> Each row in this table corresponds to a token in your vocabulary. If your model has a vocabulary of 50,000 words, your table has 50,000 rows.</p></li>
<li><p><strong>Columns:</strong> Each column represents a dimension of the embedding. If you’re using 300-dimensional embeddings, your table has 300 columns.</p></li>
<li><p><strong>Cells:</strong> Each cell in this table contains a single number, typically a floating-point value.</p>
<pre><code>                           Embedding(token_id) = LookupTable[token_id]</code></pre></li>
</ul>
<p>where LookupTable is a matrix of shape (vocab_size, embedding_dim)</p></li>
<li><p><strong>The Transformer Layers</strong>: These layers have been covered very well in multiple posts all over the net, specifically look at [3] for more detail. This is the real engine of your factory. Imagine a room full of savants, each one looking at your partial sentence, conferring with other savants based on information they have, and then scribbling notes about what word should come next. Now imagine doing this dozens of times in parallel while sharing information. That’s what’s happening here, except the savants are matrix multiplications and the notes are more vectors. There are other parts to the layer but at the heart of of this communcation and compute is the attention layer which can be specified by Key, Value, Query triplets (K,Q,V) as follows: <span class="math display">\[
                                 Attention(Q, K, V) = softmax((QK^T) / √d_k) V
\]</span></p>
<p>where, Key represents information that each of them has, Query is each one asking - hey who has this information that I need and Value is the information that they share with each other once they identify the overlap between whats needed and what they have based on the dot product between K and Q.</p></li>
<li><p><strong>The Output Layer</strong>: This is your quality control department. It takes all those scribbled savant notes and turns them into actual probabilities for each word in your vocabulary. “The” might get a 2% chance, “database” a 0.5% chance, and “aardvark” a 0.0001% chance. (Hey, you never know when you might need an aardvark in your limerick about databases).</p>
<p>This is embodied by our Softmax layer. It takes the raw scores (often called logits) associated with each possible token in the vocabulary and transforms them into a probability distribution. Essentially, for each position in a sequence, the softmax layer calculates the likelihood of each token in the vocabulary appearing in that position. It does this by exponentiating the scores and then normalizing them so that they sum to 1. This process ensures that the model outputs a valid probability distribution over all possible tokens, allowing it to make predictions about the most likely next token in a sequence or to classify tokens into different categories. Mathematically, you can write it as: <span class="math display">\[
softmax(x_i) = \frac{\exp(x_i)}{\sum_j \exp(x_j)}
\]</span></p>
<p>where i is the current token and j iterates over all tokens in the vocabulary.</p></li>
<li><p><strong>Token Selection and Iteration</strong>: This is where your factory’s assembly line completes its cycle by choosing the next token based on Softmax output, transforming probability distributions into the next token in the sequence. This step is also where you decide how wild you want your factory’s output to be. e.g.&nbsp;Greedy search tends to produce more repetitive text, while sampling methods can lead to more varied and sometimes surprising results*.</p>
<p>Finally, we get a sequence of tokens that we convert into readable text using the inverse transform from look up tables in Step 1. i.e.&nbsp;if you were inputting rows and reading off columns as the output, here you input columns and read off rows and vice-versa.</p></li>
</ol>
<p>By repeating this process, your AI factory can churn out everything from simple sentences to complex essays - Or entire essay about why PostgreSQL is actually a misunderstood performance art piece. (Don’t judge. GPT-4o has some weird hobbies.) How you generate tokens, iterate, and post process in this last step is what defines how “thoughtful” your LLM sounds. Lets dig into that in the next section.</p>
<blockquote class="blockquote">
<p><strong>Navigation Prompt:</strong> Details of algorithms in Step 5 is what inference scaling is all about. If you are familiar with various token generation strategies skip ahead otherwise please read on.</p>
</blockquote>
<section id="the-fundamentals-of-token-selection-navigating-vast-probability-spaces" class="level2">
<h2 data-anchor-id="the-fundamentals-of-token-selection-navigating-vast-probability-spaces">The Fundamentals of Token Selection: Navigating Vast Probability Spaces</h2>
<p>At its core, inference in a Large Language Model (LLM) is about predicting the next token in a sequence, given the previous tokens. It’s like playing a cosmic game of “what comes next?” where the stakes are coherent communication. LLMs accomplish this feat by learning the underlying distribution of the training data and storing a compressed version of it in their parameters. However, this nominally simple task hides a universe of complexity that would make even the most seasoned computer scientists break out in a cold sweat.</p>
<section id="the-vastness-of-possibility" class="level3">
<h3 data-anchor-id="the-vastness-of-possibility">The Vastness of Possibility</h3>
<p>Imagine you’re an LLM with a vocabulary of |V| tokens, trying to generate a sequence of length T. The naive search space for this task is O(|V|^T). To put this into perspective, if you have a modest vocabulary of 50,000 tokens (which is on the small side for modern LLMs) and you’re generating a sequence of just 20 tokens, you’re looking at 50,000^20 possibilities.</p>
<p>That’s a number so large it makes the number of atoms in the observable universe look like pocket change. If each possibility were a grain of sand, you’d have enough to build a beach that stretches from here to Alpha Centauri, with enough left over to fill the Mariana Trench. Twice.</p>
<p>Clearly, brute-forcing our way through this cosmic beach of possibilities isn’t feasible, even with the most powerful supercomputers at our disposal. We need a smarter approach.</p>
</section>
<section id="enter-the-decoding-strategies" class="level3">
<h3 data-anchor-id="enter-the-decoding-strategies">Enter the Decoding Strategies</h3>
<p>So given these insurmountable odds, how do LLMs figure out what token to generate next? Enter the world of decoding strategies! Depending on the strategies used and the choice of parameters used to tune them you might get text that is coherent, engaging, surprising or a combination there of. Just remember it will always be sampled from the underlying probability distribution that was learned during the training phase.</p>
<p>Let’s dive into some of these strategies which we will need to understand inference scaling. There are many more which are well covered in [2] and similar articles.</p>
<section id="beam-search-the-chess-grandmaster" class="level4">
<h4 data-anchor-id="beam-search-the-chess-grandmaster">1. Beam Search: The Chess Grandmaster</h4>
<p>Beam search is like a chess grandmaster, always thinking several moves ahead to find the best overall strategy. It explores multiple possible sequences simultaneously, keeping track of the most promising paths.</p>
<p>At each step, beam search maintains a set number (let’s call it <code>k</code>) of the most likely partial sequences, known as hypotheses. This allows the model to consider sequences that may start with lower probability tokens but have higher overall probability, potentially leading to better quality outputs than simple greedy search.</p>
<p>The beauty of beam search is that it strikes a balance between exploration and exploitation. It’s not just picking the best immediate option, but considering how that choice might pan out in the long run. The final output is the sequence with the highest overall probability.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Beam Search Example</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>checkpoint <span class="op">=</span> <span class="st">"openai-community/gpt2-medium"</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(checkpoint)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> AutoModelForCausalLM.from_pretrained(checkpoint)</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> tokenizer(<span class="st">"The cat sat on the"</span>, return_tensors<span class="op">=</span><span class="st">"pt"</span>)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> model.generate(<span class="op">**</span>inputs, num_beams<span class="op">=</span><span class="dv">5</span>, max_new_tokens<span class="op">=</span><span class="dv">50</span>)</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(tokenizer.decode(outputs[<span class="dv">0</span>], skip_special_tokens<span class="op">=</span><span class="va">True</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Importantly, traditional beam search with a width k reduces our astronomical search space from O(|V|^T) to O(k|V|T). That’s a significant improvement, though still a hefty computational load for real-time applications!</p>
</section>
<section id="multinomial-sampling-the-creative-writer" class="level4">
<h4 data-anchor-id="multinomial-sampling-the-creative-writer">2. Multinomial Sampling: The Creative Writer</h4>
<p>If beam search is our chess grandmaster, multinomial sampling is the free-spirited writer who throws caution to the wind and lets inspiration guide their pen.</p>
<p>This method introduces an element of controlled randomness into the text generation process. Unlike greedy search, which always picks the most probable token, multinomial sampling randomly selects the next token based on the probability distribution provided by the model. It’s like rolling a weighted die, where each face represents a possible next word, and the size of each face corresponds to its probability.</p>
<p>Why is this important? It allows for more diverse outputs. Every token with a non-zero probability has a chance of being selected, reducing the risk of bland, repetitive text. It’s how AI can surprise us with creative turns of phrase or unexpected (yet coherent) continuations of a prompt.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Multinomial Sampling Example</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoTokenizer, AutoModelForCausalLM</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>checkpoint <span class="op">=</span> <span class="st">"openai-community/gpt2-large"</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(checkpoint)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> AutoModelForCausalLM.from_pretrained(checkpoint)</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> tokenizer(<span class="st">"The cat sat on the"</span>, return_tensors<span class="op">=</span><span class="st">"pt"</span>)</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> model.generate(<span class="op">**</span>inputs, do_sample<span class="op">=</span><span class="va">True</span>, num_beams<span class="op">=</span><span class="dv">1</span>, max_new_tokens<span class="op">=</span><span class="dv">50</span>)</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(tokenizer.decode(outputs[<span class="dv">0</span>], skip_special_tokens<span class="op">=</span><span class="va">True</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="beam-search-multinomial-sampling-the-best-of-both-worlds" class="level4">
<h4 data-anchor-id="beam-search-multinomial-sampling-the-best-of-both-worlds">3. Beam Search Multinomial Sampling: The Best of Both Worlds</h4>
<p>What if we could combine the strategic foresight of beam search with the creative spark of multinomial sampling? Enter beam search multinomial sampling, the hybrid approach that aims to give us the best of both worlds.</p>
<p>This method maintains multiple hypotheses like beam search, but instead of always choosing the most probable token for each beam, it samples from the probability distribution. It’s as if our chess player occasionally makes a slightly unorthodox move, not because it’s objectively the best, but because it might lead to an interesting game state.</p>
<p>The result? Outputs that are both high-quality and diverse, striking a delicate balance between coherence and creativity.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Beam Search Multinomial Sampling Example</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoModelForSeq2SeqLM</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>checkpoint <span class="op">=</span> <span class="st">"google-t5/t5-small"</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(checkpoint)</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> AutoModelForSeq2SeqLM.from_pretrained(checkpoint)</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> tokenizer(<span class="st">"translate English to German: The cat sat on the mat."</span>, return_tensors<span class="op">=</span><span class="st">"pt"</span>)</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> model.generate(<span class="op">**</span>inputs, num_beams<span class="op">=</span><span class="dv">5</span>, do_sample<span class="op">=</span><span class="va">True</span>, max_new_tokens<span class="op">=</span><span class="dv">50</span>)</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(tokenizer.decode(outputs[<span class="dv">0</span>], skip_special_tokens<span class="op">=</span><span class="va">True</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="diverse-beam-search-the-brainstorming-session" class="level4">
<h4 data-anchor-id="diverse-beam-search-the-brainstorming-session">4. Diverse Beam Search: The Brainstorming Session</h4>
<p>Sometimes, we don’t just want one good output – we want several distinctly different options. That’s where diverse beam search comes in. Think of it as a brainstorming session where participants are explicitly told to come up with ideas that are different from each other.</p>
<p>Diverse beam search divides the beams into groups and applies a diversity penalty to ensure that the outputs from different groups are distinct. Within each group, standard beam search is applied. This approach is particularly useful when you want to generate multiple alternative outputs that are significantly different from each other, not just minor variations.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Diverse Beam Search Example</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>checkpoint <span class="op">=</span> <span class="st">"google/pegasus-xsum"</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(checkpoint)</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> AutoModelForSeq2SeqLM.from_pretrained(checkpoint)</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> tokenizer(<span class="st">"Summarize: The cat sat on the mat. The dog slept by the fire. The bird sang in the tree."</span>, return_tensors<span class="op">=</span><span class="st">"pt"</span>)</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> model.generate(<span class="op">**</span>inputs, num_beams<span class="op">=</span><span class="dv">10</span>, num_beam_groups<span class="op">=</span><span class="dv">5</span>, diversity_penalty<span class="op">=</span><span class="fl">1.0</span>, max_new_tokens<span class="op">=</span><span class="dv">30</span>)</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(tokenizer.decode(outputs[<span class="dv">0</span>], skip_special_tokens<span class="op">=</span><span class="va">True</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="the-art-of-balancing" class="level3">
<h3 data-anchor-id="the-art-of-balancing">The Art of Balancing</h3>
<p>Each of these decoding strategies offers different trade-offs between output quality, diversity, &amp; computational cost. The choice of strategy depends on the specific requirements of your task and the desired characteristics of the generated text. For example by playing with just a few parameters you can go from Greedy algorithm with extremely repetitive responses to versatile text giving human like responses.</p>
<p>Understanding these strategies is crucial for anyone working with or developing language models. They’re not just technical details – they’re the brushstrokes that determine the final picture painted by our AI artists and to emulate reflection and thoughtfulness by your friendly neighborhood AI which is what we cover in the next section.</p>
</section>
</section>
<section id="in-ais-deep-reflection-where-probability-flirts-with-search-and-models-judge-other-models-pickup-lines" class="level2">
<h2 data-anchor-id="in-ais-deep-reflection-where-probability-flirts-with-search-and-models-judge-other-models-pickup-lines">In AI’s Deep Reflection: Where Probability Flirts with Search and Models Judge Other Models’ Pickup Lines</h2>
<p>For years, the AI community has been on a bulk-up routine that would make bodybuilders jealous - just feed the model more parameters and data. Inference scaling changes this up by giving AI models a gym membership and a personal trainer instead. It asks the question - instead of always making our models bigger, how do we make them think harder? Let’s dive into the three main approaches for that with a bit more technical detail this time.</p>
<section id="parallel-sampling-the-brute-force-charmer" class="level3">
<h3 data-anchor-id="parallel-sampling-the-brute-force-charmer">1. Parallel Sampling: The Brute Force Charmer</h3>
<p>Parallel sampling is a straightforward yet powerful technique in the world of inference scaling. At its core, it’s about generating multiple independent solutions and selecting the best one.</p>
<p><strong>How It Works:</strong></p>
<p>When presented with an input, the AI model doesn’t generate just one answer. Instead, it produces N complete, independent answers. This process is akin to running the model N times in parallel, each time generating a full response to the input.</p>
<p>The key to this method’s effectiveness lies in its evaluation step. We employ a sophisticated evaluator, an Outcome Reward Model (ORM). This is an AI model, trained to assess the quality of generated answers based on various criteria. The ORM examines each of the N answers and assigns them a score. This scoring isn’t just based on correctness, but can include factors such as the quality of reasoning, clarity of explanation, and adherence to the task requirements.</p>
<p>Once all N answers have been scored, we can use a “best-of-N weighted” selection method to choose the final answer. This method is more nuanced than simply selecting the highest-scoring response. Instead, it considers all answers that arrived at the same conclusion and sums their scores. The conclusion with the highest total score is then selected as the final output.</p>
<p><strong>Mathematical Formulation:</strong></p>
<p>For a given input x:</p>
<ol type="1">
<li>Generate outputs: y₁, y₂, …, yₙ</li>
<li>Score each output: s₁ = PRM(y₁), s₂ = PRM(y₂), …, sₙ = PRM(yₙ)</li>
<li>Final selection: y* = argmax(s₁, s₂, …, sₙ)</li>
</ol>
<p>In practice, the selection process might be more complex, involving grouping similar outputs and summing their scores before selecting the highest-scoring group.</p>
<p><strong>Strengths:</strong></p>
<ol type="1">
<li>Simple and scalable. Improving results often involves simply increasing N, effectively leveraging additional computational resources to boost performance - a clear trade-off.</li>
<li>Works well for easier questions or tasks where generating a large number of attempts is likely to produce at least one high-quality answer.</li>
<li>Works especially when the space of possible good answers is relatively large and diverse.</li>
</ol>
<p><strong>Weaknesses:</strong></p>
<ol type="1">
<li>Computationally inefficient for complex problems.</li>
<li>While it will eventually find a good solution if N is large enough, it may use significantly more resources than more targeted approaches.</li>
<li>The compute costs can become prohibitive if N is set too high, especially for resource-intensive models.</li>
<li>Each generation is independent, potentially repeating work or mistakes made in other attempts. It doesn’t learn from or build upon partial successes within a single attempt.</li>
</ol>
<p>All-in-all it is a simple but brute-force approach to inference scaling. Its straightforward nature makes it an attractive option, especially when dealing with tasks where the generation of multiple diverse attempts is likely to yield at least one high-quality result. However, its effectiveness needs to be balanced against its potential computational costs, especially for more complex tasks or when scaling to very large N.</p>
</section>
<section id="beam-search-the-chess-grandmaster-of-inference" class="level3">
<h3 data-anchor-id="beam-search-the-chess-grandmaster-of-inference">2. Beam Search: The Chess Grandmaster of Inference</h3>
<p>Unlike parallel sampling, which generates complete independent answers, beam search constructs solutions incrementally, making decisions at each step about which partial solutions are most promising to develop further.</p>
<p><strong>How It Works:</strong></p>
<ol type="1">
<li>Initialization: The process begins by generating N initial predictions for the first step of the solution. These represent different starting points for the answer.</li>
<li>Scoring: A Process Reward Model (PRM) is used to score each of these N initial steps. The PRM evaluates the quality and potential of each partial solution. Note that this is also how it is different from an ORM above.</li>
<li>Pruning: Instead of keeping all N initial steps, beam search retains only the top N/M highest-scoring ones, where M is the beam width. This step focuses the search on the most promising partial solutions.</li>
<li>Expansion: For each of the retained partial solutions, the model generates M new proposals for the next step. This brings the total number of candidates back to N (N/M * M = N).</li>
<li>Iteration: Steps 2-4 are repeated until either a complete solution is reached or a maximum number of rounds is hit.</li>
</ol>
<p>This process allows beam search to maintain a diverse set of partially completed solutions, continually evaluating and refining them as it progresses.</p>
<p><strong>Mathematical Formulation:</strong></p>
<p>At each step t:</p>
<ol type="1">
<li>Generate candidates: C_t = {c₁, c₂, …, cₙ}</li>
<li>Score candidates: S_t = {PRM(c₁), PRM(c₂), …, PRM(cₙ)}</li>
<li>Keep top k: B_t = top_k(C_t, S_t, k=N/M)</li>
<li>Expand: C_{t+1} = ⋃_{b ∈ B_t} expand(b, M)</li>
</ol>
<p>where:</p>
<ul>
<li>C_t is the set of candidate partial solutions at step t</li>
<li>S_t is the set of scores for these candidates</li>
<li>B_t is the set of top-scoring candidates retained</li>
<li>expand(b, M) generates M new candidates from partial solution b</li>
</ul>
<p><strong>Strengths:</strong></p>
<ol type="1">
<li>Complex problem-solving where the solution is built incrementally.</li>
<li>Natural language generation tasks requiring coherent, long-form responses.</li>
<li>Multi-step planning or decision-making processes.</li>
</ol>
<p><strong>Weaknesses:</strong></p>
<ol type="1">
<li>There is potential for pruning prematurely. By discarding lower-scoring partial solutions early, beam search might miss unconventional but ultimately superior solutions that don’t appear promising in their early stages.</li>
<li>While more efficient than exhaustive search, beam search can still be computationally intensive, especially with large beam widths or for problems requiring many steps.</li>
<li>Its performance can be highly dependent on the choice of N and M. Optimal values may vary significantly between different types of problems.</li>
</ol>
<p>Beam search is particularly suited for problems where the solution quality depends on a series of interconnected decisions. Its ability to maintain and explore multiple promising solution paths makes it a go-to approach for complex, multi-step reasoning tasks in AI.</p>
</section>
<section id="revision-based-approach-the-perfectionists-dream" class="level3">
<h3 data-anchor-id="revision-based-approach-the-perfectionists-dream">3. Revision-Based Approach: The Perfectionist’s Dream</h3>
<p>This approach is focuses on iterative improvement of an initial response. Unlike parallel sampling or beam search, which generate multiple independent answers or explore multiple paths simultaneously, this approach generates a single answer and then repeatedly refines it.</p>
<p><strong>How It Works:</strong></p>
<ol type="1">
<li>Initial Generation: The model produces an initial answer to the given input.</li>
<li>Iterative Refinement: The model is then presented with its previous answer(s) along with the original input and asked to generate an improved version.</li>
<li>Repetition: This process of refinement is repeated for a predetermined number of iterations or until some convergence criterion is met.</li>
<li>Selection: Once all iterations are complete, the best version is selected using either an Outcome Reward Model (ORM) or a majority voting mechanism.</li>
</ol>
<p><strong>Mathematical Formulation:</strong></p>
<pre><code>1. Initial generation: y₀ = model(x)
2. for i = 1 to k:
      yᵢ = model(x, y₀, y₁, ..., yᵢ₋₁)
3. Final selection:
      y* = argmax(ORM(y₀), ORM(y₁), ..., ORM(yₖ))
      or
      y* = mode(y₀, y₁, ..., yₖ)</code></pre>
<p>Where:</p>
<ul>
<li>x is the input</li>
<li>yᵢ is the i-th revision</li>
<li>k is the total number of revisions</li>
<li>ORM is the Outcome Reward Model</li>
</ul>
<p><strong>Key Components:</strong></p>
<ol type="1">
<li>Revision Model: This is typically a fine-tuned version of the base language model, specifically trained to improve upon previous answers. The training process involves exposing the model to sequences of increasingly better answers to the same question.</li>
<li>Outcome Reward Model (ORM): This is a separate model trained to evaluate the quality of complete answers. It’s used to score each revision and select the best one.</li>
<li>Majority Voting: An alternative to ORM, this method selects the most common answer among all revisions. It’s particularly useful when multiple independent revision sequences are generated.</li>
</ol>
<p><strong>Strengths:</strong></p>
<ol type="1">
<li>This approach allows for gradual refinement of the answer, potentially leading to high-quality results, especially for medium-difficulty questions.</li>
<li>It can be more computationally efficient than generating multiple complete answers, as in parallel sampling.</li>
<li>By considering previous attempts, the model can build upon its own insights and correct its mistakes.</li>
</ol>
<p><strong>Weaknesses:</strong></p>
<ol type="1">
<li>This approach may get stuck in a local optimum, repeatedly making similar refinements without considering radically different approaches.</li>
<li>The quality of the final output can be heavily influenced by the quality of the initial answer.</li>
<li>Training a model to effectively revise its own work is a complex task, requiring sophisticated training data and techniques.</li>
</ol>
<p>Revision-based approach represents a unique strategy in inference scaling, mimicking a facet of the human creative process of drafting and refining ideas. As I have mentioned elsewhere that I don’t think that’s the whole story as far as human creativity is concerned. However, the ability to iteratively improve upon its own output makes for a powerful tool in the AI toolkit, especially for tasks where quality can be enhanced through careful refinement and consideration of previous attempts.</p>
</section>
</section>
<section id="conclusion-the-art-of-thinking-harder-not-just-bigger" class="level2">
<h2 data-anchor-id="conclusion-the-art-of-thinking-harder-not-just-bigger">Conclusion: The Art of Thinking Harder, Not Just Bigger</h2>
<p>This post has become way longer than originally intended so thanks for staying with it. For more details on performance metrics, and specific use-cases for each technique, I highly recommend checking out reference [3] below.</p>
<p>As I wrap up this journey, it’s clear to me that we are amid a paradigm shift (no not AGI). We’re moving from an era of “bigger is better” during training to a more nuanced approach that folds in clever inference algorithms as well in which ORM and PRM models are the unsung heroes. Their quality will set the upper bound on the quality of your inference scaling strategy.</p>
<p>And while these techniques will enable us to use copilots capable of more nuanced approaches to the problem this shift also means what earlier used to be a heavy training related capital expense borne by the model provider will turn more into an operational expense borne by the user.</p>
</section>
<section id="references" class="level2">
<h2 data-anchor-id="references">References</h2>
<ol type="1">
<li><a href="https://jalammar.github.io/illustrated-transformer/">The illustrated transformer</a></li>
<li><a href="https://huggingface.co/docs/transformers/en/generation_strategies">Text generation strategies</a></li>
<li><a href="https://arxiv.org/pdf/2408.03314">Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters</a></li>
</ol>


</section>
</section>


<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/shwetank-kumar\.github\.io\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://utteranc.es/client.js" repo="shwetank-kumar/blog-comments" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->




</body></html>