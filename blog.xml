<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Shwetank Kumar</title>
<link>https://shwetank-kumar.github.io/blog.html</link>
<atom:link href="https://shwetank-kumar.github.io/blog.xml" rel="self" type="application/rss+xml"/>
<description>Shwetank Kumar&#39;s personal website</description>
<generator>quarto-1.5.57</generator>
<lastBuildDate>Thu, 14 Nov 2024 08:00:00 GMT</lastBuildDate>
<item>
  <title>🌙 AI Afterhours: Top AI Papers for Nov 11 - Nov 14, 2024</title>
  <dc:creator>Shwetank Kumar</dc:creator>
  <link>https://shwetank-kumar.github.io/posts/summary-2024-11-15/ai_paper_summaries.html</link>
  <description><![CDATA[ 





<p>Welcome to this week’s AI Afterhours! Your weekly digest of most upvoted papers in AI. Below is gist of the results, how they got them, and why you should care. With that, let’s dive into the most exciting AI research from November 11 to November 14, 2024.</p>
<blockquote class="blockquote">
<p><strong>Programming note 1:</strong> The audio version is on pause this week. If you typically listen to these summaries rather than read them, drop me a note - your feedback will help me prioritize bringing back the audio format.</p>
</blockquote>
<blockquote class="blockquote">
<p><strong>Programming note 2:</strong> Heads up - AI Afterhours will be taking a brief hiatus as I’ll be away until early December. When we return, I’ll have a special edition covering all the fascinating AI developments that occurred during this period. Technology moves quickly, and I’m looking forward to catching you up on everything we missed.</p>
</blockquote>
<iframe src="../../subscribe.html" width="600" height="400" class="newsletter-form">
</iframe>
<p><strong>“Add-it”</strong> tackles one of the most challenging problems in image editing - inserting objects into existing images without requiring additional training. Think of those times you wished you could just drag and drop an object into a photo and have it look natural. That’s exactly what this system does, using something called weighted extended self-attention (imagine a sophisticated way of ensuring new objects fit naturally into their surroundings). The results are impressive - achieving an 83% score on their new affordance benchmark and winning 80% of human evaluations against existing methods. What makes this particularly interesting is that it works with pre-trained models, meaning we could see this technology integrated into consumer photo editing tools sooner rather than later.</p>
<p><a href="https://arxiv.org/pdf/2411.07232v2">arXiv:2411.07232v2</a> 👍53</p>
<p><strong>“OmniEdit”</strong> builds on this theme by creating what they call a “generalist” image editor that learns from specialist models. Think of it as assembling a dream team of image editing experts, each specialized in different tasks, and then teaching a single model to learn from all of them. The approach works remarkably well, scoring 8.38 on their technical VIEScore metric and 7.06 in human evaluations. The practical implications are significant - instead of having different tools for different editing tasks, we might soon have a single, highly capable editor that can handle everything from style transfer to object manipulation.</p>
<p><a href="https://arxiv.org/pdf/2411.07199v1">arXiv:2411.07199v1</a> 👍42</p>
<p><strong>“Large Language Models Can Self-Improve in Long-context Reasoning”</strong> presents SEALONG, a method that helps AI models get better at handling long pieces of text. The interesting part? The model improves itself by generating multiple possible answers and then picking the most consistent one - similar to how humans might solve a complex problem by considering different approaches. The results are impressive: Llama-3.1-8B-Instruct saw a 4.2 point improvement, and when using 128 samples, the system outperformed traditional methods by 11.5% on MuSiQue and 5% on both HotpotQA and 2WikiMultihopQA. This could be a game-changer for applications requiring deep understanding of lengthy documents, from legal analysis to research synthesis.</p>
<p><a href="https://arxiv.org/pdf/2411.08147v1">arXiv:2411.08147v1</a> 👍42</p>
<p><strong>“Language Models are Hidden Reasoners”</strong> introduces LaTRO, a fascinating approach to making AI systems better at reasoning without needing external guidance. Think of it as teaching an AI to be its own critic and coach. The results speak for themselves: models using this approach showed a 12.5% improvement over base models and 9.6% over supervised fine-tuning on the GSM8K dataset. What’s particularly exciting is that this could lead to AI systems that continuously improve their problem-solving abilities, much like how humans learn from experience.</p>
<p><a href="https://arxiv.org/pdf/2411.04282v1">arXiv:2411.04282v1</a> 👍22</p>
<p><strong>“Edify Image”</strong> brings a new approach to generating high-quality images using what’s called Laplacian diffusion models. Imagine building an image like a painter, starting with broad strokes and progressively adding finer details. The system achieves impressive technical metrics - a peak signal-to-noise ratio of 35.6 dB and a structural similarity index of 0.95. What makes this particularly exciting is its ability to handle everything from text-to-image generation to 4K upscaling and 360° HDR panoramas. This could revolutionize how we create and enhance images across industries, from entertainment to professional photography.</p>
<p><a href="https://arxiv.org/pdf/2411.07126v1">arXiv:2411.07126v1</a> 👍27</p>
<p><strong>“M-Longdoc”</strong> tackles the challenge of helping AI understand long, complex documents with both text and images. The researchers created a benchmark dataset with 851 samples and developed a new approach that improved response accuracy by 4.6% compared to existing models. This has huge implications for fields like legal research, academic literature review, and business intelligence where handling lengthy, multimodal documents is crucial.</p>
<p><a href="https://arxiv.org/pdf/2411.06176v1">arXiv:2411.06176v1</a> 👍42</p>
<p><strong>“SAMPart3D”</strong> introduces a fascinating system for breaking down 3D objects into meaningful parts without needing predefined labels. The system achieved a 34.7% score on zero-shot semantic segmentation and 53.7% on class-agnostic part segmentation on their benchmark dataset. This could transform how we work with 3D models in everything from computer-aided design to robotics, making it easier to modify and understand complex 3D objects.</p>
<p><a href="https://arxiv.org/pdf/2411.07184v1">arXiv:2411.07184v1</a> 👍22</p>
<p>That’s a wrap for this week’s AI Afterhours!</p>



 ]]></description>
  <category>Large Language Models</category>
  <category>Natural Language Processing</category>
  <category>Diffusion Models</category>
  <category>3D Object Segmentation</category>
  <category>Laplacian Diffusion Models</category>
  <category>Multimodal Document Understanding</category>
  <category>Language Models</category>
  <category>Image Editing</category>
  <guid>https://shwetank-kumar.github.io/posts/summary-2024-11-15/ai_paper_summaries.html</guid>
  <pubDate>Thu, 14 Nov 2024 08:00:00 GMT</pubDate>
</item>
<item>
  <title>🌙 AI Afterhours: Top AI Papers for Nov 01 - Nov 07, 2024</title>
  <dc:creator>Shwetank Kumar</dc:creator>
  <link>https://shwetank-kumar.github.io/posts/summary-2024-11-08/ai_paper_summaries.html</link>
  <description><![CDATA[ 





<p>Welcome to this week’s AI Afterhours! Your weekly digest of most upvoted papers in AI. Below is gist of the results, how they got them, and why you should care. With that, let’s dive into the most exciting AI research from November 01 to November 07, 2024.</p>
<iframe src="https://podcasters.spotify.com/pod/show/shwetankkumar/embed" height="200px" width="400px" frameborder="0" scrolling="no">
</iframe>
<iframe src="../../subscribe.html" width="600" height="400" class="newsletter-form">
</iframe>
<p><strong>HtmlRAG: HTML is Better Than Plain Text for Modeling Retrieved Knowledge in RAG Systems</strong> introduces a fascinating improvement to how AI systems use external knowledge. Instead of stripping away structure and treating everything as plain text, they show that keeping HTML structure helps AI better understand and use the information. Their approach reduces the size of retrieved content from 1.6M tokens to just 4K while improving accuracy by 4-12%. This is particularly interesting for anyone building systems that need to work with web content - imagine search engines or chatbots that can better understand structured content from the web rather than treating everything as a wall of text.</p>
<p><a href="https://arxiv.org/pdf/2411.02959v1">arXiv:2411.02959v1</a> 👍 49</p>
<p><strong>AndroidLab: Training and Systematic Benchmarking of Android Autonomous Agents</strong> presents a comprehensive framework for developing AI that can actually use Android phones. Their system achieved a 21.5% success rate with text-only models and 13.3% with multimodal models (ones that can see and interact with the screen). While these numbers might seem low, they represent a significant step forward - their best models are now performing at levels comparable to GPT-4 and better than Gemini-1.5-Pro. Think about the implications: we’re getting closer to having AI assistants that can actually help users navigate complex mobile interfaces or automate repetitive tasks on phones.</p>
<p><a href="https://arxiv.org/pdf/2410.24024v2">arXiv:2410.24024v2</a> 👍 45</p>
<p><strong>OS-ATLAS: A Foundation Action Model for Generalist GUI Agents</strong> tackles the challenge of creating AI that can interact with graphical user interfaces across different platforms. Their model achieves an impressive 85.7% accuracy on web interfaces and 58.5% on desktop applications - representing a 20% and 16% improvement over previous best results respectively. This is a big deal for anyone interested in automation or accessibility as it will enable AI assistants that can reliably help users navigate complex software interfaces or automate repetitive tasks across different types of applications.</p>
<p><a href="https://arxiv.org/pdf/2410.23218v1">arXiv:2410.23218v1</a> 👍 43</p>
<p><strong>OpenCoder: The Open Cookbook for Top-Tier Code Large Language Models</strong> shares the complete recipe for building high-performing AI coding assistants. They’re not just sharing code - they’re releasing the full training data (960 billion tokens across 607 programming languages), complete processing pipeline, and detailed protocols. Their model achieves a score of 94.5 on HumanEval, a key coding benchmark. This level of transparency is rare in AI research and could accelerate progress in making better coding assistants accessible to everyone, not just those with massive compute resources.</p>
<p><a href="https://arxiv.org/pdf/2411.04905v1">arXiv:2411.04905v1</a> 👍 41</p>
<p><strong>Unpacking SDXL Turbo: Interpreting Text-to-Image Models with Sparse Autoencoders</strong> dives deep into understanding how modern AI image generation actually works. Using sparse autoencoders, they reveal how different parts of the model specialize in handling different aspects of image generation, with specific components showing causal strengths of up to 0.71 in their specialized areas. Better understanding of these models will lead to more controllable and efficient image generation, potentially reducing the computational resources needed for high-quality AI art.</p>
<p><a href="https://arxiv.org/pdf/2410.22366v1">arXiv:2410.22366v1</a> 👍 38</p>
<p><strong>Both Text and Images Leaked! A Systematic Analysis of Multimodal LLM Data Contamination</strong> reveals concerning insights about data leakage in AI models that work with both text and images. Their detection framework, MM-Detect, found contamination levels of up to 8.7% in certain tasks, with the issue affecting both open-source and proprietary models. Data contamination is the a big issues for developing better models since it makes their evaluation unreliable. This paper provides crucial information for anyone deploying these systems in production environments.</p>
<p><a href="https://arxiv.org/pdf/2411.03823v1">arXiv:2411.03823v1</a> 👍 36</p>
<p><strong>Large Language Models Orchestrating Structured Reasoning Achieve Kaggle Grandmaster Level</strong> describes an AI system that can compete in data science competitions at an expert level. The system achieved a 92.5% success rate across various types of competitions and earned 6 gold medals, placing it in the top 38% of nearly 6,000 human competitors. This demonstrates how AI could automate significant portions of the data science workflow, potentially making sophisticated data analysis more accessible to non-experts. You will need to of course ensure that the cost of getting it wrong 7.5% of the times is not very high!</p>
<p><a href="https://arxiv.org/pdf/2411.03562v1">arXiv:2411.03562v1</a> 👍 32</p>
<p><strong>WebRL: Training LLM Web Agents via Self-Evolving Online Curriculum Reinforcement Learning</strong> introduces a new way to train AI systems that interact with web interfaces. Using their approach, a relatively modest model (Llama-3.1-8B) achieved 42.4% accuracy on web tasks, outperforming even GPT-4-Turbo in specific scenarios like Gitlab (46.7%) and CMS (54.3%). This is particularly exciting because it shows how smaller, more efficient models can be trained to match or exceed the performance of much larger systems in specific domains.</p>
<p><a href="https://arxiv.org/pdf/2411.02337v1">arXiv:2411.02337v1</a> 👍 30</p>
<p>That’s a wrap for this week’s AI Afterhours!</p>



 ]]></description>
  <category>Sparse Autoencoders</category>
  <category>GUI Grounding</category>
  <category>Reinforcement Learning</category>
  <category>Large Language Models</category>
  <category>Multimodal Large Language Models</category>
  <category>Retrieval-Augmented Generation (RAG)</category>
  <category>Android autonomous agents</category>
  <guid>https://shwetank-kumar.github.io/posts/summary-2024-11-08/ai_paper_summaries.html</guid>
  <pubDate>Thu, 07 Nov 2024 08:00:00 GMT</pubDate>
</item>
<item>
  <title>From JSON Chaos to Structure: Your Practical Guide to Taming LLM Output 🎯</title>
  <dc:creator>Shwetank Kumar</dc:creator>
  <link>https://shwetank-kumar.github.io/posts/constrained-generation-guide/</link>
  <description><![CDATA[ 





<p>Ready to transform your LLM outputs from a wild beast into a well-behaved JSON generator? Buckle up, because we’re about to turn that unpredictable text generator into your personal structured data dispenser! Picking up where our <a href="https://shwetank-kumar.github.io/posts/constrained-generation/">deep dive on structured generation</a> using constrained token generation left off, let’s turn theory into practice with a hands-on guide to structured generation!</p>
<p>In this guide (and the accompanying video), we’re diving deep into the world of JSON generation with LLMs. And trust me, if you’ve ever found yourself desperately parsing through paragraphs of explanatory text just to find that one JSON object you asked for, this is going to be your new favorite bedtime story.</p>
<p>Warning: side effects may include significantly fewer 3 AM production incidents!😉</p>
<br><br>
<div class="video-container">
<p><iframe width="560" height="315" src="https://www.youtube.com/embed/_fSphczt7_g" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen=""></iframe></p>
</div>
<p><br></p>
<iframe src="../../subscribe.html" width="600" height="400" class="newsletter-form">
</iframe>
<section id="the-problem-when-llms-try-too-hard-to-help" class="level2">
<h2 class="anchored" data-anchor-id="the-problem-when-llms-try-too-hard-to-help">The Problem: When LLMs Try Too Hard to Help</h2>
<p>Let’s start with what I like to call the “overly helpful assistant syndrome.” Watch what happens when we try to get a simple JSON object for a video game character:</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1">prompt <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"""Generate a video game character using JSON format:</span></span>
<span id="cb1-2"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">{</span></span>
<span id="cb1-3"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    "name": "character name",</span></span>
<span id="cb1-4"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    "age": "character age"</span></span>
<span id="cb1-5"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">}"""</span></span></code></pre></div>
<p>Oh boy. Our LLM friend here decides to throw in a doctoral thesis worth of explanations, complete with Python examples, implementation suggestions, and probably its grandmother’s secret recipe. It’s like asking for directions and getting the entire history of cartography! 🗺️</p>
</section>
<section id="the-be-more-specific-trap" class="level2">
<h2 class="anchored" data-anchor-id="the-be-more-specific-trap">The “Be More Specific” Trap</h2>
<p>“Aha!” you might think, “I’ll just tell it to ONLY give me JSON!” Nice try, but:</p>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1">prompt <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"""Generate ONLY a JSON object for a character.</span></span>
<span id="cb2-2"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">NO explanations. NO additional text. JUST JSON."""</span></span></code></pre></div>
<p>Congratulations! You’ve now received the JSON… plus a step-by-step commentary on why it gave you exactly what you asked for. It’s like telling someone “don’t think about elephants” - guess what they’re thinking about? 🐘</p>
</section>
<section id="the-minimalist-approach-almost-but-not-quite" class="level2">
<h2 class="anchored" data-anchor-id="the-minimalist-approach-almost-but-not-quite">The Minimalist Approach: Almost, But Not Quite</h2>
<p>Getting desperate, we try the bare minimum:</p>
<div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1">prompt <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"""Create a character in JSON format: {"""</span></span></code></pre></div>
<p>This actually gets us closer! But it’s like trying to hit a bullseye while riding a unicycle - sometimes you nail it, sometimes you end up with half a JSON object and a lot of regret.</p>
</section>
<section id="enter-outlines-your-json-whisperer" class="level2">
<h2 class="anchored" data-anchor-id="enter-outlines-your-json-whisperer">Enter Outlines: Your JSON Whisperer</h2>
<p>Here’s where things get interesting. Remember all those tricks and prompting gymnastics? And remember how we talked about coalescence making LLM inference 5x faster in our <a href="previous-post-link">last post</a>? Well, throw out the prompting tricks - Outlines is about to change your life faster than a Silicon Valley startup’s pivot strategy:</p>
<div class="sourceCode" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> pydantic <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> BaseModel, constr</span>
<span id="cb4-2"></span>
<span id="cb4-3"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">class</span> Character(BaseModel):</span>
<span id="cb4-4">    name: constr(max_length<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>)</span>
<span id="cb4-5">    age: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span></span>
<span id="cb4-6"></span>
<span id="cb4-7">generator <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> outlines.generate.json(llm, Character)</span></code></pre></div>
<p>And boom! 💥 Clean, valid JSON every single time:</p>
<div class="sourceCode" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode json code-with-copy"><code class="sourceCode json"><span id="cb5-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">{</span><span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">"name"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Luna"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">"age"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">:</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">25</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">}</span></span></code></pre></div>
<p>No explanations. No war stories. Just pure, pristine JSON that would make your database admin weep tears of joy. This is coalescence in action - turning those theoretical token optimizations we discussed into practical, production-ready code.</p>
</section>
<section id="why-this-is-actually-revolutionary" class="level2">
<h2 class="anchored" data-anchor-id="why-this-is-actually-revolutionary">Why This Is Actually Revolutionary</h2>
<p>Think about what we just did:</p>
<ul>
<li>Turned an unpredictable text generator into a reliable JSON factory</li>
<li>Eliminated the need for prompt engineering wizardry</li>
<li>Got type-safe, schema-validated output every time</li>
<li>Made our error rates drop faster than a tech stock in a bear market</li>
<li>Put those token-level optimizations from our deep dive to work</li>
</ul>
<p>And the best part? This isn’t just about making your code prettier. This is about building reliable systems that don’t fall apart the moment your LLM tries to get creative with its responses - since by construction it cant generate anything outside of the schema we provided it.</p>
</section>
<section id="the-real-magic-scaling-this-up" class="level2">
<h2 class="anchored" data-anchor-id="the-real-magic-scaling-this-up">The Real Magic: Scaling This Up</h2>
<p>Imagine building a product where every API call needs structured data from an LLM. Without proper constraints, you’re basically running a digital Russian roulette. With Outlines / Guidance / Formatron etc. using the power of constrained generation, you’re running a Swiss watch factory.</p>
</section>
<section id="whats-next" class="level2">
<h2 class="anchored" data-anchor-id="whats-next">What’s Next?</h2>
<p>Watch the video above to see this transformation in action. We’ll walk through each step, from chaos to structure, and show you exactly how to implement this in your own projects. Your future self (and your error logs) will thank you.</p>
<p>Remember: In the world of LLMs, structure isn’t just about keeping things neat - it’s about turning possibility into probability, and probability into certainty. Now go forth and generate some JSON! 🚀</p>
<p>P.S. If you’re still using regex or Pydantic to parse LLM outputs, first read our <a href="https://shwetank-kumar.github.io/posts/constrained-generation/">theory deep dive</a> on the subject, then we need to talk. Seriously. 😉</p>


</section>

 ]]></description>
  <category>genai</category>
  <category>inference</category>
  <category>structured generation</category>
  <guid>https://shwetank-kumar.github.io/posts/constrained-generation-guide/</guid>
  <pubDate>Tue, 05 Nov 2024 08:00:00 GMT</pubDate>
</item>
<item>
  <title>🌙 AI Afterhours: Top AI Papers for Oct 25 - Oct 31, 2024</title>
  <dc:creator>Shwetank Kumar</dc:creator>
  <link>https://shwetank-kumar.github.io/posts/summary-2024-11-01/ai_paper_summaries.html</link>
  <description><![CDATA[ 





<p>Welcome to this week’s AI Afterhours! Your weekly digest of most upvoted papers in AI. Below is gist of the results, how they got them, and why you should care. With that, let’s dive into the most exciting AI research from October 25 to October 31, 2024.</p>
<iframe src="https://podcasters.spotify.com/pod/show/shwetankkumar/embed" height="200px" width="400px" frameborder="0" scrolling="no">
</iframe>
<iframe src="../../subscribe.html" width="600" height="400" class="newsletter-form">
</iframe>
<p><strong>CLEAR</strong> tackles a fascinating challenge in AI ethics - helping AI systems “forget” specific individuals across both text and images. Think of it as implementing a digital “right to be forgotten.” The researchers created a benchmark with 200 fictional authors and nearly 8,000 test cases to evaluate how well different methods could selectively remove knowledge about specific people while preserving everything else. They found that a simple mathematical trick - using L1 regularization on specific neural network weights - could significantly reduce unwanted forgetting of other information, which has been a major hurdle in this field. This work is particularly relevant as regulations around AI and privacy rights continue to evolve, potentially requiring AI systems to “unlearn” specific data on request.</p>
<p><a href="https://arxiv.org/pdf/2410.18057v1">arxiv.org/pdf/2410.18057v1</a> 👍 174</p>
<p><strong>Breaking the Memory Barrier</strong> introduces a clever solution to one of the key bottlenecks in training large AI models - GPU memory limitations during contrastive learning. The researchers developed “Inf-CL,” which breaks down massive computations into smaller, manageable chunks that can be processed sequentially. The results are impressive: they reduced memory usage by 78 times for medium-sized batches and 281 times for larger ones, while maintaining competitive processing speeds. Most importantly, they managed to train with batch sizes of up to 12 million samples on 32 A800 GPUs - nearly 10 times more than previous methods. This breakthrough could significantly accelerate the development of self-supervised learning systems and dense text retrieval models.</p>
<p><a href="https://arxiv.org/pdf/2410.17243v1">arxiv.org/pdf/2410.17243v1</a> 👍 46</p>
<p><strong>CORAL</strong> introduces a comprehensive way to evaluate AI systems that combine conversation abilities with fact retrieval. The researchers created a benchmark of 8,000 diverse information-seeking conversations to test how well these systems can find relevant information and generate accurate responses. Interestingly, they found that fine-tuned open-source language models outperformed commercial closed-source ones in retrieval accuracy by 10%. They also discovered that while longer conversation histories generally improved performance, they could introduce redundant information. This work is crucial for developing more reliable AI assistants that can engage in factual conversations while accurately citing their sources.</p>
<p><a href="https://arxiv.org/pdf/2410.23090v1">arxiv.org/pdf/2410.23090v1</a> 👍 35</p>
<p><strong>Can Knowledge Editing Really Correct Hallucinations?</strong> brings some sobering news about our ability to fix AI models that make things up. The researchers developed HalluEditBench, a new way to test how well we can correct false information in large language models. Their findings suggest that current methods for fixing these hallucinations might be less effective than previously thought, with performance varying significantly across different domains and models. While some methods like ICE and GRACE showed promise with efficacy scores above 0.80, they found that fixing one type of error could sometimes create new problems elsewhere. This work is crucial for understanding the limitations of current approaches to making AI systems more truthful.</p>
<p><a href="https://arxiv.org/pdf/2410.16251v2">arxiv.org/pdf/2410.16251v2</a> 👍 35</p>
<p><strong>LOGO</strong> presents a new approach to help large language models better handle long pieces of text. The method achieved a 5-point improvement in average scores on standard benchmarks and managed to match GPT-4’s performance on real-world tasks involving long text. What’s particularly interesting is that LOGO can help smaller models work with longer texts more effectively than other methods, potentially making it easier to deploy more efficient AI systems. This could be especially valuable for applications like document analysis or conversation systems that need to maintain context over long exchanges.</p>
<p><a href="https://arxiv.org/pdf/2410.18533v1">arxiv.org/pdf/2410.18533v1</a> 👍 34</p>
<p><strong>ROCKET-1</strong> demonstrates an impressive advance in getting AI to interact with open-world environments, particularly in Minecraft. Using a combination of visual processing and action planning, their system achieved remarkable success rates: 91% for placing doors, 75% for wool dyeing, and even 100% for basic tool crafting tasks. Think of it as teaching AI to understand and interact with its environment the same way humans do - by seeing, understanding context, and taking appropriate actions. This work has significant implications for robotics and autonomous systems that need to operate in unstructured real-world environments.</p>
<p><a href="https://arxiv.org/pdf/2410.17856v1">arxiv.org/pdf/2410.17856v1</a> 👍 33</p>
<p><strong>ScaleQuest</strong> tackles the challenge of teaching AI systems to reason better through a clever data generation approach. Instead of relying on expensive human-created examples, they used smaller AI models to generate high-quality reasoning problems. The results were impressive: improvements of 29.2% to 46.4% on the MATH dataset compared to baseline models, and a 5.6% to 11.5% improvement over previous best results. They managed to generate 1 million question-answer pairs, showing how we might be able to scale up AI training data creation without prohibitive costs. This could be a game-changer for developing more capable AI systems, especially in domains requiring complex reasoning.</p>
<p><a href="https://arxiv.org/pdf/2410.18693v1">arxiv.org/pdf/2410.18693v1</a> 👍 32</p>
<p><strong>AgentStore</strong> introduces a fascinating new way to make AI systems better at handling computer tasks by combining multiple specialized agents. Think of it as an app store, but for AI agents - each specializing in different tasks but working together seamlessly. The system more than doubled the success rate on the OSWorld benchmark (from 11.21% to 23.85%) and demonstrated a 75% success rate when using their novel AgentToken strategy. With over 20 specialized agents integrated, this work shows how we might build more capable AI assistants by combining multiple experts rather than trying to create a single do-it-all system.</p>
<p><a href="https://arxiv.org/pdf/2410.18603v1">arxiv.org/pdf/2410.18603v1</a> 👍 25</p>
<p><strong>SALAD</strong> brings improvements to AI-generated speech synthesis through a new approach called per-token latent diffusion. The system achieved an impressive intelligibility error rate of just 0.739% compared to baseline approaches that had error rates of 1.231% to 2.298%. What’s particularly interesting is how well it handles challenging cases like accented speech and poor-quality recordings. This could lead to more natural-sounding AI voices and better text-to-speech systems, particularly important for accessibility tools and virtual assistants.</p>
<p><a href="https://arxiv.org/pdf/2410.16048v1">arxiv.org/pdf/2410.16048v1</a> 👍 22</p>
<p><strong>Framer</strong> introduces an interactive way to create smooth transitions between images, essentially letting users control how one image morphs into another. The system achieved the best FVD score (a measure of video quality) among all tested approaches (25.04 compared to competitors’ 24.16-27.04) and was overwhelmingly preferred by human raters (90.5% versus 1.2-4.4% for alternatives). The system’s “autopilot” mode can automatically handle many cases, but users can still step in to guide the process when needed. This could revolutionize video editing and special effects, making complex animation tasks more accessible to creators.</p>
<p><a href="https://arxiv.org/pdf/2410.18978v1">arxiv.org/pdf/2410.18978v1</a> 👍 22</p>
<p>That’s a wrap for this week’s AI Afterhours!</p>



 ]]></description>
  <category>Multi-Agent Systems</category>
  <category>Visual-Temporal Context Prompting</category>
  <category>Machine Learning</category>
  <category>Long-Context Alignment</category>
  <category>Conversational AI</category>
  <category>Large Language Models (LLMs)</category>
  <category>Video Interpolation</category>
  <category>Text-to-Speech Synthesis</category>
  <category>Contrastive Learning</category>
  <category>Large Language Models</category>
  <guid>https://shwetank-kumar.github.io/posts/summary-2024-11-01/ai_paper_summaries.html</guid>
  <pubDate>Thu, 31 Oct 2024 07:00:00 GMT</pubDate>
</item>
<item>
  <title>Why Your Large Language Model Needs Grammar School: The Case for Constrained Generation</title>
  <dc:creator>Shwetank Kumar</dc:creator>
  <link>https://shwetank-kumar.github.io/posts/constrained-generation/</link>
  <description><![CDATA[ 





<p>Ever had that moment when your shiny new AI system confidently generates the most beautiful, grammatically perfect, and utterly broken JSON you’ve ever seen? You know, the kind that makes your production system throw its hands up in digital despair? Welcome to the club! These language models are like that brilliant friend who can explain quantum physics but somehow can’t remember to close their parentheses.</p>
<p>And that’s not just a minor inconvenience – your business runs on structured text. Not the forgiving chatter of Slack, but rigid formats that tolerate zero mistakes. JSON, XML, SQL, CSV – these aren’t just data formats, they’re diplomatic protocols between sovereign digital nations. One misplaced comma, and everything stops. Consider what’s at stake:</p>
<ul>
<li><strong>Payment Processing</strong>: Every transaction triggers dozens of precisely formatted messages coordinating fraud checks, inventory, and settlement</li>
<li><strong>Supply Chain</strong>: Your entire logistics network depends on perfectly structured data exchange</li>
<li><strong>Financial Systems</strong>: Where even minor format errors can trigger compliance alerts and halt operations</li>
</ul>
<p>As a Chief Data Officer, I’ve watched the same crisis unfold countless times: a missing quote freezes orders, a rogue comma corrupts exports, a malformed XML turns a routine deployment into an all-hands crisis. These are so common that companies like Monte Carlo and Accel Data built billion-dollar empires just ensuring data stays clean and correct. In this world, format errors aren’t bugs – they’re declarations of war.</p>
<blockquote class="blockquote">
<p><strong>Programming note:</strong> Welcome new subscribers! This is a special deep dive into a critical gap in the GenAI stack that I’ve been researching extensively. If you have GenAI anywhere near your product, you need to know this: companies are quietly wasting up to 70% of their AI spend. Bookmark this longer-than-usual post for a thorough read. Regular bite-sized programming returns this weekend. Got papers or topics you want covered? Drop me a line at ai.afterhours.shwetank@gmail.com.</p>
</blockquote>
<iframe src="../../subscribe.html" width="600" height="400" class="newsletter-form">
</iframe>
<section id="the-hidden-cost-of-ai-errors" class="level2">
<h2 class="anchored" data-anchor-id="the-hidden-cost-of-ai-errors">The Hidden Cost of AI Errors</h2>
<p>Now enter the Large Language Model (LLM). These systems are remarkable at generating human-like text - ask them to write you a poem about debugging JavaScript, and they’ll craft something worthy of a tech conference keynote. But ask them to generate strictly formatted output like a JSON packet with a specified schema, and they stumble. They’re like brilliant novelists trying to file your taxes - technically capable of understanding numbers, but prone to adding creative flourishes that will make the IRS frown. Let’s try to understand the impact of these structural errors by quantifying them:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 50%">
<col style="width: 50%">
</colgroup>
<thead>
<tr class="header">
<th>Metric</th>
<th>Impact</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Base Error Rate</td>
<td>1-2% of generated structured outputs contain syntax errors</td>
</tr>
<tr class="even">
<td>Daily Requests</td>
<td>10,000 typical for medium-scale API</td>
</tr>
<tr class="odd">
<td>Daily Failures</td>
<td>100-200 requests require regeneration</td>
</tr>
<tr class="even">
<td>Engineer Time per Failure</td>
<td>15 minutes average investigation time</td>
</tr>
<tr class="odd">
<td>Daily Engineering Cost</td>
<td>$3,000-$6,000 (at $200/hour fully loaded cost)</td>
</tr>
<tr class="even">
<td>Annual Impact</td>
<td>$1.1M - $2.2M in direct engineering costs</td>
</tr>
</tbody>
</table>
<p><br></p>
<p>And these calculations assume your errors are merely expensive, not catastrophic. In financial services, where I’ve overseen numerous AI implementations, even a single malformed response can trigger automated safety protocols that halt entire processing pipelines. Several major banks now mandate strict format validation and fallback systems for any AI-generated data - requirements born from hard-learned lessons about what happens when structured data isn’t quite as structured as you thought. The true cost wasn’t just in fixing errors - it was in the growing lack of trust in their AI systems.</p>
</section>
<section id="the-its-just-json-fallacy" class="level2">
<h2 class="anchored" data-anchor-id="the-its-just-json-fallacy">The “It’s Just JSON” Fallacy</h2>
<p>At this point, I can hear the seasoned engineers in the room saying, “Hold on - this is a solved problem, right? We’ll just validate the output after generation. It’s just JSON/XML/SQL, after all!” I call this the “It’s Just JSON” fallacy, and I’ve watched it drain millions from engineering budgets. The reasoning seems sound at first: we have parsers, we have schema validators, we have retry logic. But here’s what actually happens in production:</p>
<ol type="1">
<li>Validation after generation is like spell-checking a letter after you’ve mailed it - you’ve already paid for the postage</li>
<li>Every validation failure triggers another API call to regenerate the content</li>
<li>Each retry not only costs money but adds latency to your customer-facing systems impacting important business metrics like conversion</li>
</ol>
<p>Even worse: some responses can be syntactically perfect but semantically nonsensical - they pass your validators but corrupt your data. For example, a well-formed JSON packet with <code>{"age": -2147483648}</code> might sail through basic JSON schema validation (it’s a valid integer!) while representing an impossible human age that could skew your analytics pipeline.</p>
<p>Here’s what this looks like at scale: One large marketplace built what seemed like a bulletproof system around AI-generated structured data. They had validation layers, retry logic, semantic checks, and fallback systems. Six months in, their cloud bill had tripled and their API costs had quintupled. Their engineers were spending more time fine-tuning validation rules than building new features. They weren’t fixing the problem - they were just getting better at handling failures.</p>
<p>This pattern is so common in enterprise AI that cloud providers have started offering specific tooling around retry logic and validation pipelines. But adding more safety nets doesn’t solve the fundamental problem - it just makes failing more expensive. Welcome to the world of Agentic workflows!</p>
</section>
<section id="teaching-ai-to-mind-its-manners-constrained-generation" class="level2">
<h2 class="anchored" data-anchor-id="teaching-ai-to-mind-its-manners-constrained-generation">Teaching AI to Mind Its Manners: Constrained Generation</h2>
<p>So how do we fix this? The answer is constrained generation, and it’s more elegant than you might expect. Instead of the current “generate and pray” approach, we’re going to put our AI through grammar school. Think of it like teaching an overenthusiastic five-year-old to write - you wouldn’t just hand them a blank piece of paper and correct their mistakes afterward. Instead, you’d give them a template with clear rules.</p>
<p>Let’s look at a concrete example with generating a simple JSON: <code>{"name": "Alice"}</code>. At each step:</p>
<ol type="1">
<li><p>The LLM predicts probabilities for every possible next token:</p>
<ul>
<li>“name” (30% likely)</li>
<li>“{” (20% likely)</li>
<li>“Alice” (15% likely)</li>
<li>“}” (10% likely)</li>
<li>And thousands more possibilities…</li>
</ul></li>
<li><p>A grammar filter examines these predictions and removes any tokens that would break our formatting rules. After typing <code>{"name</code>, only a closing quotation mark is valid - everything else gets zeroed out.</p></li>
<li><p>The system picks from the remaining valid tokens and updates its state. It’s like having autocorrect, but instead of fixing mistakes after you make them, it prevents them entirely.</p></li>
</ol>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: center;"><img src="https://shwetank-kumar.github.io/posts/constrained-generation/flow.png" class="img-fluid" alt="Flow diagram of constrained generation process"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><em>Constrained token generation for structured text</em></td>
</tr>
</tbody>
</table>
<p><br></p>
<p>For the visual learners among us, here’s a flowchart of how these pieces fit together. A structured output is enforced by combining two key components: a state machine that tracks our position in the structure, and a filter that controls what the LLM can generate next. The state machine knows if we’re inside a JSON object, array, or string, while the filter ensures only valid tokens can be selected at each step. For example, after an opening brace ‘{’, the filter only allows string literals that could be valid property names.</p>
</section>
<section id="the-fast-lane-trick" class="level2">
<h2 class="anchored" data-anchor-id="the-fast-lane-trick">The “Fast Lane” Trick</h2>
<p>Now here’s where it gets really interesting: once we know the valid patterns, we can optimize them. Think about it - why should an AI model waste time (and your money) generating tokens that are completely predictable? Using our <code>{"name": "Alice"}</code> example, let’s look at which parts actually need creative thinking versus which parts are just following rules:</p>
<pre><code>{ "name" : "Alice" }
↑   ↑    ↑   ↑     ↑
1   2    3   4     5

1: Must start with {
2: Need to generate the key name
3: Must be ": " (guaranteed sequence)
4: Need to generate the value
5: Must end with }</code></pre>
<p>If you look carefully at position 3 - after you’ve written <code>{"name"</code>, what comes next isn’t a creative decision. It <strong>has to be</strong> a quote mark followed by a colon and a space. There’s no other possibility. So why waste time (and money) having the AI model pretend to think about each of these tokens? Instead, we can skip straight past the <code>": "</code> sequence.</p>
<p>This is like having a GPS that doesn’t just keep you on the right road, but also tells you where you can safely put your foot down. In our example:</p>
<ul>
<li>Have to think: What should the key name be? → “name”</li>
<li>Fast lane: The <code>": "</code> sequence is automatic</li>
<li>Have to think: What should the value be? → “Alice”</li>
<li>Fast lane: The closing <code>}</code> is automatic</li>
</ul>
<p>By identifying these guaranteed sequences, we can skip generating probabilities for tokens that are 100% predetermined. In a simple example like this, we might save just a few tokens, but in complex JSON structures with nested objects and arrays, these savings add up quickly.</p>
</section>
<section id="zero-not-fewer-making-structural-llm-errors-impossible" class="level2">
<h2 class="anchored" data-anchor-id="zero-not-fewer-making-structural-llm-errors-impossible">Zero, Not “Fewer”: Making Structural LLM Errors Impossible</h2>
<p>Let’s examine what happens when an LLM encounters decision points while generating structured data. At the value position in our <code>{"name": "Alice"}</code> example (after <code>{"name":</code>), here’s what the model initially predicts:</p>
<pre><code>"Alice"    : 0.15
"Bob"      : 0.12
42         : 0.08
true       : 0.05
{          : 0.04
[          : 0.03
... (other tokens)</code></pre>
<p>Without constraints, this leads to common JSON errors like unquoted numbers (<code>{"name": 42}</code>), incomplete objects (<code>{"name": {"age"}</code>), or unquoted text (<code>{"name": Alice}</code>).</p>
<p>Constrained generation on the other hand applies token masking, transforming those probabilities to:</p>
<pre><code>"          : 1.0  (only valid option to start a string)
42         : 0.0  (masked - would create invalid JSON)
{          : 0.0  (masked - would create invalid JSON)
... (all other tokens masked)</code></pre>
<p>This isn’t making the model smarter - it’s enforcing structural guarantees through probability masking. The impact? When generating thousands of records, even a 99.9% success rate means consistent failures at scale. Constrained generation doesn’t improve those odds - it makes structural errors mathematically impossible.</p>
</section>
<section id="the-punctuation-tax-a-cfos-nightmare" class="level2">
<h2 class="anchored" data-anchor-id="the-punctuation-tax-a-cfos-nightmare">The Punctuation Tax: A CFO’s Nightmare</h2>
<p>Let me translate the impact of all of this into plain CFO-speak:</p>
<ul>
<li>Zero invalid outputs. Not “fewer.” Zero. It is mathematically impossible to generate invalid output</li>
<li>40-60% lower API costs - no more retries</li>
<li>30-50% additional savings by skipping predictable tokens</li>
<li>No more “ai-output-broken” emergency Slack channels</li>
</ul>
<p>Let’s be blunt: you’re paying AI model prices for punctuation marks. Every time your LLM generates a JSON structure, you’re burning tokens on braces and commas. That’s like hiring a McKinsey consultant to type semicolons. Here’s what a typical JSON response looks like:</p>
<div class="sourceCode" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode json code-with-copy"><code class="sourceCode json"><span id="cb4-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">{</span></span>
<span id="cb4-2">  <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">"apiVersion"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"v1"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb4-3">  <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">"metadata"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">:</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">{</span></span>
<span id="cb4-4">    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">"timestamp"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"2024-10-29T12:00:00Z"</span></span>
<span id="cb4-5">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">}</span></span>
<span id="cb4-6"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">}</span></span></code></pre></div>
<p>This boilerplate alone is 20-30 tokens of pure syntax. At $0.01 per 1K tokens:</p>
<ul>
<li>100,000 daily API calls</li>
<li>25 tokens of boilerplate each</li>
<li>That’s $9,125 annually just for punctuation</li>
</ul>
<p>For a company spending $100K monthly on AI calls, up to $60K goes to generating predictable tokens. Most structured data is 50-65% syntax:</p>
<ul>
<li>JSON: ~60% structural tokens</li>
<li>XML: ~65% structural tokens</li>
<li>SQL: ~50% structural tokens</li>
</ul>
<blockquote class="blockquote">
<p>🔮 <strong>Future Impact:</strong> This matters even more for autonomous AI workflows. Today, when AI systems chain operations together, each step needs extensive validation and error handling. With constrained generation, we shift from catching errors at runtime to preventing them entirely - like catching type errors in development instead of production. For businesses building autonomous systems, structural reliability isn’t just improved - it’s guaranteed. Your AI agents can’t generate structurally invalid queries or corrupt data structures so long as they can be defined by a grammar because they’re mathematically incapable of breaking those rules.</p>
</blockquote>
</section>
<section id="putting-it-all-together-a-path-to-predictable-ai" class="level2">
<h2 class="anchored" data-anchor-id="putting-it-all-together-a-path-to-predictable-ai">Putting It All Together: A Path to Predictable AI</h2>
<p>I’ve thrown a lot of technical detail and business math at you. But here’s what I really want you to take away: constrained generation isn’t optional anymore. It’s table stakes for any business serious about deploying AI in production.</p>
<p>Think about it this way: We don’t debate whether to use HTTPS anymore. We don’t have meetings about whether to validate user input. We don’t write blog posts weighing the pros and cons of using version control. These are just part of what we call “engineering.” Constrained generation is on the same trajectory – it’s rapidly moving from “interesting technique” to “standard practice.”</p>
<p>Remember those numbers we walked through earlier? Let’s put them in perspective: - You’re either paying a “punctuation tax” of 40-70% on your API calls, or you’re not - Your engineering team is either firefighting format errors, or they’re building features - Your AI systems are either probabilistically reliable, or they’re mathematically guaranteed</p>
<p>This isn’t about optimization anymore – it’s about basic engineering competence. When you’re processing millions of structured outputs, “mostly correct” isn’t a standard, it’s a liability. Every CTO I know who has implemented constrained generation has the same reaction: “I can’t believe we used to do this any other way.”</p>
<p>The pattern in software engineering is always the same: first we make something possible, then we make it reliable, then we make it efficient. We’re watching this play out with AI in real-time. The companies that get ahead of this curve aren’t just going to save money – they’re going to be the ones whose AI initiatives succeed while their competitors are still debugging edge cases.</p>
</section>
<section id="getting-started-with-constrained-generation" class="level2">
<h2 class="anchored" data-anchor-id="getting-started-with-constrained-generation">Getting Started with Constrained Generation</h2>
<p>For the engineers in your life (or the engineering-curious), there are several frameworks that make constrained generation accessible - some more nascent than others:</p>
<ul>
<li><p><strong>Guidance</strong>: Microsoft Research’s powerful framework that focuses on interleaving control flow with generation. It allows you to write pure Python code with LLM-specific extensions, making it feel natural for developers.</p></li>
<li><p><strong>Outlines</strong>: A lean, efficient library focused purely on structured generation to make AI speak a language that computers will understand.</p></li>
<li><p><strong>Formatron</strong>: The new performance-focused entrant, with an emphasis on efficient parsing and generation. It implements constraints using the Earley algorithm in Rust, making it both theoretically optimal and practically fast.</p></li>
</ul>
<p>These frameworks are very much under active development and evolving fast. I am sure each of them will carve a niche for itself.</p>
<p>In a future post I plan to create a video that will help engineers get set up with a solution quickly. If you’re an engineer (or work with engineers), you’ll want to bookmark these two – it’s going to be the kind of post that saves weeks of trial and error.</p>
<hr>
<p><em>Found this valuable? Share it with your friends! Subscribe to ensure you don’t miss it – your future self will thank you when your AI systems are running smoothly at 3 AM instead of generating support tickets.</em></p>
<iframe src="../../subscribe.html" width="600" height="400" class="newsletter-form">
</iframe>


</section>

 ]]></description>
  <category>Large Language Models</category>
  <category>AI Engineering</category>
  <category>Production AI</category>
  <category>Constrained Generation</category>
  <category>Cost Optimization</category>
  <guid>https://shwetank-kumar.github.io/posts/constrained-generation/</guid>
  <pubDate>Tue, 29 Oct 2024 07:00:00 GMT</pubDate>
</item>
<item>
  <title>🌙 AI Afterhours: Top AI Papers for Oct 18 - Oct 24, 2024</title>
  <dc:creator>Shwetank Kumar</dc:creator>
  <link>https://shwetank-kumar.github.io/posts/summary-2024-10-25/ai_paper_summaries.html</link>
  <description><![CDATA[ 





<p>Welcome to this week’s AI Afterhours! Your weekly digest of most upvoted papers in AI. Below is gist of the results, how they got them, and why you should care. With that, let’s dive into the most exciting AI research from the past week.</p>
<p>If you would prefer to listen instead of read here is a NotebookLM generated summary:</p>
<iframe src="https://podcasters.spotify.com/pod/show/shwetankkumar/embed" height="200px" width="400px" frameborder="0" scrolling="no">
</iframe>
<iframe src="../../subscribe.html" width="600" height="400" class="newsletter-form">
</iframe>
<p><strong>FrugalNeRF</strong> tackles one of the most pressing challenges in Neural Radiance Fields - the need for faster, more efficient scene reconstruction from limited viewpoints. By introducing a clever weight-sharing scheme across multiple voxel scales and a cross-scale geometric adaptation mechanism, the team achieved remarkable efficiency gains. The results speak for themselves: high-quality novel view synthesis in just 10 minutes on the LLFF dataset and 6 minutes on the DTU dataset, with superior PSNR, SSIM, and LPIPS scores compared to existing methods. This breakthrough could revolutionize applications from virtual reality to architectural visualization, where quick turnaround times from limited input data are crucial.</p>
<p><a href="https://arxiv.org/pdf/2410.16271v1">arXiv:2410.16271v1</a> 👍57</p>
<p><strong>CompassJudger-1</strong> represents a significant step forward in how we evaluate large language models. This comprehensive judge model, trained on 900,000 entries of diverse data, achieves an impressive 95% accuracy rate on the JDB-B benchmark. What makes this particularly interesting is its optimal training data ratio discovery: 1:3:1 for critique data, reward data, and general SFT data respectively. The implications for AI development are substantial - we’re moving towards more reliable, consistent evaluation methods that could accelerate the improvement cycle of language models.</p>
<p><a href="https://arxiv.org/pdf/2410.16256v1">arXiv:2410.16256v1</a> 👍51</p>
<p><strong>Movie Gen</strong> is pushing the boundaries of media generation with an impressive suite of foundation models. The system shows remarkable capabilities in text-to-video synthesis and editing, backed by solid numbers: a 35.02% net win rate over previous work in overall quality and 48.49% in realness. With its largest model boasting 30B parameters and capable of generating 16-second videos at 16 fps, it’s a significant leap forward. The implications for creative industries are enormous - from rapid prototyping in film production to democratizing video content creation.</p>
<p><a href="https://arxiv.org/pdf/2410.13720v1">arXiv:2410.13720v1</a> 👍50</p>
<p><strong>MixEval-X</strong> introduces a fresh approach to evaluating multi-modal AI models using real-world data mixtures. The framework demonstrates impressive correlation with real-world evaluations - 98.1% Spearman correlation with Vision Arena and 96.3% with Arena (Vision) for Image2Text tasks. What’s particularly noteworthy is its adaptation-rectification pipeline, showing a 0.75 correlation between model judges and human preference Elo. This could fundamentally change how we benchmark AI systems, providing more realistic and reliable evaluation metrics.</p>
<p><a href="https://arxiv.org/pdf/2410.13754v2">arXiv:2410.13754v2</a> 👍50</p>
<p><strong>SAM2Long</strong> addresses a critical limitation in video segmentation with a clever memory tree approach. The results are compelling: an average improvement of 3.0 points in J&amp;F score across six VOS benchmarks, with particularly impressive gains of 5.3 points on the challenging SA-V test set. The beauty of this solution lies in its training-free nature - it requires no additional parameters or training, making it immediately applicable to existing SAM 2 implementations. This could be a game-changer for long-form video analysis and editing applications.</p>
<p><a href="https://arxiv.org/pdf/2410.16268v1">arXiv:2410.16268v1</a> 👍46</p>
<p><strong>PUMA</strong> brings a fresh perspective to image generation with its multi-granular approach. The model achieves impressive CLIP scores - 0.736 for CLIP-I and 0.317 for CLIP-T in text-to-image generation, while hitting 0.846 and 0.270 respectively in image editing tasks. This balance between coarse and fine-grained control opens up new possibilities in creative applications, from detailed image editing to nuanced artistic generation.</p>
<p><a href="https://arxiv.org/pdf/2410.13861v2">arXiv:2410.13861v2</a> 👍41</p>
<p><strong>SemiEvol</strong> presents an innovative approach to adapting large language models with limited labeled data. The framework demonstrates up to 83.3% error reduction compared to traditional fine-tuning methods across seven datasets. Particularly impressive is its performance in specialized fields like Law, Engineering, and Philosophy, where it achieves over 55% improvement after just four iterations. This could be a breakthrough for organizations looking to customize LLMs for specific domains without extensive labeled datasets.</p>
<p><a href="https://arxiv.org/pdf/2410.14745v1">arXiv:2410.14745v1</a> 👍38</p>
<p><strong>AutoTrain</strong> is democratizing machine learning by offering a no-code solution for training state-of-the-art models. Supporting 22 different tasks (16 text-based, 4 image-based, and 2 tabular-based), it makes sophisticated model training accessible through both GUI and CLI interfaces. While it currently has some limitations around sample weights and model ensembling, its potential impact on democratizing AI development is significant.</p>
<p><a href="https://arxiv.org/pdf/2410.15735v1">arXiv:2410.15735v1</a> 👍36</p>
<p><strong>UCFE</strong> introduces a comprehensive benchmark for evaluating LLMs’ financial expertise. With 330 data points covering multi-round dialogues and a strong 0.78 Pearson correlation coefficient with human preferences, it provides a robust framework for assessing AI capabilities in finance. This could be crucial for financial institutions evaluating AI adoption, potentially accelerating the responsible integration of AI in financial services.</p>
<p><a href="https://arxiv.org/pdf/2410.14059v2">arXiv:2410.14059v2</a> 👍36</p>
<p><strong>Baichuan Alignment</strong> presents impressive results in optimizing large language models, achieving user experience improvements of 17-28%. The three-stage approach combining Prompt Augmentation, Supervised Fine-Tuning, and Preference Alignment shows particular strength on challenging tasks. These gains could significantly impact the practical deployment of LLMs in real-world applications.</p>
<p><a href="https://arxiv.org/pdf/2410.14940v1">arXiv:2410.14940v1</a> 👍32</p>
<p>That’s a wrap for this week’s AI Afterhours! If you enjoyed reading this or listening to it please susbscribe for weekly updates.</p>



 ]]></description>
  <category>Video Object Segmentation</category>
  <category>Multi-Modal AI</category>
  <category>PyTorch</category>
  <category>Neural Radiance Fields</category>
  <category>Text-to-Video Synthesis</category>
  <category>Large Language Models</category>
  <category>Multi-Granular Visual Generation</category>
  <guid>https://shwetank-kumar.github.io/posts/summary-2024-10-25/ai_paper_summaries.html</guid>
  <pubDate>Thu, 24 Oct 2024 07:00:00 GMT</pubDate>
</item>
<item>
  <title>🌙 AI Afterhours: Top AI Papers for Oct 11 - Oct 17, 2024</title>
  <dc:creator>Shwetank Kumar</dc:creator>
  <link>https://shwetank-kumar.github.io/posts/summary-2024-10-17/ai_paper_summaries.html</link>
  <description><![CDATA[ 





<p>Welcome to this week’s AI Afterhours! Your weekly digest of most upvoted papers in AI. Below is gist of the results, how they got them, and why you should care. With that, let’s dive into the most exciting AI research from October 11 to October 17, 2024.</p>
<iframe src="../../subscribe.html" width="600" height="400" class="newsletter-form">
</iframe>
<p><strong>The Baichuan-Omni model, detailed in a new technical report, is pushing the boundaries of multimodal large language models.</strong> This powerhouse can process text, images, videos, and audio, achieving state-of-the-art performance across various benchmarks. We’re talking about significant leaps here - it outperformed VITA by 25.6% on the CMMLU benchmark. The secret sauce? A comprehensive pipeline including multimodal alignment pre-training and supervised fine-tuning. This could revolutionize applications like multimodal dialogue systems and content generation. If you’re working on anything that requires understanding and generating content across different modalities, Baichuan-Omni is definitely one to watch.</p>
<p><a href="https://arxiv.org/pdf/2410.08565v1">arXiv:2410.08565v1</a> 👍52</p>
<p><strong>LOKI is raising the bar as a comprehensive synthetic data detection benchmark.</strong> It goes beyond simple authenticity checks, introducing coarse-grained judgment, multiple-choice questions, and fine-grained anomaly selection tasks. This allows for a more nuanced analysis of large multimodal models (LMMs) across video, image, 3D, text, and audio modalities. The results? Even the best models, like GPT-4o, are only scratching the surface with an overall accuracy of 63.9% for judgment questions. As AI synthesis technologies rapidly advance, LOKI provides a crucial framework for developing more powerful and interpretable synthetic data detection methods. If you’re working on synthetic data detection or developing LMMs, LOKI offers a robust testbed for improving your models.</p>
<p><a href="https://arxiv.org/pdf/2410.09732v1">arXiv:2410.09732v1</a> 👍47</p>
<p><strong>The MMIE benchmark is setting a new standard for evaluating large vision-language models (LVLMs) in understanding and generating interleaved text and images.</strong> With 20,103 queries across 12 fields, it’s a comprehensive test. The researchers also introduced a model-powered metric that aligns closely with human evaluation. The results? Even the best LVLMs have room for improvement, with integrated approaches outperforming previous open-source models by an average of 25.2% across all categories. This benchmark is crucial for advancing LVLMs in fields like education, healthcare, and finance. If you’re developing multimodal models, MMIE provides a thorough evaluation framework to refine your work.</p>
<p><a href="https://arxiv.org/pdf/2410.10139v1">arXiv:2410.10139v1</a> 👍43</p>
<p><strong>VidEgoThink is tackling the challenge of evaluating egocentric video understanding capabilities in Embodied AI.</strong> This comprehensive benchmark assesses four critical functions: video question-answering, hierarchy planning, visual grounding, and reward modeling. The results are eye-opening - all Multi-modal Large Language Models (MLLMs) performed poorly across all tasks, with the best average accuracy of just 32.82% in video question-answering. Interestingly, GPT-4o performed better with 8 frames than with 32 frames. This benchmark is crucial for developing intelligent robots and agents that can navigate and interact with their environment. If you’re working on Embodied AI or MLLMs, VidEgoThink provides valuable insights into current limitations and areas for improvement.</p>
<p><a href="https://arxiv.org/pdf/2410.11623v1">arXiv:2410.11623v1</a> 👍39</p>
<p><strong>Meissonic is revitalizing masked generative transformers for efficient high-resolution text-to-image synthesis.</strong> This approach introduces a novel masked image modeling (MIM) method that combines multi-modal and single-modal layers, advanced positional encoding strategies, and an adaptive masking rate. The results are impressive - Meissonic achieves competitive performance with state-of-the-art diffusion models like SDXL, but with only 1B parameters and running on consumer-grade GPUs with 8GB VRAM. It even outperforms models like DALL-E 2 and SDXL in the Human Preference Score v2 (HPSv2), scoring 29.57. If you’re interested in text-to-image synthesis, especially for applications in art, design, or entertainment, Meissonic’s efficiency and quality make it a model to watch.</p>
<p><a href="https://arxiv.org/pdf/2410.08261v1">arXiv:2410.08261v1</a> 👍35</p>
<p><strong>VIF-RAG is pushing the boundaries of instruction-following alignment in Retrieval-Augmented Generation (RAG) systems.</strong> This novel automated data synthesis framework integrates a verification process at each step of data augmentation and combination, ensuring high-quality instruction data. The results speak for themselves - VIF-RAG outperforms all baselines by over 10% on average accuracy in the FollowRAG benchmark. It’s not just a marginal improvement; VIF-RAG achieves an average IF score of 51.6% and an average RAG score of 56.5% on FollowRAG. If you’re working on RAG systems for applications like text summarization, question answering, or language translation, VIF-RAG’s approach could significantly enhance your model’s accuracy and effectiveness.</p>
<p><a href="https://arxiv.org/pdf/2410.09584v1">arXiv:2410.09584v1</a> 👍34</p>
<p><strong>MathCoder2 is taking a novel approach to enhance mathematical reasoning abilities in large language models.</strong> The researchers created a large-scale pretraining dataset called MathCode-Pile, which includes mathematical reasoning steps translated into Python code snippets. The results are promising - MathCoder2-Llama-3-8B achieves 4-shot accuracies of 38.4% on MATH and 69.9% on GSM8K, outperforming the baseline by 3.1% and 4.1%, respectively. This approach has significant implications for improving mathematical reasoning in AI, with potential applications in education, scientific research, and problem-solving. If you’re working on enhancing the mathematical capabilities of language models, MathCoder2’s method could provide valuable insights.</p>
<p><a href="https://arxiv.org/pdf/2410.08196v1">arXiv:2410.08196v1</a> 👍27</p>
<p><strong>Animate-X is introducing a universal character image animation approach with enhanced motion representation.</strong> This novel method leverages a latent diffusion model with a 3D-UNet architecture and introduces the Pose Indicator, consisting of Implicit Pose Indicator (IPI) and Explicit Pose Indicator (EPI). The results are impressive, with Animate-X outperforming state-of-the-art methods in terms of identity preservation and motion consistency. On the new Animated Anthropomorphic Benchmark (A2Bench), it achieves a PSNR* of 13.60 and SSIM of 0.452. This approach has significant implications for character animation in gaming, virtual reality, and cinematic production. If you’re working on character animation or motion representation, Animate-X’s techniques could provide valuable insights for creating more realistic and dynamic characters.</p>
<p><a href="https://arxiv.org/pdf/2410.10306v1">arXiv:2410.10306v1</a> 👍26</p>
<p><strong>PrefixQuant is introducing a novel technique for static activation quantization in Large Language Models (LLMs).</strong> This approach addresses the issue of token-wise outliers by prefixing specific tokens in the KV cache to isolate outliers. The results are impressive - PrefixQuant achieves a 7.43 WikiText2 perplexity and 71.08% average accuracy on 5 common-sense reasoning tasks in W4A4KV4 Llama-3-8B, outperforming previous per-token dynamic quantization methods. Moreover, the inference speed of W4A4 quantized models using PrefixQuant is 1.60× to 2.81× faster than FP16 models. This has significant implications for improving the performance and reducing inference time in various LLMs, including Llama-2, Llama-3, and Mistral-7B-v0.3. If you’re working on optimizing LLMs, particularly in terms of quantization and inference speed, PrefixQuant’s approach could provide valuable insights.</p>
<p><a href="https://arxiv.org/pdf/2410.05265v1">arXiv:2410.05265v1</a> 👍25</p>
<p>That’s a wrap for this week’s AI Afterhours!</p>



 ]]></description>
  <category>Retrieval-Augmented Generation</category>
  <category>Latent Diffusion Model</category>
  <category>Vision-Language Models</category>
  <category>Synthetic Data Detection</category>
  <category>Multimodal_Language_Models</category>
  <category>Embodied AI</category>
  <category>Large Language Models</category>
  <category>Text-to-Image Synthesis</category>
  <category>Natural Language Processing</category>
  <guid>https://shwetank-kumar.github.io/posts/summary-2024-10-17/ai_paper_summaries.html</guid>
  <pubDate>Thu, 17 Oct 2024 07:00:00 GMT</pubDate>
</item>
<item>
  <title>🌙 AI Afterhours: Top AI Papers for Oct 04 - Oct 10, 2024</title>
  <dc:creator>Shwetank Kumar</dc:creator>
  <link>https://shwetank-kumar.github.io/posts/summary-2024-10-10/ai_paper_summaries.html</link>
  <description><![CDATA[ 





<p>Welcome to this week’s AI Afterhours! Your weekly digest of most upvoted papers in AI. Below is gist of the results, how they got them, and why you should care. With that, let’s dive into the most exciting AI research from October 04 to October 10, 2024.</p>
<p><strong>“Addition is All You Need for Energy-efficient Language Models”</strong> tackles a crucial challenge in AI: reducing energy consumption in large neural networks. The researchers have developed a clever algorithm called Linear-Complexity Multiplication (L-Mul) that approximates floating-point multiplication using integer addition. Now, why should you care? Well, this approach achieved a whopping 95% reduction in energy cost for element-wise floating-point tensor multiplications and an 80% reduction for dot products. That’s not just a small improvement; it’s a game-changer for making AI more sustainable. The results are impressive, with L-Mul achieving 52.92% accuracy on the GSM8k benchmark and comparable performance to more energy-intensive methods on other tasks. As we push towards larger and more complex AI models, innovations like this are crucial for keeping our carbon footprint in check.</p>
<p><a href="https://arxiv.org/pdf/2410.00907v2">arXiv:2410.00907v2</a> 👍67</p>
<p>Next up, we have <strong>“GLEE: A Unified Framework and Benchmark for Language-based Economic Environments”</strong>. This paper introduces a fascinating framework for evaluating how Large Language Models (LLMs) perform in economic games compared to humans. The researchers collected a massive dataset of 954K games played between LLMs and 3,405 games with human players. What’s intriguing is how LLMs and humans performed differently across various game types. For instance, humans outperformed LLMs in bargaining games when playing as Alice, but not as Bob. In negotiation games, LLMs had the upper hand, while humans held their own in persuasion games. This research opens up new avenues for understanding AI behavior in complex social and economic scenarios, which is crucial as we integrate AI into more aspects of our daily lives and decision-making processes.</p>
<p><a href="https://arxiv.org/pdf/2410.05254v1">arXiv:2410.05254v1</a> 👍63</p>
<p>Ever wished for an AI assistant that truly gets you? <strong>“Personalized Visual Instruction Tuning”</strong> might be bringing us closer to that reality. This paper introduces a novel training framework called PVIT, which enables multimodal large language models (MLLMs) to conduct personalized conversations targeting specific individuals. The results are impressive, with the PVIT-tuned MLLM achieving accuracy rates of 95.42% to 99.43% for multiple-choice questions and 82.55% to 100% for descriptive questions in person recognition tasks. This technology could revolutionize fields like personalized therapy, visual assistants, and domestic robots. Imagine an AI that can truly understand and respond to your unique needs and preferences - that’s the potential impact of this work.</p>
<p><a href="https://arxiv.org/pdf/2410.07113v1">arXiv:2410.07113v1</a> 👍57</p>
<p>In <strong>“Revisit Large-Scale Image-Caption Data in Pre-training Multimodal Foundation Models”</strong>, researchers delve into the nitty-gritty of pre-training multimodal models. They’ve found that a hybrid approach combining synthetic captions and web-crawled AltText achieves better performance than using synthetic captions alone. The optimal mix? About 40-50% synthetic captions for CLIP models. This might sound technical, but it’s crucial for improving the performance of AI systems that work with both images and text. Better pre-training means more accurate and versatile AI models for tasks like image recognition, content moderation, and even generating images from text descriptions.</p>
<p><a href="https://arxiv.org/pdf/2410.02740v1">arXiv:2410.02740v1</a> 👍40</p>
<p><strong>“Aria: An Open Multimodal Native Mixture-of-Experts Model”</strong> introduces a powerful new player in the AI field. ARIA is an open-source model that can process and integrate diverse real-world information from text, code, images, and videos. What’s impressive is its performance - it outperforms proprietary models like GPT-4o and Gemini-1.5 on various multimodal tasks. For instance, ARIA achieves 92.6% on the DocVQA benchmark and 80.3% on the MMBench-1.1 benchmark. This is a big deal because it brings state-of-the-art multimodal AI capabilities to the open-source community, potentially accelerating research and development in this field.</p>
<p><a href="https://arxiv.org/pdf/2410.05993v1">arXiv:2410.05993v1</a> 👍39</p>
<p>If you’ve ever been frustrated by AI-generated images that just don’t quite get the composition right, <strong>“IterComp: Iterative Composition-Aware Feedback Learning from Model Gallery for Text-to-Image Generation”</strong> might be the solution you’ve been waiting for. This framework significantly outperforms previous methods in compositional generation, achieving a CLIP Score of 0.5554, Aesthetic Score of 5.936, and ImageReward of 1.437. What’s more, it’s fast, with an average inference time of just 5.63 seconds per image. This could be a game-changer for fields like digital art, advertising, and even educational content creation, where precise control over image composition is crucial.</p>
<p><a href="https://arxiv.org/pdf/2410.07171v1">arXiv:2410.07171v1</a> 👍38</p>
<p><strong>“Pixtral 12B”</strong> presents a new multimodal language model that’s punching above its weight class. Despite being 7x smaller than some competitors, it outperforms larger models like Llama-3.2 90B on various multimodal benchmarks. It even goes toe-to-toe with some closed models like Claude-3 Haiku and Gemini-1.5 Flash 8B. On the MM-MT-Bench benchmark, Pixtral 12B scores an impressive 92.5. This is exciting because it shows we can achieve state-of-the-art performance with smaller, more efficient models, potentially making advanced AI capabilities more accessible and easier to deploy.</p>
<p><a href="https://arxiv.org/pdf/2410.07073v1">arXiv:2410.07073v1</a> 👍36</p>
<p>Ever wondered how we can make AI-generated videos more realistic? <strong>“Towards World Simulator: Crafting Physical Commonsense-Based Benchmark for Video Generation”</strong> tackles this challenge head-on. The researchers introduce PhyGenBench, a benchmark with 160 prompts across 27 physical laws, and PhyGenEval, a novel evaluation framework. Their findings show current text-to-video models achieve a PCA score of only 0.51 on PhyGenBench, highlighting a significant gap in physical commonsense understanding. This research is crucial for developing more realistic world simulators and improving the quality of AI-generated videos, which could have far-reaching implications for industries like gaming, film, and virtual reality.</p>
<p><a href="https://arxiv.org/pdf/2410.05363v1">arXiv:2410.05363v1</a> 👍33</p>
<p>Lastly, <strong>“Deciphering Cross-Modal Alignment in Large Vision-Language Models with Modality Integration Rate”</strong> introduces a new metric called Modality Integration Rate (MIR) for evaluating how well vision-language models align different types of information. MIR shows a strong positive correlation (0.85) with model performance after fine-tuning. This metric could be a game-changer for developing and improving multimodal AI systems, potentially leading to more accurate and versatile models for tasks like image captioning and visual question answering.</p>
<p><a href="https://arxiv.org/pdf/2410.07167v1">arXiv:2410.07167v1</a> 👍31</p>
<p>That’s a wrap for this week’s AI Afterhours!</p>



 ]]></description>
  <category>Neural Networks</category>
  <category>Vision-Language Models</category>
  <category>Text-to-Image Generation</category>
  <category>Multimodal Large Language Models</category>
  <category>Mixture-of-Experts</category>
  <category>Text-to-Video Models</category>
  <category>Multimodal Foundation Models</category>
  <category>Large Language Models</category>
  <category>Multimodal Language Models</category>
  <guid>https://shwetank-kumar.github.io/posts/summary-2024-10-10/ai_paper_summaries.html</guid>
  <pubDate>Thu, 10 Oct 2024 07:00:00 GMT</pubDate>
</item>
<item>
  <title>🌙 AI Afterhours: Top AI Papers for Sep 30 - Oct 02, 2024</title>
  <dc:creator>Shwetank Kumar</dc:creator>
  <link>https://shwetank-kumar.github.io/posts/summary-2024-10-03/ai_paper_summaries.html</link>
  <description><![CDATA[ 





<p>Welcome to this week’s AI Afterhours! Your weekly digest of most upvoted papers in AI. Below is gist of the results, how they got them, and why you should care. With that, let’s dive into the most exciting AI research from September 30 to October 02, 2024.</p>
<p><strong>“Emu3” demonstrates that next-token prediction is all you need</strong> for achieving state-of-the-art performance in multimodal generation and perception tasks. By tokenizing images, text, and videos into a discrete space and training a single transformer from scratch, Emu3 outperforms established task-specific models. It achieves a zero-shot FID score of 0.43 and a CLIP-T score of 0.67 on various benchmarks. In video generation, it produces high-fidelity results with a PSNR of 22.69 and SSIM of 0.690. This approach suggests that next-token prediction could be a powerful paradigm for developing versatile multimodal models, potentially revolutionizing multimedia AI applications.</p>
<p><a href="https://arxiv.org/pdf/2409.18869v1">arXiv:2409.18869v1</a> 👍47</p>
<p><strong>“Molmo and PixMo” introduce open weights and open data for state-of-the-art multimodal models</strong>, addressing the challenge of building advanced vision-language models (VLMs) without relying on proprietary systems. The Molmo family outperforms other open-weight models on academic benchmarks, with the 72B model ranking second in human preference evaluations. Notably, it achieves 88.7% low-level accuracy and 69.0% high-level accuracy on the AndroidControl benchmark. This work provides a foundation for developing more transparent and open AI systems, potentially democratizing access to powerful multimodal models.</p>
<p><a href="https://arxiv.org/pdf/2409.17146v1">arXiv:2409.17146v1</a> 👍41</p>
<p><strong>“Programming Every Example” (PROX) lifts pre-training data quality like experts at scale</strong>, improving large language model (LLM) performance while reducing computing power. The PROX framework uses language models to refine pre-training data, resulting in over 2% improvement on various downstream benchmarks. Models trained on PROX-curated tokens yield significant improvements with 20× fewer tokens. The additional computational overhead is equivalent to training an extra 12B tokens on TLM-S and 5B tokens on TLM-M. This approach could be a game-changer for LLM development, making training more efficient and potentially reducing the environmental impact of AI research.</p>
<p><a href="https://arxiv.org/pdf/2409.17115v1">arXiv:2409.17115v1</a> 👍36</p>
<p><strong>The “Law of the Weakest Link” study reveals crucial insights into cross capabilities of Large Language Models</strong> (LLMs). Using the CrossEval benchmark, comprising 1,400 prompts and 8,400 human ratings, the research shows that LLMs’ cross-capability performance is significantly constrained by their weakest component. Out of 58 cross-capability scores from 17 models, 38 are lower than all individual capabilities. This finding has profound implications for LLM development, suggesting that focusing on improving weaker capabilities could lead to significant gains in cross-capability tasks, potentially revolutionizing how we approach AI model enhancement.</p>
<p><a href="https://arxiv.org/pdf/2409.19951v2">arXiv:2409.19951v2</a> 👍34</p>
<p><strong>“MIO” introduces a foundation model on multimodal tokens</strong>, integrating text, image, video, and speech modalities. Trained using a four-stage process, MIO demonstrates competitive performance compared to previous dual-modal and modality-specific baselines. It shows a 10% improvement in image captioning and 15% in visual question answering. MIO also exhibits advanced capabilities like interleaved video-text generation and chain-of-visual-thought reasoning. While it has some limitations with OCR-related images and video generation, MIO represents a significant step towards more versatile and capable AI systems that can seamlessly work across multiple modalities.</p>
<p><a href="https://arxiv.org/pdf/2409.17692v1">arXiv:2409.17692v1</a> 👍32</p>
<p><strong>“HelloBench” provides a comprehensive evaluation of long text generation capabilities in Large Language Models</strong> (LLMs). The study reveals that current LLMs struggle with generating text longer than 4000 words, often producing degraded quality for longer outputs. The proposed HelloEval method achieves a correlation of around 30 with human evaluation, highlighting the limitations of LLM-as-a-Judge approaches. The research identifies several error modes in long text generation, including repetition and meaningless content. These findings underscore the need for improved long-form text generation in LLMs, which could significantly impact applications like content creation and summarization.</p>
<p><a href="https://arxiv.org/pdf/2409.16191v1">arXiv:2409.16191v1</a> 👍28</p>
<p><strong>“MaskLLM” introduces learnable semi-structured sparsity for Large Language Models</strong>, addressing the challenge of reducing computational overhead while maintaining performance. Using Gumbel Softmax sampling, MaskLLM achieves superior results compared to state-of-the-art methods, with a Wikitext perplexity of 6.72 on LLaMA-2 7B, outperforming SparseGPT’s 10.42. The method effectively scales to large datasets and accelerates training through transfer learning with pre-computed masks. This approach could significantly enhance the efficiency of LLMs, making them more practical for real-world AI applications where computational resources are a constraint.</p>
<p><a href="https://arxiv.org/pdf/2409.17481v1">arXiv:2409.17481v1</a> 👍25</p>
<p><strong>“RATIONALYST” pre-trains process-supervision for improving reasoning</strong> in Large Language Models (LLMs). By training on implicit rationales extracted from unlabeled data, RATIONALYST improves reasoning accuracy by an average of 3.9% across seven representative benchmarks. It outperforms other verifiers, including GPT-4, and demonstrates that implicit supervision surpasses explicit supervision, with a 2.6% improvement on ECQA and 4.0% on GSM8K. This approach could significantly enhance LLMs’ reasoning capabilities, potentially leading to more reliable AI systems for complex decision-making tasks.</p>
<p><a href="https://arxiv.org/pdf/2410.01044v1">arXiv:2410.01044v1</a> 👍23</p>
<p><strong>“RACER” introduces rich language-guided failure recovery policies for imitation learning</strong> in robotic manipulation. By using a vision-language model as a supervisor and a language-conditioned visuomotor policy as an actor, RACER achieves an average success rate of 70.2% on 18 RLbench tasks, outperforming the state-of-the-art RVT by 7.3%. The use of rich instructions improves performance by 2% compared to simple instructions. This approach could significantly enhance the robustness and adaptability of robotic systems, particularly in complex and dynamic environments where failure recovery is crucial.</p>
<p><a href="https://arxiv.org/pdf/2409.14674v1">arXiv:2409.14674v1</a> 👍22</p>
<p><strong>“PHI-S” introduces distribution balancing for label-free multi-teacher distillation</strong>, addressing the challenge of balancing teacher distributions in knowledge distillation. Using a Hadamard matrix to standardize teacher outputs, PHI-S outperforms other normalization methods, achieving mean squared errors of 4.7200, 4.9010, 0.8865, and 8.3330 for DFN CLIP, SigLIP, DINOv2, and SAM, respectively. The PHI-S-RADIO-B and PHI-S-RADIO-L models reach classification accuracies of 73.16 and 80.45 on ImageNet-1K. This method could significantly improve multi-teacher knowledge distillation, benefiting various applications in computer vision and beyond.</p>
<p><a href="https://arxiv.org/pdf/2410.01680v1">arXiv:2410.01680v1</a> 👍21</p>
<p><strong>“LLaVA-3D” offers a simple yet effective pathway to empowering Large Multimodal Models (LMMs) with 3D-awareness</strong>. By introducing the 3D Patch representation, which augments 2D patch-wise features with 3D positional embeddings, LLaVA-3D achieves state-of-the-art performance on various 3D tasks. It reaches 91.7% accuracy on ScanQA, 79.21% CIDEr on Scan2Cap, and 54.1% accuracy on ScanRefer. Notably, it converges 3.5× faster than existing 3D LMMs. This approach could significantly advance the development of generalist models capable of handling both 2D and 3D tasks, potentially revolutionizing fields like robotics and augmented reality.</p>
<p><a href="https://arxiv.org/pdf/2409.18125v1">arXiv:2409.18125v1</a> 👍21</p>
<p><strong>“MIMO” enables controllable character video synthesis with spatial decomposed modeling</strong>, addressing the challenge of generating realistic videos with controllable attributes. Using a spatial decomposed diffusion model, MIMO achieves 95% accuracy in reconstructing input videos, with 90% of generated characters showing high similarity to input images. It can generate videos with novel 3D motions at 85% accuracy and interactive scenes at 90% accuracy. This technology could have far-reaching implications for entertainment, education, and advertising, offering new ways to create personalized and interactive video content.</p>
<p><a href="https://arxiv.org/pdf/2409.16160v1">arXiv:2409.16160v1</a> 👍21</p>
<p>And that’s a wrap! See you next week!</p>



 ]]></description>
  <category>Multimodal Learning</category>
  <category>Imitation Learning</category>
  <category>MIMO</category>
  <category>Natural Language Processing</category>
  <category>Multimodal Generation</category>
  <category>Knowledge Distillation</category>
  <category>3D Computer Vision</category>
  <category>Large Language Models</category>
  <category>Multimodal Models</category>
  <guid>https://shwetank-kumar.github.io/posts/summary-2024-10-03/ai_paper_summaries.html</guid>
  <pubDate>Mon, 30 Sep 2024 07:00:00 GMT</pubDate>
</item>
<item>
  <title>Llama Stack: Meta’s Impressive Stride in the AI Framework Marathon</title>
  <dc:creator>Shwetank Kumar</dc:creator>
  <link>https://shwetank-kumar.github.io/posts/llama-stack/</link>
  <description><![CDATA[ 





<p>Ready to level up from AI novice to silicon symphony conductor? Meta’s latest brainchild, Llama Stack, is here to make that happen! This freshly minted framework isn’t just another dev tool – it’s a full-fledged ecosystem designed to streamline AI model development and deployment from start to finish.</p>
<p>Why should you care? In the AI arms race, Llama Stack is your secret weapon. It’s Meta’s answer to the growing complexity of AI development, offering a standardized, efficient approach that could revolutionize how we create and deploy AI solutions. From fine-tuning language models to seamless production integration, Llama Stack is your express ticket to AI innovation.</p>
<p>Curious to see it in action? Feast your eyes on this video guide and prepare for your ascension to AI greatness - we’re not just installing software here, we’re igniting a revolution!</p>
<br><br>
<div class="video-container">
<p><iframe width="560" height="315" src="https://www.youtube.com/embed/b39eWyfjPzo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen=""></iframe></p>
</div>
<p><br><br></p>
<section id="what-is-llama-stack" class="level2">
<h2 class="anchored" data-anchor-id="what-is-llama-stack">What is Llama Stack?</h2>
<p>Llama Stack is Meta’s comprehensive framework for developing generative AI applications. It’s not just a stack; it’s a skyscraper of possibilities reaching into the cloud(s). This multi-layered, API-standardized, fine-tuning-friendly behemoth streamlines the entire AI development process, from model training to production deployment.</p>
</section>
<section id="key-components-of-llama-stack" class="level2">
<h2 class="anchored" data-anchor-id="key-components-of-llama-stack">Key Components of Llama Stack</h2>
<ol type="1">
<li><strong>Inference API:</strong> Predict the future (results may vary)</li>
<li><strong>Safety API:</strong> Because with great power comes great responsibility</li>
<li><strong>Memory API:</strong> Bend data to your will like a silicon sorcerer</li>
<li><strong>Agentic System API:</strong> Orchestrate AI agents with ease</li>
<li><strong>Telemetry API:</strong> Achieve AI enlightenment through deep insights</li>
</ol>
</section>
<section id="what-youll-learn-in-the-video" class="level2">
<h2 class="anchored" data-anchor-id="what-youll-learn-in-the-video">What You’ll Learn in the Video</h2>
<ol type="1">
<li>How to deploy Llama Stack effectively, avoiding common pitfalls like a seasoned AI navigator</li>
<li>The art of testing your AI creation using nothing but a browser - no magic wand required!</li>
<li>Exploring mind-bending use cases with an IPython Notebook, turning code into AI gold</li>
<li>Deploying AI solutions faster than you can say “Is this magic?”</li>
</ol>
</section>
<section id="getting-started-with-llama-stack" class="level2">
<h2 class="anchored" data-anchor-id="getting-started-with-llama-stack">Getting Started with Llama Stack</h2>
<p>The video guide will walk you through:</p>
<ol type="1">
<li><strong>Installing the Llama CLI:</strong> Your Swiss Army knife for all things Llama Stack</li>
<li><strong>Downloading Models:</strong> Choose from a variety of Llama models</li>
<li><strong>Building and Configuring Distributions:</strong> Customize your AI stack</li>
<li><strong>Running Your Llama Stack Server:</strong> Bring your AI creation to life</li>
<li><strong>Testing with Clients:</strong> Witness the power of your newly created AI system</li>
</ol>
</section>
<section id="embrace-the-llama-be-the-llama" class="level2">
<h2 class="anchored" data-anchor-id="embrace-the-llama-be-the-llama">Embrace the Llama, be the Llama</h2>
<p>Llama Stack represents a significant step forward in making AI development more accessible and efficient. For businesses looking to leverage AI, it’s a powerful ally. For developers, it’s an opportunity to elevate your skills and streamline your workflow.</p>
<p>After all, in the world of tech, sometimes you need to spit in the face of convention to make progress. And what better way to do that than with a framework named after a creature known for its spitting prowess? 🦙💻</p>
<p>By the end of our video, you’ll be slinging APIs, fine-tuning models, and deploying AI solutions with the casual ease of a seasoned pro. Your colleagues will whisper your name in awe, and your LinkedIn profile might just spontaneously combust from the sheer radiance of your newly acquired skills. So, are you ready to evolve from a developer into a digital deity? Watch the video above, and let’s turn that Llama into a unicorn!</p>


</section>

 ]]></description>
  <category>genai</category>
  <category>inference</category>
  <guid>https://shwetank-kumar.github.io/posts/llama-stack/</guid>
  <pubDate>Fri, 27 Sep 2024 07:00:00 GMT</pubDate>
  <media:content url="https://shwetank-kumar.github.io/posts/llama-stack/llama-stack.jpeg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Less Magic, More Math: Why Inference Scaling is the New Black</title>
  <dc:creator>Shwetank Kumar</dc:creator>
  <link>https://shwetank-kumar.github.io/posts/inference-scaling/</link>
  <description><![CDATA[ 





<p>Ah, the sweet smell of progress in the morning! OpenAI has just dropped their shiny new o1-mini and o1 models, and the internet is abuzz with hot takes faster than you can say “AGI winter is coming.” You’ve probably seen the YouTube videos: wide-eyed tech bros marveling at the models’ ability to solve differential equations or explain quantum mechanics to their goldfish.</p>
<p>But let’s be real: you’re not here for another “OMG, AI can [insert mundane task]!” video. No, you, dear reader, are a person of substance. You crave the meaty, first-principles concepts that will help you navigate the choppy waters of AI progress without drowning in a sea of hype. You’re tired of every Tom, Dick, and Elon barging into the AI field like a bull in a china shop, asking everyone to consider their groundbreaking idea of a <a href="https://en.wikipedia.org/wiki/Spherical_cow">spherically symmetric cow in n-dimensional Hilbert space.</a></p>
<p>That’s why we’re diving into inference scaling today. It’s a crucial concept that’s driving the impressive performance of these new models, and understanding it is key to grasping the current state of AI technology.</p>
<p>Now, if you’re here to debate whether we’re on the cusp of AGI or if these models are secretly plotting to overthrow humanity and replace us with more efficient toasters, I’m going to have to ask you to see yourself out. There are plenty of Twitter threads and subreddits where you can indulge in that particular flavor of speculation. We’re here for the nitty-gritty, the nuts and bolts, the “how does this actually work?” of it all.</p>
<p>So, strap in, silence your phone (unless you’re reading this on it, in which case, carry on), and let’s embark on a journey into the heart of inference scaling. It’s time to learn why OpenAI is betting big on this approach, and why you should care. Welcome to the bleeding edge of AI, where the cows are spherical, the Hilbert spaces are infinite, and the potential for groundbreaking insights is limitless.</p>
<blockquote class="blockquote">
<p><strong>Disclaimer:</strong> Before we dive in, I just want to be super clear: what follows is my best understanding of inference scaling based on publicly available information. As for what’s really going on inside OpenAI’s secret volcano lair? Well, your guess is as good as mine. They’re about as forthcoming with their methods as a cat is with its tax returns.</p>
</blockquote>
<blockquote class="blockquote">
<p><strong>Navigation Prompt:</strong> If you are familiar with different LLM layers, how they work and what roles they accomplish you can skip this section otherwise please read on.</p>
</blockquote>
<section id="the-assembly-line-of-sentences-how-language-models-work-infer" class="level1">
<h1>The Assembly Line of Sentences: How Language Models Work (Infer)</h1>
<p>Now that we’ve cleared the air of AGI speculation and spherical bovines, let’s delve into the intricacies underpinning inference scaling. Note that this is not only about throwing more FLOPs at the problem. To understand how inference scaling works we need to first understand how basic inference works.</p>
<p>Imagine, for a moment, that you’re running a factory. Not just any factory, mind you, but one that produces bespoke, artisanal sentences on demand. Your raw materials? Vectors of floating-point numbers. Your end product? Everything from Shakespearean sonnets to Python code.</p>
<p>Welcome to the world of Large Language Model inferencing. It’s a bit like running a just-in-time manufacturing operation, except instead of assembling cars, you’re assembling words. And let me tell you, the logistics are <em>fascinating</em>.</p>
<p>Let’s break down this process, shall we? It all starts with a prompt. Think of this as the order form for your sentence factory. “I need a limerick about database optimization, stat!” your customer (who is suspiciously often yourself) demands. Your factory springs into action:</p>
<ol type="1">
<li><p><strong>The Tokenizer</strong>: This is your receiving department. It breaks down the incoming order into bite-sized pieces the rest of the factory can work with. “Database” might become “data” and “base”, while “optimization” could be “optim” and “ization”. It’s like those old “FRAGILE: HANDLE WITH CARE” stamps, except here it’s “LANGUAGE: HANDLE WITH VECTORS”. The tokenizer layer is the gatekeeper of language models, transforming raw text into a format the model can understand. Think of it as a translator that converts human-readable text into a sequence of numbers (tokens) that the AI can process.</p>
<pre><code>                          Tokenize(text) = [token_1, token_2, ..., token_n]</code></pre>
<p>where <img src="https://latex.codecogs.com/png.latex?token_i%20=%20vocabulary_%7Bindex%7D(subword_i)">. Algorithmically, the process works in 3 sub-steps:</p>
<ul>
<li><strong>Text Segmentation:</strong> The input text is broken down into subwords, words, or characters, depending on the tokenization strategy.</li>
<li><strong>Vocabulary Lookup:</strong> Each subword is mapped to a unique integer index in the model’s vocabulary.</li>
<li><strong>Token Generation:</strong> The sequence of these indices forms the final token sequence.</li>
</ul></li>
<li><p><strong>The Embedding Layer</strong>: This is where the magic of turning words into numbers happens. It’s as if each word is run through a very complex, very mathy Instagram filter. “Data” doesn’t just mean “data” anymore; it’s now a list of 768 floating-point numbers that somehow capture the essence of “data-ness”. Imagine the embedding layer as a massive, multidimensional dictionary or lookup table. Here’s is its structure:</p>
<ul>
<li><p><strong>Rows:</strong> Each row in this table corresponds to a token in your vocabulary. If your model has a vocabulary of 50,000 words, your table has 50,000 rows.</p></li>
<li><p><strong>Columns:</strong> Each column represents a dimension of the embedding. If you’re using 300-dimensional embeddings, your table has 300 columns.</p></li>
<li><p><strong>Cells:</strong> Each cell in this table contains a single number, typically a floating-point value.</p>
<pre><code>                           Embedding(token_id) = LookupTable[token_id]</code></pre></li>
</ul>
<p>where LookupTable is a matrix of shape (vocab_size, embedding_dim)</p></li>
<li><p><strong>The Transformer Layers</strong>: These layers have been covered very well in multiple posts all over the net, specifically look at [3] for more detail. This is the real engine of your factory. Imagine a room full of savants, each one looking at your partial sentence, conferring with other savants based on information they have, and then scribbling notes about what word should come next. Now imagine doing this dozens of times in parallel while sharing information. That’s what’s happening here, except the savants are matrix multiplications and the notes are more vectors. There are other parts to the layer but at the heart of of this communcation and compute is the attention layer which can be specified by Key, Value, Query triplets (K,Q,V) as follows: <img src="https://latex.codecogs.com/png.latex?%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20Attention(Q,%20K,%20V)%20=%20softmax((QK%5ET)%20/%20%E2%88%9Ad_k)%20V%0A"></p>
<p>where, Key represents information that each of them has, Query is each one asking - hey who has this information that I need and Value is the information that they share with each other once they identify the overlap between whats needed and what they have based on the dot product between K and Q.</p></li>
<li><p><strong>The Output Layer</strong>: This is your quality control department. It takes all those scribbled savant notes and turns them into actual probabilities for each word in your vocabulary. “The” might get a 2% chance, “database” a 0.5% chance, and “aardvark” a 0.0001% chance. (Hey, you never know when you might need an aardvark in your limerick about databases).</p>
<p>This is embodied by our Softmax layer. It takes the raw scores (often called logits) associated with each possible token in the vocabulary and transforms them into a probability distribution. Essentially, for each position in a sequence, the softmax layer calculates the likelihood of each token in the vocabulary appearing in that position. It does this by exponentiating the scores and then normalizing them so that they sum to 1. This process ensures that the model outputs a valid probability distribution over all possible tokens, allowing it to make predictions about the most likely next token in a sequence or to classify tokens into different categories. Mathematically, you can write it as: <img src="https://latex.codecogs.com/png.latex?%0Asoftmax(x_i)%20=%20%5Cfrac%7B%5Cexp(x_i)%7D%7B%5Csum_j%20%5Cexp(x_j)%7D%0A"></p>
<p>where i is the current token and j iterates over all tokens in the vocabulary.</p></li>
<li><p><strong>Token Selection and Iteration</strong>: This is where your factory’s assembly line completes its cycle by choosing the next token based on Softmax output, transforming probability distributions into the next token in the sequence. This step is also where you decide how wild you want your factory’s output to be. e.g.&nbsp;Greedy search tends to produce more repetitive text, while sampling methods can lead to more varied and sometimes surprising results*.</p>
<p>Finally, we get a sequence of tokens that we convert into readable text using the inverse transform from look up tables in Step 1. i.e.&nbsp;if you were inputting rows and reading off columns as the output, here you input columns and read off rows and vice-versa.</p></li>
</ol>
<p>By repeating this process, your AI factory can churn out everything from simple sentences to complex essays - Or entire essay about why PostgreSQL is actually a misunderstood performance art piece. (Don’t judge. GPT-4o has some weird hobbies.) How you generate tokens, iterate, and post process in this last step is what defines how “thoughtful” your LLM sounds. Lets dig into that in the next section.</p>
<blockquote class="blockquote">
<p><strong>Navigation Prompt:</strong> Details of algorithms in Step 5 is what inference scaling is all about. If you are familiar with various token generation strategies skip ahead otherwise please read on.</p>
</blockquote>
<section id="the-fundamentals-of-token-selection-navigating-vast-probability-spaces" class="level2">
<h2 class="anchored" data-anchor-id="the-fundamentals-of-token-selection-navigating-vast-probability-spaces">The Fundamentals of Token Selection: Navigating Vast Probability Spaces</h2>
<p>At its core, inference in a Large Language Model (LLM) is about predicting the next token in a sequence, given the previous tokens. It’s like playing a cosmic game of “what comes next?” where the stakes are coherent communication. LLMs accomplish this feat by learning the underlying distribution of the training data and storing a compressed version of it in their parameters. However, this nominally simple task hides a universe of complexity that would make even the most seasoned computer scientists break out in a cold sweat.</p>
<section id="the-vastness-of-possibility" class="level3">
<h3 class="anchored" data-anchor-id="the-vastness-of-possibility">The Vastness of Possibility</h3>
<p>Imagine you’re an LLM with a vocabulary of |V| tokens, trying to generate a sequence of length T. The naive search space for this task is O(|V|^T). To put this into perspective, if you have a modest vocabulary of 50,000 tokens (which is on the small side for modern LLMs) and you’re generating a sequence of just 20 tokens, you’re looking at 50,000^20 possibilities.</p>
<p>That’s a number so large it makes the number of atoms in the observable universe look like pocket change. If each possibility were a grain of sand, you’d have enough to build a beach that stretches from here to Alpha Centauri, with enough left over to fill the Mariana Trench. Twice.</p>
<p>Clearly, brute-forcing our way through this cosmic beach of possibilities isn’t feasible, even with the most powerful supercomputers at our disposal. We need a smarter approach.</p>
</section>
<section id="enter-the-decoding-strategies" class="level3">
<h3 class="anchored" data-anchor-id="enter-the-decoding-strategies">Enter the Decoding Strategies</h3>
<p>So given these insurmountable odds, how do LLMs figure out what token to generate next? Enter the world of decoding strategies! Depending on the strategies used and the choice of parameters used to tune them you might get text that is coherent, engaging, surprising or a combination there of. Just remember it will always be sampled from the underlying probability distribution that was learned during the training phase.</p>
<p>Let’s dive into some of these strategies which we will need to understand inference scaling. There are many more which are well covered in [2] and similar articles.</p>
<section id="beam-search-the-chess-grandmaster" class="level4">
<h4 class="anchored" data-anchor-id="beam-search-the-chess-grandmaster">1. Beam Search: The Chess Grandmaster</h4>
<p>Beam search is like a chess grandmaster, always thinking several moves ahead to find the best overall strategy. It explores multiple possible sequences simultaneously, keeping track of the most promising paths.</p>
<p>At each step, beam search maintains a set number (let’s call it <code>k</code>) of the most likely partial sequences, known as hypotheses. This allows the model to consider sequences that may start with lower probability tokens but have higher overall probability, potentially leading to better quality outputs than simple greedy search.</p>
<p>The beauty of beam search is that it strikes a balance between exploration and exploitation. It’s not just picking the best immediate option, but considering how that choice might pan out in the long run. The final output is the sequence with the highest overall probability.</p>
<div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Beam Search Example</span></span>
<span id="cb3-2">checkpoint <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"openai-community/gpt2-medium"</span></span>
<span id="cb3-3">tokenizer <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> AutoTokenizer.from_pretrained(checkpoint)</span>
<span id="cb3-4">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> AutoModelForCausalLM.from_pretrained(checkpoint)</span>
<span id="cb3-5"></span>
<span id="cb3-6">inputs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tokenizer(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"The cat sat on the"</span>, return_tensors<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"pt"</span>)</span>
<span id="cb3-7">outputs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model.generate(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span>inputs, num_beams<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, max_new_tokens<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>)</span>
<span id="cb3-8"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(tokenizer.decode(outputs[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>], skip_special_tokens<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>))</span></code></pre></div>
<p>Importantly, traditional beam search with a width k reduces our astronomical search space from O(|V|^T) to O(k|V|T). That’s a significant improvement, though still a hefty computational load for real-time applications!</p>
</section>
<section id="multinomial-sampling-the-creative-writer" class="level4">
<h4 class="anchored" data-anchor-id="multinomial-sampling-the-creative-writer">2. Multinomial Sampling: The Creative Writer</h4>
<p>If beam search is our chess grandmaster, multinomial sampling is the free-spirited writer who throws caution to the wind and lets inspiration guide their pen.</p>
<p>This method introduces an element of controlled randomness into the text generation process. Unlike greedy search, which always picks the most probable token, multinomial sampling randomly selects the next token based on the probability distribution provided by the model. It’s like rolling a weighted die, where each face represents a possible next word, and the size of each face corresponds to its probability.</p>
<p>Why is this important? It allows for more diverse outputs. Every token with a non-zero probability has a chance of being selected, reducing the risk of bland, repetitive text. It’s how AI can surprise us with creative turns of phrase or unexpected (yet coherent) continuations of a prompt.</p>
<div class="sourceCode" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Multinomial Sampling Example</span></span>
<span id="cb4-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> transformers <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> AutoTokenizer, AutoModelForCausalLM</span>
<span id="cb4-3"></span>
<span id="cb4-4">checkpoint <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"openai-community/gpt2-large"</span></span>
<span id="cb4-5">tokenizer <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> AutoTokenizer.from_pretrained(checkpoint)</span>
<span id="cb4-6">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> AutoModelForCausalLM.from_pretrained(checkpoint)</span>
<span id="cb4-7"></span>
<span id="cb4-8">inputs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tokenizer(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"The cat sat on the"</span>, return_tensors<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"pt"</span>)</span>
<span id="cb4-9">outputs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model.generate(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span>inputs, do_sample<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, num_beams<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, max_new_tokens<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>)</span>
<span id="cb4-10"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(tokenizer.decode(outputs[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>], skip_special_tokens<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>))</span></code></pre></div>
</section>
<section id="beam-search-multinomial-sampling-the-best-of-both-worlds" class="level4">
<h4 class="anchored" data-anchor-id="beam-search-multinomial-sampling-the-best-of-both-worlds">3. Beam Search Multinomial Sampling: The Best of Both Worlds</h4>
<p>What if we could combine the strategic foresight of beam search with the creative spark of multinomial sampling? Enter beam search multinomial sampling, the hybrid approach that aims to give us the best of both worlds.</p>
<p>This method maintains multiple hypotheses like beam search, but instead of always choosing the most probable token for each beam, it samples from the probability distribution. It’s as if our chess player occasionally makes a slightly unorthodox move, not because it’s objectively the best, but because it might lead to an interesting game state.</p>
<p>The result? Outputs that are both high-quality and diverse, striking a delicate balance between coherence and creativity.</p>
<div class="sourceCode" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Beam Search Multinomial Sampling Example</span></span>
<span id="cb5-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> transformers <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> AutoModelForSeq2SeqLM</span>
<span id="cb5-3"></span>
<span id="cb5-4">checkpoint <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"google-t5/t5-small"</span></span>
<span id="cb5-5">tokenizer <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> AutoTokenizer.from_pretrained(checkpoint)</span>
<span id="cb5-6">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> AutoModelForSeq2SeqLM.from_pretrained(checkpoint)</span>
<span id="cb5-7"></span>
<span id="cb5-8">inputs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tokenizer(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"translate English to German: The cat sat on the mat."</span>, return_tensors<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"pt"</span>)</span>
<span id="cb5-9">outputs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model.generate(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span>inputs, num_beams<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, do_sample<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, max_new_tokens<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>)</span>
<span id="cb5-10"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(tokenizer.decode(outputs[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>], skip_special_tokens<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>))</span></code></pre></div>
</section>
<section id="diverse-beam-search-the-brainstorming-session" class="level4">
<h4 class="anchored" data-anchor-id="diverse-beam-search-the-brainstorming-session">4. Diverse Beam Search: The Brainstorming Session</h4>
<p>Sometimes, we don’t just want one good output – we want several distinctly different options. That’s where diverse beam search comes in. Think of it as a brainstorming session where participants are explicitly told to come up with ideas that are different from each other.</p>
<p>Diverse beam search divides the beams into groups and applies a diversity penalty to ensure that the outputs from different groups are distinct. Within each group, standard beam search is applied. This approach is particularly useful when you want to generate multiple alternative outputs that are significantly different from each other, not just minor variations.</p>
<div class="sourceCode" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Diverse Beam Search Example</span></span>
<span id="cb6-2">checkpoint <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"google/pegasus-xsum"</span></span>
<span id="cb6-3">tokenizer <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> AutoTokenizer.from_pretrained(checkpoint)</span>
<span id="cb6-4">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> AutoModelForSeq2SeqLM.from_pretrained(checkpoint)</span>
<span id="cb6-5"></span>
<span id="cb6-6">inputs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tokenizer(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Summarize: The cat sat on the mat. The dog slept by the fire. The bird sang in the tree."</span>, return_tensors<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"pt"</span>)</span>
<span id="cb6-7">outputs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model.generate(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span>inputs, num_beams<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, num_beam_groups<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, diversity_penalty<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.0</span>, max_new_tokens<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">30</span>)</span>
<span id="cb6-8"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(tokenizer.decode(outputs[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>], skip_special_tokens<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>))</span></code></pre></div>
</section>
</section>
<section id="the-art-of-balancing" class="level3">
<h3 class="anchored" data-anchor-id="the-art-of-balancing">The Art of Balancing</h3>
<p>Each of these decoding strategies offers different trade-offs between output quality, diversity, &amp; computational cost. The choice of strategy depends on the specific requirements of your task and the desired characteristics of the generated text. For example by playing with just a few parameters you can go from Greedy algorithm with extremely repetitive responses to versatile text giving human like responses.</p>
<p>Understanding these strategies is crucial for anyone working with or developing language models. They’re not just technical details – they’re the brushstrokes that determine the final picture painted by our AI artists and to emulate reflection and thoughtfulness by your friendly neighborhood AI which is what we cover in the next section.</p>
</section>
</section>
<section id="in-ais-deep-reflection-where-probability-flirts-with-search-and-models-judge-other-models-pickup-lines" class="level2">
<h2 class="anchored" data-anchor-id="in-ais-deep-reflection-where-probability-flirts-with-search-and-models-judge-other-models-pickup-lines">In AI’s Deep Reflection: Where Probability Flirts with Search and Models Judge Other Models’ Pickup Lines</h2>
<p>For years, the AI community has been on a bulk-up routine that would make bodybuilders jealous - just feed the model more parameters and data. Inference scaling changes this up by giving AI models a gym membership and a personal trainer instead. It asks the question - instead of always making our models bigger, how do we make them think harder? Let’s dive into the three main approaches for that with a bit more technical detail this time.</p>
<section id="parallel-sampling-the-brute-force-charmer" class="level3">
<h3 class="anchored" data-anchor-id="parallel-sampling-the-brute-force-charmer">1. Parallel Sampling: The Brute Force Charmer</h3>
<p>Parallel sampling is a straightforward yet powerful technique in the world of inference scaling. At its core, it’s about generating multiple independent solutions and selecting the best one.</p>
<p><strong>How It Works:</strong></p>
<p>When presented with an input, the AI model doesn’t generate just one answer. Instead, it produces N complete, independent answers. This process is akin to running the model N times in parallel, each time generating a full response to the input.</p>
<p>The key to this method’s effectiveness lies in its evaluation step. We employ a sophisticated evaluator, an Outcome Reward Model (ORM). This is an AI model, trained to assess the quality of generated answers based on various criteria. The ORM examines each of the N answers and assigns them a score. This scoring isn’t just based on correctness, but can include factors such as the quality of reasoning, clarity of explanation, and adherence to the task requirements.</p>
<p>Once all N answers have been scored, we can use a “best-of-N weighted” selection method to choose the final answer. This method is more nuanced than simply selecting the highest-scoring response. Instead, it considers all answers that arrived at the same conclusion and sums their scores. The conclusion with the highest total score is then selected as the final output.</p>
<p><strong>Mathematical Formulation:</strong></p>
<p>For a given input x:</p>
<ol type="1">
<li>Generate outputs: y₁, y₂, …, yₙ</li>
<li>Score each output: s₁ = PRM(y₁), s₂ = PRM(y₂), …, sₙ = PRM(yₙ)</li>
<li>Final selection: y* = argmax(s₁, s₂, …, sₙ)</li>
</ol>
<p>In practice, the selection process might be more complex, involving grouping similar outputs and summing their scores before selecting the highest-scoring group.</p>
<p><strong>Strengths:</strong></p>
<ol type="1">
<li>Simple and scalable. Improving results often involves simply increasing N, effectively leveraging additional computational resources to boost performance - a clear trade-off.</li>
<li>Works well for easier questions or tasks where generating a large number of attempts is likely to produce at least one high-quality answer.</li>
<li>Works especially when the space of possible good answers is relatively large and diverse.</li>
</ol>
<p><strong>Weaknesses:</strong></p>
<ol type="1">
<li>Computationally inefficient for complex problems.</li>
<li>While it will eventually find a good solution if N is large enough, it may use significantly more resources than more targeted approaches.</li>
<li>The compute costs can become prohibitive if N is set too high, especially for resource-intensive models.</li>
<li>Each generation is independent, potentially repeating work or mistakes made in other attempts. It doesn’t learn from or build upon partial successes within a single attempt.</li>
</ol>
<p>All-in-all it is a simple but brute-force approach to inference scaling. Its straightforward nature makes it an attractive option, especially when dealing with tasks where the generation of multiple diverse attempts is likely to yield at least one high-quality result. However, its effectiveness needs to be balanced against its potential computational costs, especially for more complex tasks or when scaling to very large N.</p>
</section>
<section id="beam-search-the-chess-grandmaster-of-inference" class="level3">
<h3 class="anchored" data-anchor-id="beam-search-the-chess-grandmaster-of-inference">2. Beam Search: The Chess Grandmaster of Inference</h3>
<p>Unlike parallel sampling, which generates complete independent answers, beam search constructs solutions incrementally, making decisions at each step about which partial solutions are most promising to develop further.</p>
<p><strong>How It Works:</strong></p>
<ol type="1">
<li>Initialization: The process begins by generating N initial predictions for the first step of the solution. These represent different starting points for the answer.</li>
<li>Scoring: A Process Reward Model (PRM) is used to score each of these N initial steps. The PRM evaluates the quality and potential of each partial solution. Note that this is also how it is different from an ORM above.</li>
<li>Pruning: Instead of keeping all N initial steps, beam search retains only the top N/M highest-scoring ones, where M is the beam width. This step focuses the search on the most promising partial solutions.</li>
<li>Expansion: For each of the retained partial solutions, the model generates M new proposals for the next step. This brings the total number of candidates back to N (N/M * M = N).</li>
<li>Iteration: Steps 2-4 are repeated until either a complete solution is reached or a maximum number of rounds is hit.</li>
</ol>
<p>This process allows beam search to maintain a diverse set of partially completed solutions, continually evaluating and refining them as it progresses.</p>
<p><strong>Mathematical Formulation:</strong></p>
<p>At each step t:</p>
<ol type="1">
<li>Generate candidates: C_t = {c₁, c₂, …, cₙ}</li>
<li>Score candidates: S_t = {PRM(c₁), PRM(c₂), …, PRM(cₙ)}</li>
<li>Keep top k: B_t = top_k(C_t, S_t, k=N/M)</li>
<li>Expand: C_{t+1} = ⋃_{b ∈ B_t} expand(b, M)</li>
</ol>
<p>where:</p>
<ul>
<li>C_t is the set of candidate partial solutions at step t</li>
<li>S_t is the set of scores for these candidates</li>
<li>B_t is the set of top-scoring candidates retained</li>
<li>expand(b, M) generates M new candidates from partial solution b</li>
</ul>
<p><strong>Strengths:</strong></p>
<ol type="1">
<li>Complex problem-solving where the solution is built incrementally.</li>
<li>Natural language generation tasks requiring coherent, long-form responses.</li>
<li>Multi-step planning or decision-making processes.</li>
</ol>
<p><strong>Weaknesses:</strong></p>
<ol type="1">
<li>There is potential for pruning prematurely. By discarding lower-scoring partial solutions early, beam search might miss unconventional but ultimately superior solutions that don’t appear promising in their early stages.</li>
<li>While more efficient than exhaustive search, beam search can still be computationally intensive, especially with large beam widths or for problems requiring many steps.</li>
<li>Its performance can be highly dependent on the choice of N and M. Optimal values may vary significantly between different types of problems.</li>
</ol>
<p>Beam search is particularly suited for problems where the solution quality depends on a series of interconnected decisions. Its ability to maintain and explore multiple promising solution paths makes it a go-to approach for complex, multi-step reasoning tasks in AI.</p>
</section>
<section id="revision-based-approach-the-perfectionists-dream" class="level3">
<h3 class="anchored" data-anchor-id="revision-based-approach-the-perfectionists-dream">3. Revision-Based Approach: The Perfectionist’s Dream</h3>
<p>This approach is focuses on iterative improvement of an initial response. Unlike parallel sampling or beam search, which generate multiple independent answers or explore multiple paths simultaneously, this approach generates a single answer and then repeatedly refines it.</p>
<p><strong>How It Works:</strong></p>
<ol type="1">
<li>Initial Generation: The model produces an initial answer to the given input.</li>
<li>Iterative Refinement: The model is then presented with its previous answer(s) along with the original input and asked to generate an improved version.</li>
<li>Repetition: This process of refinement is repeated for a predetermined number of iterations or until some convergence criterion is met.</li>
<li>Selection: Once all iterations are complete, the best version is selected using either an Outcome Reward Model (ORM) or a majority voting mechanism.</li>
</ol>
<p><strong>Mathematical Formulation:</strong></p>
<pre><code>1. Initial generation: y₀ = model(x)
2. for i = 1 to k:
      yᵢ = model(x, y₀, y₁, ..., yᵢ₋₁)
3. Final selection:
      y* = argmax(ORM(y₀), ORM(y₁), ..., ORM(yₖ))
      or
      y* = mode(y₀, y₁, ..., yₖ)</code></pre>
<p>Where:</p>
<ul>
<li>x is the input</li>
<li>yᵢ is the i-th revision</li>
<li>k is the total number of revisions</li>
<li>ORM is the Outcome Reward Model</li>
</ul>
<p><strong>Key Components:</strong></p>
<ol type="1">
<li>Revision Model: This is typically a fine-tuned version of the base language model, specifically trained to improve upon previous answers. The training process involves exposing the model to sequences of increasingly better answers to the same question.</li>
<li>Outcome Reward Model (ORM): This is a separate model trained to evaluate the quality of complete answers. It’s used to score each revision and select the best one.</li>
<li>Majority Voting: An alternative to ORM, this method selects the most common answer among all revisions. It’s particularly useful when multiple independent revision sequences are generated.</li>
</ol>
<p><strong>Strengths:</strong></p>
<ol type="1">
<li>This approach allows for gradual refinement of the answer, potentially leading to high-quality results, especially for medium-difficulty questions.</li>
<li>It can be more computationally efficient than generating multiple complete answers, as in parallel sampling.</li>
<li>By considering previous attempts, the model can build upon its own insights and correct its mistakes.</li>
</ol>
<p><strong>Weaknesses:</strong></p>
<ol type="1">
<li>This approach may get stuck in a local optimum, repeatedly making similar refinements without considering radically different approaches.</li>
<li>The quality of the final output can be heavily influenced by the quality of the initial answer.</li>
<li>Training a model to effectively revise its own work is a complex task, requiring sophisticated training data and techniques.</li>
</ol>
<p>Revision-based approach represents a unique strategy in inference scaling, mimicking a facet of the human creative process of drafting and refining ideas. As I have mentioned elsewhere that I don’t think that’s the whole story as far as human creativity is concerned. However, the ability to iteratively improve upon its own output makes for a powerful tool in the AI toolkit, especially for tasks where quality can be enhanced through careful refinement and consideration of previous attempts.</p>
</section>
</section>
<section id="conclusion-the-art-of-thinking-harder-not-just-bigger" class="level2">
<h2 class="anchored" data-anchor-id="conclusion-the-art-of-thinking-harder-not-just-bigger">Conclusion: The Art of Thinking Harder, Not Just Bigger</h2>
<p>This post has become way longer than originally intended so thanks for staying with it. For more details on performance metrics, and specific use-cases for each technique, I highly recommend checking out reference [3] below.</p>
<p>As I wrap up this journey, it’s clear to me that we are amid a paradigm shift (no not AGI). We’re moving from an era of “bigger is better” during training to a more nuanced approach that folds in clever inference algorithms as well in which ORM and PRM models are the unsung heroes. Their quality will set the upper bound on the quality of your inference scaling strategy.</p>
<p>And while these techniques will enable us to use copilots capable of more nuanced approaches to the problem this shift also means what earlier used to be a heavy training related capital expense borne by the model provider will turn more into an operational expense borne by the user.</p>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<ol type="1">
<li><a href="https://jalammar.github.io/illustrated-transformer/">The illustrated transformer</a></li>
<li><a href="https://huggingface.co/docs/transformers/en/generation_strategies">Text generation strategies</a></li>
<li><a href="https://arxiv.org/pdf/2408.03314">Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters</a></li>
</ol>


</section>
</section>

 ]]></description>
  <category>genai</category>
  <category>inference</category>
  <guid>https://shwetank-kumar.github.io/posts/inference-scaling/</guid>
  <pubDate>Mon, 16 Sep 2024 07:00:00 GMT</pubDate>
  <media:content url="https://shwetank-kumar.github.io/posts/inference-scaling/robot.png" medium="image" type="image/png" height="144" width="144"/>
</item>
<item>
  <title>🌙 AI Afterhours: Top AI Papers for Sep 06 - Sep 12, 2024</title>
  <dc:creator>Shwetank Kumar</dc:creator>
  <link>https://shwetank-kumar.github.io/posts/summary-2024-09-12/ai_paper_summaries.html</link>
  <description><![CDATA[ 





<p>Here’s a summary of the AI papers from last week, formatted as requested for your blog post:</p>
<hr>
<section id="ai-afterhours-top-ai-papers-for-sep-06---sep-12-2024" class="level1">
<h1>🌙 AI Afterhours: Top AI Papers for Sep 06 - Sep 12, 2024</h1>
<p>Welcome to this week’s AI Afterhours! Let’s dive into the most exciting AI research from September 06 to September 12, 2024.</p>
<p><strong>“Guide-and-Rescale” introduces a self-guidance mechanism for effective tuning-free real image editing</strong>, tackling the challenge of balancing editing quality with preserving original image structure. By leveraging a modified diffusion sampling process and introducing layout-preserving energy functions, the method achieves impressive results. With a CLIP score of 0.243, LPIPS score of 0.228, and FID score of 39.07, it outperforms existing approaches. User studies show a strong preference for Guide-and-Rescale, with 85% favoring its editing quality and 70% its preservation quality. This breakthrough could revolutionize image editing, offering a fast, high-quality solution that maintains the essence of the original image.</p>
<p><a href="https://arxiv.org/pdf/2409.01322v3">arXiv:2409.01322v3</a> 👍71</p>
<p><strong>The “Attention Heads of Large Language Models” survey provides a comprehensive framework for understanding the internal mechanisms of LLMs</strong>, specifically focusing on attention heads and their role in reasoning. By proposing a four-stage framework that draws parallels between human cognition and LLMs, the authors categorize existing research into Knowledge Recalling, In-Context Identification, Latent Reasoning, and Expression Preparation stages. The survey identifies 17, 14, 15, and 5 special attention heads in each stage respectively. This work is crucial for improving LLM interpretability and enhancing problem-solving capabilities, potentially leading to more advanced and transparent AI systems.</p>
<p><a href="https://arxiv.org/pdf/2409.03752v2">arXiv:2409.03752v2</a> 👍54</p>
<p><strong>“Towards a Unified View of Preference Learning for Large Language Models” presents a comprehensive survey on aligning LLMs with human preferences</strong>. The authors propose a unified framework decomposing existing methods into model, data, feedback, and algorithm components. They categorize preference learning algorithms into four groups and highlight the importance of high-quality preference data and reliable feedback. Key findings include the use of 20K comparisons in the Webgpt dataset and 170K chats in the HH-RLHF dataset. This work is essential for developing more accurate and informative LLMs, addressing the critical challenge of AI alignment.</p>
<p><a href="https://arxiv.org/pdf/2409.02795v3">arXiv:2409.02795v3</a> 👍48</p>
<p><strong>“PingPong” introduces a benchmark for evaluating role-playing capabilities of language models</strong> using a novel approach that leverages LMs to emulate users in dynamic, multi-turn conversations. The benchmark consists of player, interrogator, and judge models. Experiments show strong correlations between automated evaluations and human annotations, with Spearman correlations ranging from 0.3 to 0.64. Claude 3.5 Sonnet emerged as the best model in both English and Russian, while Llama 3.1 70B and Gemma 2 Ataraxy 9B were the top open models for English and Russian respectively. This benchmark provides a robust foundation for evaluating LMs in interactive scenarios, crucial for advancing conversational AI.</p>
<p><a href="https://arxiv.org/pdf/2409.06820v1">arXiv:2409.06820v1</a> 👍46</p>
<p><strong>“FuzzCoder” proposes a novel approach to improve fuzz testing by leveraging large language models</strong> to predict mutation positions and strategies. By formulating fuzzing as a sequence-to-sequence paradigm and using fine-tuned LLMs, FuzzCoder significantly improves the effective proportion of mutation (EPM) and number of crashes (NC) compared to previous baselines. EPM values range from 2.10 to 19.51, while NC values span from 1 to 224. This innovative approach has the potential to revolutionize software vulnerability detection, leading to more secure software development and reduced risk of security breaches.</p>
<p><a href="https://arxiv.org/pdf/2409.01944v1">arXiv:2409.01944v1</a> 👍34</p>
<p><strong>The “MEDIC” framework provides a comprehensive approach to evaluating LLMs in clinical applications</strong>, addressing the limitations of previous methods by assessing models across five critical dimensions: Medical reasoning, Ethical and bias concerns, Data and language understanding, In-context learning, and Clinical safety and risk assessment. The framework reveals that larger models excel in closed-ended tasks but not necessarily in open-ended clinical Q&amp;A. Notably, the Med42-Llama3.1-70b model achieved a high score of 97.9 in the Clinical Trials dataset for safety and ethical considerations. MEDIC offers a robust foundation for the safe and effective implementation of LLMs in healthcare, potentially transforming clinical practice.</p>
<p><a href="https://arxiv.org/pdf/2409.07314v1">arXiv:2409.07314v1</a> 👍33</p>
<p><strong>“MMEvol” introduces a novel multimodal instruction data evolution framework</strong> to enhance the diversity and complexity of training data for multimodal large language models. By employing three evolution methods - fine-grained perceptual, cognitive reasoning, and interactive evolution - MMEvol achieves state-of-the-art performance on 13 vision-language benchmarks, with an average accuracy improvement of 3.1 percentage points compared to baseline models. Remarkably, it reaches top performance in nine tasks using significantly less data than current state-of-the-art models. This approach could significantly advance the development of more accurate and versatile multimodal AI systems.</p>
<p><a href="https://arxiv.org/pdf/2409.05840v3">arXiv:2409.05840v3</a> 👍33</p>
<p><strong>“OneGen” presents an efficient one-pass unified generation and retrieval framework for LLMs</strong>, addressing the limitations of traditional models in handling retrieval tasks. By integrating retrieval and generation within the same context using a special [RQ] token, OneGen outperforms previous solutions in various tasks. It achieves a 1.5-point improvement on average in Single-hop QA datasets, a 3.3-point F1 increase in Multi-hop QA datasets, and a 3.2-point accuracy boost in out-of-domain entity linking datasets. This approach has the potential to enhance performance in numerous NLP applications, from information extraction to text summarization.</p>
<p><a href="https://arxiv.org/pdf/2409.05152v2">arXiv:2409.05152v2</a> 👍21</p>
<p><strong>The “CDM” metric introduces a reliable approach for fair and accurate formula recognition evaluation</strong>, addressing the shortcomings of existing text-based character matching methods. By utilizing spatial character matching and incorporating visual feature extraction, CDM achieves a 96% consistency with human evaluation. It demonstrates robustness by scoring 1 in all samples of the Tiny-Doc-Math evaluation. Moreover, CDM enables efficient data selection, achieving comparable performance to using the entire dataset while only utilizing less than 20% of the data. This metric could significantly improve the accuracy and fairness of formula recognition evaluation across different models and datasets.</p>
<p><a href="https://arxiv.org/pdf/2409.03643v1">arXiv:2409.03643v1</a> 👍16</p>
<p><strong>“mPLUG-DocOwl2” presents a high-resolution compressing technique for OCR-free multi-page document understanding</strong>, addressing the challenge of compressing high-resolution document images while retaining crucial visual information. The introduced High-resolution DocCompressor achieves 98% of the baseline model’s performance while reducing visual tokens from 2,560 to 324 and cutting first token latency by over 50%. This approach not only excels in multi-page document understanding but also achieves comparable performance to state-of-the-art models on single-page tasks with fewer visual tokens. The implications for multimodal large language models and OCR-free document understanding are significant, potentially transforming how we process and analyze complex documents across various fields.</p>
<p><a href="https://arxiv.org/pdf/2409.03420v2">arXiv:2409.03420v2</a> 👍16</p>
<p>And that’s a wrap! See you next week!</p>


</section>

 ]]></description>
  <category>Fuzzing</category>
  <category>Multimodal Large Language Models</category>
  <category>Document Compressing</category>
  <category>Real Image Editing</category>
  <category>Natural Language Processing</category>
  <category>Formula Recognition</category>
  <category>Large Language Models</category>
  <guid>https://shwetank-kumar.github.io/posts/summary-2024-09-12/ai_paper_summaries.html</guid>
  <pubDate>Thu, 12 Sep 2024 07:00:00 GMT</pubDate>
</item>
<item>
  <title>The Great Data Famine: Why the AI that Ate the Web Is Still Starving</title>
  <dc:creator>Shwetank Kumar</dc:creator>
  <link>https://shwetank-kumar.github.io/posts/data-scarcity/</link>
  <description><![CDATA[ 





<p>If you’re a software engineer or data scientist who’s been losing sleep over the Twitter-fueled hysteria that “English is the new coding language,” I’ve got news for you: put down the panic button and step away from the job boards. I am here to tell you that despite what the Twitterverse might have you believe, AI isn’t about to steal your job or create Skynet. And if you’re an exec or investor who thinks otherwise, I’ve got a Nigerian prince who’d love to chat.</p>
<p>After a weekend of wrestling with the latest and supposedly greatest codegen tools, I’ve come to a startling conclusion: our AI overlords aren’t quite ready to steal your job or usher in the age of Skynet. In fact, they’re struggling with tasks that would make a junior dev roll their eyes.</p>
<p>Let me paint you a picture: There I was, surrounded by caffeine and a stack of AI whitepapers, trying to coax these silicon savants into solving some basic coding problems. It became painfully clear that to tackle even the most fundamental challenges, we’re in desperate need of a quantum leap in LLM complexity.</p>
<p>What we really need is for these models to channel their inner project manager – breaking down problems into bite-sized chunks, crafting plans, and methodically conquering each incremental hurdle. But instead of this sophisticated problem-solving, our current crop of AI tools often resemble a caffeinated squirrel trying to solve a Rubik’s cube – lots of frantic activity, not a lot of progress.</p>
<section id="the-transformer-plateau-when-bigger-isnt-always-better" class="level2">
<h2 class="anchored" data-anchor-id="the-transformer-plateau-when-bigger-isnt-always-better">The Transformer Plateau: When Bigger Isn’t Always Better</h2>
<p>Remember when we thought the solution to every AI challenge was simply to make it bigger? It was like Silicon Valley’s version of “supersize me,” but instead of fries, we were supersizing parameters and datasets. Well, folks, that all-you-can-eat buffet of data and compute has reached its limit.</p>
<p>Since the “Attention is all you need” paper dropped and gave us the Transformer architecture, we’ve been playing a game of “who can build the biggest model?” It’s been like a digital arms race, with each new model flexing more parameters. And for a while, it worked! We fed these hungry, hungry hippos more data, cranked up the parameter count, and watched the benchmarks climb.</p>
<p>But now, we’ve hit a wall. A big, data-shaped wall that’s putting a damper on our “bigger is better” party. We’re now facing a trifecta of challenges that even the most optimistic VC can’t hand-wave away:</p>
<ol type="1">
<li><p><strong>Data Scarcity:</strong> We’re scraping the bottom of the digital barrel, and it’s not pretty. Turns out, the internet isn’t infinite after all.</p></li>
<li><p><strong>Power Consumption:</strong> Our models are energy gluttons that could outshine Las Vegas. At this rate, we’ll need a small nuclear reactor for each training run.</p></li>
<li><p><strong>System Complexity:</strong> We’re playing high-stakes Jenga with GPUs, hoping that one wobbly card doesn’t bring down the entire expensive house of silicon (aka cluster).</p></li>
</ol>
<p>Unless we see some major breakthroughs in model architecture or how these Large Language Models (LLMs) learn, we’re stuck. Future AI products will be reheating the same capabilities we have today, just with fancier marketing. It’s like we’ve trained our AI to be a really eloquent parrot, but now we need it to write a novel.</p>
<p>Lets explore why our current AI models are perpetually data-starved, uncover the fundamental limitations of our learning approaches (backprop, SGD …), and maybe even find a path forward that doesn’t involve sacrificing our firstborns to the GPU gods. Welcome to the frontlines of the AI data crisis, in the land of diminishing returns – where the future is bright, but the training datasets are running on empty.</p>
</section>
<section id="beyond-chinchilla-llama-3.1-and-the-quest-for-more-data" class="level2">
<h2 class="anchored" data-anchor-id="beyond-chinchilla-llama-3.1-and-the-quest-for-more-data">Beyond Chinchilla: Llama 3.1 and the Quest for More Data</h2>
<p>Imagine an AI as a toddler with an endless appetite for knowledge. Now, picture that toddler devouring the entire internet and still asking for seconds. That’s our current predicament with Large Language Models (LLMs). These digital gluttons are slow learners with an insatiable hunger for data, and folks, we’re running out of internet to feed them. Who would’ve thought “we’ve run out of internet” would be a legitimate concern in 2024?</p>
<section id="exhibit-a-metas-llama-3.1---the-data-devourer" class="level3">
<h3 class="anchored" data-anchor-id="exhibit-a-metas-llama-3.1---the-data-devourer">Exhibit A: Meta’s Llama 3.1 - The Data Devourer</h3>
<p>Let’s dive into a real-world example that’ll make your head spin: Meta’s latest 8B parameter Llama 3.1 model. This digital beast was fed a whopping 15 trillion tokens during training. For those keeping score at home, that’s essentially the entire publicly available internet. The Llama paper [1] claims it’s all public data, but they’re keeping the exact dataset under wraps. The closest we’ve got to peeking behind the curtain is the “Fine Web Dataset” [2] on Hugging Face, tipping the scales at a hefty 44TB.</p>
</section>
<section id="breaking-the-rules-when-more-is-more" class="level3">
<h3 class="anchored" data-anchor-id="breaking-the-rules-when-more-is-more">Breaking the Rules: When More is… More?</h3>
<p>Now, if you’ve been following the AI literature like it’s the new Netflix, you might be thinking, “Wait a minute, isn’t that overkill?” And you’d be right - sort of. The Chinchilla paper [3], our previous guidebook for “compute optimal” training, would suggest that an 8B parameter model only needs about 160B tokens. Llama 3.1 ate roughly 100 times that amount!</p>
<p>But here’s the kicker: it worked. Meta’s decision to massively overindulge their model led to continued performance improvements. This reveals two mind-bending facts:</p>
<ol type="1">
<li>Many existing models are actually undernourished by comparison.</li>
<li>High-quality data is the new oil in the AI gold rush.</li>
</ol>
<p>Even after this data feast, Llama 3.1 hadn’t reached what we’d classically call convergence. It was still improving, like a bottomless pit of potential [4]. This, combined with Microsoft’s Tiny Stories paper [5], is forcing us to rethink the data requirements for training these models.</p>
</section>
<section id="the-bigger-they-are-the-hungrier-they-get" class="level3">
<h3 class="anchored" data-anchor-id="the-bigger-they-are-the-hungrier-they-get">The Bigger They Are, The Hungrier They Get</h3>
<p>And now consider the data requirements for 405B parameter version of Llama 3.1. It should ideally be trained on proportionally more data - we’re talking “several internets” worth. But guess what? It was trained on the same 15T tokens as its smaller sibling. If that dataset was barely enough for the 8B model, it’s like trying to feed a blue whale with a goldfish bowl for the 405B version.</p>
</section>
<section id="a-silver-lining-in-the-ai-cloud" class="level3">
<h3 class="anchored" data-anchor-id="a-silver-lining-in-the-ai-cloud">A Silver Lining in the AI Cloud</h3>
<p>Before you start stockpiling hard drives and building your own internet, there’s a glimmer of hope. For enterprises sitting on a goldmine of non-public data (that you’re legally allowed to use, of course), you’re in luck. This is your chance to fine-tune these models for specialized tasks and potentially push their performance beyond what’s publicly possible. And for the efficiency enthusiasts out there, there’s still plenty of room to explore knowledge distillation - teaching smaller models to mimic their bigger, data-guzzling cousins.</p>
</section>
<section id="the-tldr-version" class="level3">
<h3 class="anchored" data-anchor-id="the-tldr-version">The TL;DR Version</h3>
<ol type="1">
<li>Llama 3.1’s training reveals that our “well-trained” models might actually be underfed data-wise.</li>
<li>We’ve essentially run out of high-quality public data to train these ever-growing models.</li>
<li>The next frontier? Leveraging private data to push these models even further.</li>
</ol>
<p>Welcome to the era of data scarcity in AI - where the models are hungry, the internet is finite, and every byte counts!</p>
</section>
</section>
<section id="synthetic-data-ais-infinite-mirror-of-confusion" class="level2">
<h2 class="anchored" data-anchor-id="synthetic-data-ais-infinite-mirror-of-confusion">Synthetic Data: AI’s Infinite Mirror of Confusion</h2>
<p>When faced with data scarcity, researchers and engineers came up with a brilliant idea: Why not leverage AI models to generate their own training data? This concept, far from being a desperate measure, is actually a legitimate and innovative approach to addressing the data hunger of large language models which offers many advantages like unparalleled scalability, solution for privacy preservation in training data, and customization for specialized tasks. However, it also comes with its own set of challenges.</p>
<section id="model-collapse-when-ai-goes-on-a-bland-diet" class="level3">
<h3 class="anchored" data-anchor-id="model-collapse-when-ai-goes-on-a-bland-diet">Model Collapse: When AI Goes on a Bland Diet</h3>
<p>This self-cannibalization of data leads to what’s ominously known as ‘model collapse’. Don’t let the fancy term scare you - it’s simply what happens when your AI goes on a diet of nothing but its own increasingly bland word salad.</p>
<p>Here’s how it works: the model (or its bigger cousin) generates tokens based on probability distributions so it favors tokens closer to the mean (the “average” outputs) providing fewer examples of tokens out in the wings of the distribution. After a few cycles of generating and training on synthetic data, you lose all the diverse content from the original dataset. Result? Your models over generations lose the brilliance and versatility that would come from diversity and start generating singular, monochromatic data which doesn’t capture the real world anymore.</p>
<p>Its like making a photocopy of a photocopy - each generation gets blurrier and weirder until you end up with something that looks like it came from a glitchy parallel universe where creativity went to die.</p>
</section>
<section id="bias-on-steroids" class="level3">
<h3 class="anchored" data-anchor-id="bias-on-steroids">Bias on Steroids</h3>
<p>The opposite side of the same coin is that any small biases in the initial dataset get amplified out of propostion. So now the slight bias in your dataset is dialed to 11 on a scale of 10 and it thinks the entire world population consists of cat-loving, pizza-eating coders who never see the sun. Diversity? That’s just a myth, like inbox zero or bug-free code.</p>
</section>
<section id="quality-control-nightmare" class="level3">
<h3 class="anchored" data-anchor-id="quality-control-nightmare">Quality Control Nightmare</h3>
<p>Validating synthetic data is like fact-checking a politician’s promises - a Sisyphusean task (i.e.&nbsp;can’t be done, easily anyway). It’s a guessing game where the grand prize is “maybe your AI won’t embarrass itself in public.” And good luck keeping it current - by the time you’ve generated your synthetic data, the real world has moved on, leaving your AI stuck in last season’s trends like a digital fashion faux pas.</p>
</section>
<section id="cybersecurity-swiss-cheese" class="level3">
<h3 class="anchored" data-anchor-id="cybersecurity-swiss-cheese">Cybersecurity Swiss Cheese</h3>
<p>Just when you thought it couldn’t get worse, enter the hackers. Synthetic data is like a new chew toy for cybercriminals. They’re gleefully exploring all the ways they can mess with your data generation process, turning your AI into their personal puppet show.</p>
</section>
<section id="silver-linings-for-synthetic-data" class="level3">
<h3 class="anchored" data-anchor-id="silver-linings-for-synthetic-data">Silver Linings for Synthetic Data</h3>
<p>But wait! Don’t despair just yet. One person’s data dilemma is another’s research opportunity. Here are some tantalizing questions for the brave AI researchers of tomorrow:</p>
<ul>
<li>Can we develop smarter sampling strategies to generate synthetic data from the neglected “wings” of the probability distribution?</li>
<li>What’s the perfect cocktail of real and synthetic data? Is there a golden ratio, or does it depend on the task?</li>
<li>How can we build Fort Knox-level security around our synthetic data generation process?</li>
</ul>
</section>
<section id="the-tldr-version-1" class="level3">
<h3 class="anchored" data-anchor-id="the-tldr-version-1">The TL;DR Version</h3>
<ol type="1">
<li>Synthetic data is a promising solution to data scarcity, offering scalability, privacy, and customization.</li>
<li>However, it comes with significant challenges: model collapse, bias amplification, quality control issues, and security risks.</li>
<li>The future of synthetic data lies in developing better generation strategies, finding optimal real-synthetic data ratios, and creating robust security frameworks.</li>
</ol>
<p>Until we crack these problems, synthetic data is the AI world’s equivalent of combating climate change by painting everything green. It looks fantastic in PowerPoint presentations, but step outside, and you’ll find a world of plastic trees where your AI thinks photosynthesis is just the latest Instagram filter.</p>
</section>
</section>
<section id="deep-networks-are-slow-learners" class="level2">
<h2 class="anchored" data-anchor-id="deep-networks-are-slow-learners">Deep Networks are Slow Learners</h2>
<p>The underlying problem that is a root cause of all our AI woes is that neural networks and other architectures using a combination of back propagation and stochastic gradient descent (let’s call these SGD &amp; Progeny Pvt. Ltd.&nbsp;to include other algorithms like Adam, Ada etc.) are slow learners - absorbing knowledge at the speed of continental drift. They’re the Pangaea of machine learning, slowly but inexorably shuffling bits of information around until, eons later, you might just have a functional model. Let’s look at how these work.</p>
<section id="the-sgd-conundrum-navigating-ikea-blindfolded" class="level3">
<h3 class="anchored" data-anchor-id="the-sgd-conundrum-navigating-ikea-blindfolded">The SGD Conundrum: Navigating IKEA Blindfolded</h3>
<p>Stochastic Gradient Descent (SGD) and its variants are the cornerstone of modern machine learning optimization techniques. However, their effectiveness is limited by their inherent randomness. It’s basically like trying to find your way out of an IKEA blindfolded by randomly stumbling around, making small adjustments to your trajectory based on very limited local information. You might eventually find the exit, but you’ll bump into a lot of BILLY bookcases along the way.</p>
</section>
<section id="escape-from-local-minima-the-comfortable-rut" class="level3">
<h3 class="anchored" data-anchor-id="escape-from-local-minima-the-comfortable-rut">Escape from Local Minima: The Comfortable Rut</h3>
<p>Neural networks often get trapped in suboptimal solutions, or “local minima,” during training. To overcome this, we need to expose our models to diverse, high-quality data repeatedly. However, the scale at which this needs to happen is staggering - often requiring millions of iterations. Of course, there are heuristics that can help you along in the process, but coming up with the right set of parameters for training and escaping each minima requires a lot of experimentation with hyperparameters.</p>
</section>
<section id="the-hyperparameter-labyrinth-cracking-the-infinite-safe" class="level3">
<h3 class="anchored" data-anchor-id="the-hyperparameter-labyrinth-cracking-the-infinite-safe">The Hyperparameter Labyrinth: Cracking the Infinite Safe</h3>
<p>Hyperparameter tuning in machine learning is akin to trying to crack a safe with an infinite number of dials. Each parameter - learning rate, batch size, network architecture, etc. - can dramatically affect model performance, yet their interactions are often unpredictable and non-linear. It’s not uncommon for researchers to spend more time tuning hyperparameters than actually training models. This process is often more art than science, relying heavily on intuition, experience, and, frankly, a fair bit of luck.</p>
<p>Automated hyperparameter optimization techniques exist, and while helpful, often require significant computational resources and can still miss optimal configurations due to the vast search space. Moreover, hyperparameters that work well for one dataset or task might fail spectacularly on another, making it challenging to develop generalizable best practices.</p>
<p>The complexity of hyperparameter tuning also raises questions about the robustness and interpretability of our models. If slight tweaks to these parameters can lead to drastically different results, how can we trust the stability and reliability of our AI systems?</p>
</section>
<section id="the-curse-of-memorization-ais-the-office-obsession" class="level3">
<h3 class="anchored" data-anchor-id="the-curse-of-memorization-ais-the-office-obsession">The Curse of Memorization: AI’s “The Office” Obsession</h3>
<p>These models are turning into the Rain Man of useless information. Great if you need to count toothpicks, not so great for actual intelligence. As these models grow larger, they’re not getting smarter – they’re just getting better at regurgitating what they’ve seen before. It’s like that friend who can recite every line from “The Office” but can’t hold a conversation about anything else.</p>
</section>
<section id="the-silver-lining-future-directions" class="level3">
<h3 class="anchored" data-anchor-id="the-silver-lining-future-directions">The Silver Lining: Future Directions</h3>
<p>Data scarcity, which is a direct result of these fundamental limitations, is pushing us towards an inflection point. We can’t just keep making models bigger and feeding them more data. We need to fundamentally rethink how these models learn and generalize. It’s like we’ve been trying to build a skyscraper by just piling up more and more bricks. Now we need to stop and think about architecture, efficiency, and maybe invest in an elevator or two.</p>
<p>Some promising directions include:</p>
<ul>
<li>Algorithms that rely on second-order moments, though they require more memory to store gradients.</li>
<li>Combining these techniques with simplifying assumptions about the nature of the matrix [7], allowing us to store sparser versions.</li>
<li>Coupling these approaches with newer execution engines in GPUs for sparse matrices.</li>
<li>Exploring hardware-software co-design as a powerful research direction.</li>
</ul>
</section>
<section id="the-tldr-version-2" class="level3">
<h3 class="anchored" data-anchor-id="the-tldr-version-2">The TL;DR Version</h3>
<ol type="1">
<li>Deep networks learn slowly due to limitations in optimization techniques like SGD.</li>
<li>Key challenges include navigating complex loss landscapes, escaping local minima, tuning hyperparameters, and avoiding mere memorization.</li>
<li>We need to rethink our approach to model architecture and learning processes.</li>
<li>Promising directions include advanced optimization algorithms, sparse matrix techniques, and hardware-software co-design.</li>
</ol>
<p>Remember, in the world of AI, we’re not just teaching machines to learn – we’re learning how to teach. And right now, we’re realizing we might need to go back to teacher school ourselves.</p>
</section>
</section>
<section id="conclusion-the-tip-of-the-ai-iceberg" class="level2">
<h2 class="anchored" data-anchor-id="conclusion-the-tip-of-the-ai-iceberg">Conclusion: The Tip of the AI Iceberg</h2>
<p>As we’ve explored in this article, the challenges facing AI development are numerous and complex. We’ve only scratched the surface of the data scarcity issue, and there are still two major hurdles we haven’t yet discussed: power consumption and system complexity.</p>
<p>The energy requirements for training and running these increasingly large AI models are staggering. As we push the boundaries of model size and complexity, we’re also pushing the limits of our computational infrastructure. The power consumption of these models isn’t just a technical issue—it’s an environmental and economic concern that the AI community will need to address.</p>
<p>System complexity is another critical challenge. As our AI systems grow more sophisticated, managing and optimizing them becomes increasingly difficult. We’re rapidly approaching a point where the complexity of these systems may outpace our ability to understand and control them effectively.</p>
<p>Down the road I intend to delve deeper into these issues, exploring the implications of AI’s growing energy appetite and the challenges posed by increasingly complex systems. In the meanwhile if there are things in this space that you would like to learn about - DMs are always open!</p>
<ol type="1">
<li><a href="https://arxiv.org/pdf/2407.21783">The Llama 3 Herd of Models</a></li>
<li><a href="https://huggingface.co/datasets/HuggingFaceFW/fineweb">Fineweb dataset</a></li>
<li><a href="https://arxiv.org/abs/2203.15556">Training Compute-Optimal Large Language Models</a></li>
<li><a href="https://x.com/karpathy/status/1781028605709234613?lang=en"><span class="citation" data-cites="Karpathy">@Karpathy</span></a></li>
<li><a href="https://arxiv.org/abs/2305.07759">TinyStories: How Small Can Language Models Be and Still Speak Coherent English?</a></li>
<li><a href="https://arxiv.org/abs/2404.01413">Is Model Collapse Inevitable? Breaking the Curse of Recursion by Accumulating Real and Synthetic Data</a></li>
<li><a href="https://arxiv.org/pdf/1802.09568">Shampoo: Preconditioned Stochastic Tensor Optimization</a></li>
</ol>


</section>

 ]]></description>
  <category>genai</category>
  <category>strategy</category>
  <category>investment</category>
  <guid>https://shwetank-kumar.github.io/posts/data-scarcity/</guid>
  <pubDate>Fri, 06 Sep 2024 07:00:00 GMT</pubDate>
  <media:content url="https://shwetank-kumar.github.io/posts/data-scarcity/robot.jpeg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>🌙 AI Afterhours: Top AI Papers for Aug 30 - Sep 05, 2024</title>
  <dc:creator>Shwetank Kumar</dc:creator>
  <link>https://shwetank-kumar.github.io/posts/summary-2024-09-05/ai_paper_summaries.html</link>
  <description><![CDATA[ 





<p>Welcome to this week’s AI Afterhours! Your weekly digest of most upvoted papers in AI. Below is gist of the results, how they got them, and why you should care. With that, let’s dive into the most exciting AI research from August 30 to September 05, 2024.</p>
<p><strong>Writing in the Margins</strong> introduces a novel inference pattern for large language models (LLMs) that significantly enhances their performance on long-context retrieval tasks. By leveraging chunked prefill of the key-value cache, this approach achieves an impressive 7.5% boost in reasoning accuracy and a whopping 30% increase in F1-score for aggregation tasks. What’s particularly exciting is that these improvements come without the need for fine-tuning, potentially revolutionizing how we handle complex, multi-hop reasoning and summarization tasks with off-the-shelf models. This could be a game-changer for applications requiring efficient processing of lengthy documents or intricate data analysis.</p>
<p><a href="https://arxiv.org/pdf/2408.14906v1">arXiv:2408.14906v1</a> 👍102</p>
<p>Ever imagined your favorite video game running entirely on AI? <strong>Diffusion Models Are Real-Time Game Engines</strong> brings us one step closer to that reality. By training a generative diffusion model on game trajectories, researchers have created a system capable of simulating DOOM at over 20 frames per second on a single TPU. The model achieves a PSNR of 29.4, rivaling lossy JPEG compression, and produces simulations so convincing that human raters struggle to distinguish them from the real game. This breakthrough could revolutionize game development, enabling more dynamic, AI-driven environments and potentially opening new avenues for interactive software systems.</p>
<p><a href="https://arxiv.org/pdf/2408.14837v1">arXiv:2408.14837v1</a> 👍76</p>
<p>In the realm of multimodal AI, <strong>Law of Vision Representation in MLLMs</strong> offers a groundbreaking approach to optimizing vision representations in large language models. By introducing the Alignment and Correspondence (AC) score, the researchers have found a way to predict model performance with an astonishing 95.72% accuracy. This means we can now identify the best vision representation without the computational burden of fine-tuning, potentially slashing costs by 99.7%. The study also reveals that increasing resolution and combining features can significantly boost model performance, with a 89.69% Recall@3. These insights could accelerate the development of more efficient and powerful multimodal AI systems across various applications.</p>
<p><a href="https://arxiv.org/pdf/2408.16357v1">arXiv:2408.16357v1</a> 👍55</p>
<p><strong>Loopy</strong> tackles the challenge of creating more natural audio-driven portrait avatars by addressing the weak correlation between audio and portrait motion in existing methods. The framework’s innovative inter/intra-clip temporal layer design and audio-to-latents module significantly improve temporal stability and motion diversity. On the CelebV-HQ test set, Loopy outperforms existing methods with an IQA score of 3.780 and a Sync-C score of 4.849. For the RAVDESS dataset, it achieves an impressive IQA of 4.506 and FVD-Inc of 220.634. These advancements could lead to more realistic and engaging virtual avatars for applications ranging from virtual assistants to immersive entertainment experiences.</p>
<p><a href="https://arxiv.org/pdf/2409.02634v2">arXiv:2409.02634v2</a> 👍41</p>
<p>The <strong>CogVLM2</strong> family of models pushes the boundaries of visual language understanding, tackling both image and video tasks with impressive results. By employing a Vision Transformer encoder and a novel adapter for vision-language fusion, these models achieve state-of-the-art performance across various benchmarks. Notable achievements include 68.25% on TextVQA, 74.5% on DocVQA, and 74.0% on MMVet for video understanding. The versatility of CogVLM2 in handling diverse tasks from document analysis to video interpretation showcases its potential to drive advancements in AI-powered visual understanding systems, with applications spanning from automated content analysis to advanced human-computer interaction.</p>
<p><a href="https://arxiv.org/pdf/2408.16500v1">arXiv:2408.16500v1</a> 👍37</p>
<p>And that’s a wrap! See you next week!</p>



 ]]></description>
  <category>Vision Transformers</category>
  <category>Audio-Driven Video Generation</category>
  <category>Large Language Models</category>
  <category>Generative Diffusion Models</category>
  <category>Multimodal Large Language Models (MLLMs)</category>
  <guid>https://shwetank-kumar.github.io/posts/summary-2024-09-05/ai_paper_summaries.html</guid>
  <pubDate>Fri, 30 Aug 2024 07:00:00 GMT</pubDate>
</item>
<item>
  <title>🌙 AI Afterhours: Top AI Papers for Aug 21 - Aug 27, 2024</title>
  <dc:creator>Shwetank Kumar</dc:creator>
  <link>https://shwetank-kumar.github.io/posts/summary-2024-08-27/ai_paper_summaries.html</link>
  <description><![CDATA[ 





<p>Welcome to this week’s AI Afterhours! Your weekly digest of most upvoted papers in AI. Below is gist of the results, how they got them, and why you should care.</p>
<p>With that, let’s dive into the most exciting AI research from August 21 to August 27, 2024.</p>
<p><strong>TWLV-I: Analysis and Insights from Holistic Evaluation on Video Foundation Models</strong> tackles the challenge of comprehensively understanding both appearance and motion in videos. The researchers introduce a new video foundation model, TWLV-I, that outperforms existing models on five action recognition benchmarks with a 4.6% improvement in average top-1 accuracy. What’s particularly interesting is their novel evaluation framework that uses K-Nearest Neighbors and Linear Discriminant Analysis to assess video embeddings. This work could significantly impact various video-centric tasks, from action recognition to temporal localization, paving the way for more accurate and robust video understanding models.</p>
<p><a href="https://arxiv.org/pdf/2408.11318v2">ArXiv: 2408.11318v2</a> 👍42</p>
<p><strong>SwiftBrush v2: Make Your One-step Diffusion Model Better Than Its Teacher</strong> addresses the quality-diversity trade-off in one-step text-to-image diffusion models. The researchers have managed to achieve an impressive FID score of 8.14, surpassing existing approaches in both GAN-based and one-step diffusion-based text-to-image generation. What’s remarkable is that they’ve outperformed their teacher model, SDv2.1, while maintaining equivalent model size and inference times. The secret sauce? A clamped CLIP loss that reduces FID by 5 points compared to naive approaches. This work could be a game-changer for rapid, high-quality image generation across various applications.</p>
<p><a href="https://arxiv.org/pdf/2408.14176v2">ArXiv: 2408.14176v2</a> 👍41</p>
<p><strong>Controllable Text Generation for Large Language Models: A Survey</strong> provides a comprehensive review of the current state of Controllable Text Generation (CTG) for Large Language Models. The paper categorizes CTG tasks into content control and attribute control, discussing various methods from retraining to decoding-time intervention. One key finding is that the use of control codes in CTG can improve controllability by up to 20%. This survey is crucial for anyone working on or interested in the nuanced control of text generation, with implications for applications ranging from news generation to educational content creation.</p>
<p><a href="https://arxiv.org/pdf/2408.12599v1">ArXiv: 2408.12599v1</a> 👍40</p>
<p><strong>Sapiens: Foundation for Human Vision Models</strong> introduces a family of vision transformer models pre-trained on a large-scale dataset of human images. The Sapiens-2B model achieves a mean error of around 12° on the THuman2.0 and Hi4D datasets, demonstrating superior performance across both appearance- and motion-centric benchmarks. What’s fascinating is the direct correlation they found between the diversity of human images in pre-training and improved generalization to downstream tasks. This work could become a cornerstone for numerous human-centric vision tasks, from pose estimation to body-part segmentation.</p>
<p><a href="https://arxiv.org/pdf/2408.12569v3">ArXiv: 2408.12569v3</a> 👍37</p>
<p><strong>Building and better understanding vision-language models: insights and future directions</strong> delves into the challenges of building vision-language models (VLMs) and proposes methods to enhance their performance, particularly in document understanding tasks. The researchers introduce the Docmatix dataset, which includes 2.4 million images and 9.5 million QA pairs from 1.3 million PDF documents. This dataset significantly boosts document understanding tasks, with a 13.7-point improvement on DocVQA. Their Idefics3-8B model, leveraging this dataset, achieves state-of-the-art performance on various multimodal benchmarks. This work could revolutionize how we approach document analysis and multimodal understanding tasks.</p>
<p><a href="https://arxiv.org/pdf/2408.12637v1">ArXiv: 2408.12637v1</a> 👍36</p>
<p><strong>TableBench: A Comprehensive and Complex Benchmark for Table Question Answering</strong> addresses the gap between academic benchmarks and real-world applications in table question answering tasks. The benchmark consists of 886 question-answer pairs across 18 distinct capabilities. Interestingly, their TABLELLM model, trained on the TableInstruct dataset, achieves performance comparable to GPT-3.5. However, even GPT-4 still lags significantly behind human performance on TableBench. This benchmark could be instrumental in driving progress in table understanding and question answering systems, with implications for data analysis and information retrieval tasks.</p>
<p><a href="https://arxiv.org/pdf/2408.09174v1">ArXiv: 2408.09174v1</a> 👍34</p>
<p><strong>SWE-bench-java: A GitHub Issue Resolving Benchmark for Java</strong> introduces a benchmark to evaluate the issue-resolving capabilities of large language models in software engineering tasks, specifically for Java. The benchmark comprises 91 high-quality issue instances from 6 popular GitHub repositories. Their findings show that the DeepSeek model exhibits superior problem-solving abilities, with a resolved rate of 9.89% on SWE-bench-java-verified. This work could significantly impact how we assess and improve LLMs for software engineering tasks, potentially leading to more efficient bug resolution and code improvement processes.</p>
<p><a href="https://arxiv.org/pdf/2408.14354v1">ArXiv: 2408.14354v1</a> 👍31</p>
<p><strong>LLM Pruning and Distillation in Practice: The Minitron Approach</strong> tackles the challenge of compressing large language models while maintaining their performance. Applying the Minitron compression strategy to Llama 3.1 8B and Mistral NeMo 12B models, they achieve impressive results. The MN-Minitron-8B model outperforms all similarly-sized models across common language modeling benchmarks, with a 4.5% improvement over the teacher model on the Winogrande task. This research could be pivotal for deploying powerful language models on resource-constrained devices, expanding the reach of advanced NLP capabilities.</p>
<p><a href="https://arxiv.org/pdf/2408.11796v2">ArXiv: 2408.11796v2</a> 👍30</p>
<p><strong>K-Sort Arena: Efficient and Reliable Benchmarking for Generative Models via K-wise Human Preferences</strong> introduces a novel platform for evaluating visual generative models. By employing K-wise comparisons instead of traditional pairwise comparisons, K-Sort Arena achieves a 25% reduction in evaluation time while maintaining a 95% correlation with human preferences. Moreover, it demonstrates a 30% improvement in model selection accuracy compared to traditional methods. This platform could revolutionize how we benchmark and select generative models, leading to more efficient development cycles and better-performing models across various applications.</p>
<p><a href="https://arxiv.org/pdf/2408.14468v1">ArXiv: 2408.14468v1</a> 👍28</p>
<p>And that’s a wrap! See you next week!</p>



 ]]></description>
  <category>Generative Models</category>
  <category>Video Foundation Models</category>
  <category>Vision Transformers</category>
  <category>Table Question Answering</category>
  <category>Controllable Text Generation</category>
  <category>Large Language Models</category>
  <category>Diffusion Models</category>
  <category>Vision-Language Models</category>
  <category>Language Model Compression</category>
  <guid>https://shwetank-kumar.github.io/posts/summary-2024-08-27/ai_paper_summaries.html</guid>
  <pubDate>Tue, 27 Aug 2024 07:00:00 GMT</pubDate>
</item>
<item>
  <title>The Surprisingly Lucrative Journey of Bootstrapping a Brand Recommender System: From Chaos to Cash</title>
  <dc:creator>Shwetank Kumar</dc:creator>
  <link>https://shwetank-kumar.github.io/posts/cold-start/</link>
  <description><![CDATA[ 





<p>While everyone seems captivated by AGI and busy building Just-Another-Chatbot (JAC<sup>TM</sup>), they’re overlooking the real problems that can be solved (and the money to be made) through practical ML Engineering. In this inaugural series of blog posts, I’ll dive deep into one such problem: bootstrapping and building a brand recommender system from the ground up. Drawing from my experience as an engineer, executive, and consultant across multiple consumer tech companies and marketplaces, I’ll guide you through the process of creating a recommendation engine that not only predicts customer preferences but also enhances their overall shopping experience.</p>
<p>We’ll explore the journey from raw data to a sophisticated system that can significantly boost your bottom line while offering your customers a personalized and delightful experience. So, let’s embark on this data-driven adventure and unlock the potential of personalized recommendations in e-commerce. By the end of this series, you’ll have the tools to transform your e-commerce platform into a cash machine that keeps both your customers and CFOs happy.</p>
<section id="the-problem-e-commerce-is-a-jungle-and-your-customers-are-lost" class="level2">
<h2 class="anchored" data-anchor-id="the-problem-e-commerce-is-a-jungle-and-your-customers-are-lost">The Problem: E-commerce Is a Jungle (And Your Customers Are Lost)</h2>
<p>Imagine you’ve just launched an e-commerce platform. It’s sleek, efficient, and boasts an impressive array of brands. Initially, you’re confident in its success. However, user feedback quickly reveals a common challenge in the e-commerce world:</p>
<ul>
<li>“I’m overwhelmed by the number of options.”</li>
<li>“The recommendations don’t seem relevant to my interests.”</li>
<li>“I can’t find products that match my specific needs.”</li>
</ul>
<p>These concerns are not uncommon. In the realm of online retail, the balance between variety and accessibility is crucial. Insufficient options can leave customers feeling limited, while an overabundance can lead to decision fatigue. This phenomenon, often referred to as the “paradox of choice,” can significantly impact user experience and, consequently, your conversion rates.</p>
<p>This, my friends, is why a good recommendation system is invaluable. It’s like having a wise, all-knowing friend who gently guides your customers to their next favorite purchase. And today, we’re going to build that friend from scratch.</p>
</section>
<section id="step-0-the-oh-crap-we-have-no-data-phase---totally-random-model" class="level2">
<h2 class="anchored" data-anchor-id="step-0-the-oh-crap-we-have-no-data-phase---totally-random-model">Step 0: The “Oh Crap, We Have No Data” Phase - Totally Random Model</h2>
<p>Let’s start at the beginning (a very good place to start…). You’ve just launched your e-commerce platform. It looks great, it works smoothly, but there’s one big problem: you have no data. Your recommendation system is a blank slate. What do you do?</p>
<p>Take a deep breath and repeat after me: “Random is better than nothing.” Now, I know what you’re thinking. “But this is just throwing darts blindfolded! How is this helping anyone?” And you’re right, it’s not ideal. But here’s the secret: it’s not about being perfect; it’s about starting the flywheel.</p>
<p>Every time a user sees a random recommendation, you’re gathering data. Maybe they ignore it (data point!). Maybe they click on it (data point!). Maybe they buy it (cha-ching and data point!). Every product interaction (or lack thereof) is a breadcrumb that will lead you out of the data desert.</p>
<p>Let’s whip up a quick Python function to generate random recommendations:</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> random</span>
<span id="cb1-2"></span>
<span id="cb1-3"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> get_random_recommendations(all_brands, n<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>):</span>
<span id="cb1-4">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> random.sample(all_brands, <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">min</span>(n, <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(all_brands)))</span>
<span id="cb1-5"></span>
<span id="cb1-6"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Example usage</span></span>
<span id="cb1-7">all_brands <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'recommending_brand'</span>].unique().tolist()</span>
<span id="cb1-8"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(get_random_recommendations(all_brands))</span></code></pre></div>
<p>Pro tip: While you’re showing random recommendations, make sure you’re logging EVERYTHING. Every view, every click, every purchase. This data will be worth its weight in gold later on. Trust me, future you will thank present you for this foresight. And while you are at it do make sure that the data is high quality. Algorithms are fickle and state of the art on those changes every week – nay day! But poor quality data once logged sets the ceiling on what you can do with it.</p>
</section>
<section id="step-1-the-we-have-some-data-but-its-not-about-users-phase---feature-based-clustering-model" class="level2">
<h2 class="anchored" data-anchor-id="step-1-the-we-have-some-data-but-its-not-about-users-phase---feature-based-clustering-model">Step 1: The “We Have Some Data, But It’s Not About Users” Phase - Feature-Based Clustering Model</h2>
<p>Alright, so you’ve been running your random recommendation engine for a while. You’ve got some sales, you’ve got some brand data, but you still don’t have enough user interaction data to build a proper collaborative filtering system. Don’t worry, we’re going to make lemonade out of these lemons.</p>
<p>Enter: Feature-Based Clustering.</p>
<p>Now, gather ’round, because I’m about to share a secret: brands, like people, have personalities. And just like you wouldn’t set up your quiet, bookish friend with your party-animal cousin (trust me, I’ve made that mistake), you shouldn’t be recommending wildly dissimilar brands to your users.</p>
<p>Let’s create a simple example using K-means clustering. Don’t let the fancy name scare you - it’s just a way of grouping similar things together.</p>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.cluster <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> KMeans</span>
<span id="cb2-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb2-3"></span>
<span id="cb2-4"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Example brand features (price, target_age, sportiness)</span></span>
<span id="cb2-5">brand_features <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {</span>
<span id="cb2-6">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Nike"</span>: [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">80</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">25</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">9</span>],</span>
<span id="cb2-7">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Adidas"</span>: [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">75</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">30</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span>],</span>
<span id="cb2-8">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Puma"</span>: [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">60</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">28</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">7</span>],</span>
<span id="cb2-9">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Reebok"</span>: [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">65</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">35</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span>],</span>
<span id="cb2-10">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Under Armour"</span>: [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">70</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">27</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">9</span>],</span>
<span id="cb2-11">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"New Balance"</span>: [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">85</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">40</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>],</span>
<span id="cb2-12">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Asics"</span>: [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">90</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">35</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span>],</span>
<span id="cb2-13">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Converse"</span>: [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">55</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">22</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>],</span>
<span id="cb2-14">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Vans"</span>: [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">20</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>],</span>
<span id="cb2-15">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Skechers"</span>: [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">45</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">45</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>]</span>
<span id="cb2-16">}</span>
<span id="cb2-17"></span>
<span id="cb2-18"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> cluster_brands(brand_features, n_clusters<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>):</span>
<span id="cb2-19">    brands <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">list</span>(brand_features.keys())</span>
<span id="cb2-20">    features <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">list</span>(brand_features.values()))</span>
<span id="cb2-21">    </span>
<span id="cb2-22">    kmeans <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> KMeans(n_clusters<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>n_clusters, random_state<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">42</span>)</span>
<span id="cb2-23">    clusters <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> kmeans.fit_predict(features)</span>
<span id="cb2-24">    </span>
<span id="cb2-25">    brand_clusters <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {brand: cluster <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> brand, cluster <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">zip</span>(brands, clusters)}</span>
<span id="cb2-26">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> brand_clusters</span>
<span id="cb2-27"></span>
<span id="cb2-28">brand_clusters <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> cluster_brands(brand_features)</span>
<span id="cb2-29"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(brand_clusters)</span>
<span id="cb2-30"></span>
<span id="cb2-31"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> get_cluster_recommendations(purchased_brand, brand_clusters, n<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>):</span>
<span id="cb2-32">    cluster <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> brand_clusters[purchased_brand]</span>
<span id="cb2-33">    cluster_brands <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [brand <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> brand, c <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> brand_clusters.items() <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> c <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> cluster]</span>
<span id="cb2-34">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> random.sample(cluster_brands, <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">min</span>(n, <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(cluster_brands)))</span>
<span id="cb2-35"></span>
<span id="cb2-36"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Example usage</span></span>
<span id="cb2-37">purchased_brand <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Nike"</span></span>
<span id="cb2-38"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(get_cluster_recommendations(purchased_brand, brand_clusters))</span></code></pre></div>
<p>Let me share a real-world example that illustrates the power of this approach. Back in the day I worked with a startup that had a range of products - everything from $5 friendship bracelets that screamed “summer camp chic” to $500 mid-century modern chairs that whispered “I have a trust fund.” Their recommendation system? About as sophisticated as a Magic 8-Ball with a hangover. It was the digital equivalent of that one clueless sales associate who tries to upsell you a tuxedo when you’re shopping for gym shorts.</p>
<p>The result? Their conversion rate was lower than my undergrad GPA (and trust me, that’s saying something). They addressed this by implementing a clustering system based on various product attributes. We’re talking price, category, style, color palette - if it could be quantified, they clustered it. And boom! Faster than you can say “artisanal hand-knitted cat sweater,” their click-through rates went through the roof! Their engineering blog was practically giddy with excitement (in that restrained, data-scientist kind of way) about the boost in overall sales.</p>
<p>Now a good data science algorithm needs to do at least two things (and definitely the second of these two): 1. It needs needs to improve a KPI - in this case conversions, although it can be anything. (e.g.&nbsp;if you are a Ferengi it will invariably be profits as laid out in the Ferengi Rules of Acquisition) 2. Regardless of whether it does 1. it needs to light the way to how you might improve the KPI in the future.</p>
<p>In this case, it wasn’t just the algorithm doing the work automatically, but the entire system we built around it. We maintained a degree of randomness in our recommendations to continue generating new data and learning from it. It’s like saying, “We know you’re interested in vintage decor, but have you considered this modern minimalist piece?” This approach not only serves immediate customer interests but also introduces them to new products they might enjoy. The lesson here? Even a relatively simple clustering approach can significantly improve your recommendation engine. It’s not about having the fanciest algorithm on the block; it’s about understanding your customers and not trying to sell snowshoes to someone shopping for flip-flops. So, whether you’re dealing with luxury items and budget options, or niche products and mainstream goods, remember: cluster wisely, but maintain some variety. Your conversion rates (and your customers) will thank you.</p>
</section>
<section id="step-2-the-now-were-cooking-with-gas-phase---purchase-based-association-model" class="level2">
<h2 class="anchored" data-anchor-id="step-2-the-now-were-cooking-with-gas-phase---purchase-based-association-model">Step 2: The “Now We’re Cooking with Gas” Phase - Purchase-Based Association Model</h2>
<p>Alright, my data-hungry friends, we’ve arrived at the juicy part. You’ve been diligently collecting user interaction data (you have, haven’t you?), and now it’s time to put it to use. We’re going to build a purchase-based association model.</p>
<p>This is where the magic really starts to happen. We’re going to create a system that understands that people who buy brand A often buy brand B, even if we don’t know why. It’s like being a really good matchmaker without understanding the intricacies of human psychology.</p>
<p>Let’s cook up a simple association model:</p>
<div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> collections <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> defaultdict</span>
<span id="cb3-2"></span>
<span id="cb3-3"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> build_association_model(purchase_data):</span>
<span id="cb3-4">    associations <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> defaultdict(<span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">lambda</span>: defaultdict(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>))</span>
<span id="cb3-5">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> purchase <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> purchase_data:</span>
<span id="cb3-6">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i, brand1 <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">enumerate</span>(purchase):</span>
<span id="cb3-7">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> brand2 <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> purchase[i<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>:]:</span>
<span id="cb3-8">                associations[brand1][brand2] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span>
<span id="cb3-9">                associations[brand2][brand1] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span>
<span id="cb3-10">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> associations</span>
<span id="cb3-11"></span>
<span id="cb3-12"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Example purchase data</span></span>
<span id="cb3-13">purchase_data <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [</span>
<span id="cb3-14">    [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Nike"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Adidas"</span>],</span>
<span id="cb3-15">    [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Nike"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Under Armour"</span>],</span>
<span id="cb3-16">    [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Adidas"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Puma"</span>],</span>
<span id="cb3-17">    [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Puma"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Reebok"</span>],</span>
<span id="cb3-18">    [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Nike"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Converse"</span>],</span>
<span id="cb3-19">    [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Vans"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Converse"</span>],</span>
<span id="cb3-20">    [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"New Balance"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Asics"</span>],</span>
<span id="cb3-21">    [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Skechers"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"New Balance"</span>]</span>
<span id="cb3-22">]</span>
<span id="cb3-23"></span>
<span id="cb3-24">association_model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> build_association_model(purchase_data)</span>
<span id="cb3-25"></span>
<span id="cb3-26"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> get_associated_brands(brand, association_model, n<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>):</span>
<span id="cb3-27">    associated <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sorted</span>(association_model[brand].items(), key<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">lambda</span> x: x[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>], reverse<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb3-28">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> [b <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> b, _ <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> associated[:n]]</span>
<span id="cb3-29"></span>
<span id="cb3-30"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Example usage</span></span>
<span id="cb3-31">purchased_brand <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Nike"</span></span>
<span id="cb3-32"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(get_associated_brands(purchased_brand, association_model))</span></code></pre></div>
<p>Now, let me tell you why this is a game-changer. Consider the case of Amazon, the e-commerce giant. In their early days, they primarily sold books. But as they expanded into other product categories, they faced a massive challenge: how to effectively cross-sell across these diverse categories? Their solution was to implement a sophisticated association model, much like the one we’ve just built (though admittedly, theirs was far more complex). This “item-to-item collaborative filtering” approach, as they called it, allowed them to say, “Customers who bought this item also bought…”</p>
<p>The impact was significant. According to a paper published by Amazon’s engineers in 2003 titled “Amazon.com Recommendations: Item-to-Item Collaborative Filtering”, this recommendation system offered substantial advantages over traditional collaborative filtering techniques as it could:</p>
<ul>
<li>Handle a massive scale of data - tens of millions of customers and millions of catalog items.</li>
<li>Produce high-quality recommendations in real-time, scaling well to sudden spikes in traffic.</li>
<li>Recommend across diverse product categories, from books to electronics to clothing.</li>
</ul>
<p>While the paper doesn’t provide specific sales figures, it does mention that Amazon’s recommendation system significantly improved click-through and conversion rates compared to untargeted content such as top sellers.</p>
</section>
<section id="step-3-the-six-degrees-of-kevin-bacon-phase---transitive-association-model" class="level2">
<h2 class="anchored" data-anchor-id="step-3-the-six-degrees-of-kevin-bacon-phase---transitive-association-model">Step 3: The “Six Degrees of Kevin Bacon” Phase - Transitive Association Model</h2>
<p>Ever played “Six Degrees of Kevin Bacon”? Well, we’re about to do something similar with our brands, and it’s going to blow your recommendation socks off.</p>
<p>Direct associations are great, but they’re limited. What if we could create a web of associations, where brand A is connected to brand B, which is connected to brand C, creating an indirect link between A and C? It’s like being at a party where your network explodes as friends introduce you to their friends.</p>
<p>By considering second-order and third-order connections, we can uncover hidden relationships in our data, leading to nuanced and unexpected recommendations. Here’s how to do it in &lt; 10 lines of code:</p>
<div class="sourceCode" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> expand_adjacency_matrix(adj_matrix, max_order<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, weight_factor<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span>):</span>
<span id="cb4-2">    expanded_matrix <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> adj_matrix.copy()</span>
<span id="cb4-3">    current_matrix <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> normalize_matrix(adj_matrix)</span>
<span id="cb4-4"></span>
<span id="cb4-5">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> order <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, max_order <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>):</span>
<span id="cb4-6">        current_matrix <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> current_matrix.dot(normalize_matrix(adj_matrix))</span>
<span id="cb4-7">        weighted_connections <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> current_matrix <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> (weight_factor <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span> (order <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>))</span>
<span id="cb4-8">        expanded_matrix <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> weighted_connections</span>
<span id="cb4-9"></span>
<span id="cb4-10">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> normalize_matrix(expanded_matrix)</span></code></pre></div>
<p>Let’s break this down:</p>
<ol type="1">
<li>We start with our original adjacency matrix of direct brand connections.</li>
<li>We calculate higher-order connections up to a specified maximum (default 5, because six degrees of separation, right?).</li>
<li>Here’s the clever bit: we apply a weight factor. Each higher-order connection gets reduced weight because your friend’s friend’s friend’s opinion shouldn’t count as much as your direct friend’s.</li>
<li>We add all these weighted connections to our original matrix, creating a rich tapestry of brand relationships.</li>
<li>Finally, we normalize everything to keep our numbers manageable.</li>
</ol>
<p>The result? A supercharged adjacency matrix capturing not just direct relationships, but a whole network of indirect connections.</p>
<p>Now, imagine a niche online bookstore specializing in obscure academic texts. Their inventory is so specific that direct associations are as rare as a first-edition Gutenberg Bible at a yard sale.</p>
<p>A customer buys “The Mating Habits of 12th Century Mongolian Horses” (yes, I’m having fun with these titles). In a traditional system, we’d be stuck. But with our transitive associations, we might recommend “The Economic Impact of Horse Trading in Medieval Asia”, even if no one had ever purchased these together. Our system could find a chain of associations linking them through other related books.</p>
<p>This approach enables academics to joyfully tumble down rabbit holes of related obscure topics, much to the store owner’s delight. It illustrates the power of transitive associations, especially for businesses with niche catalogs or sparse collaborative filtering matrices. We’re transforming a sparse matrix into a dense one, uncovering “hidden” connections that create a discovery engine to surprise and delight customers, potentially leading them down purchasing paths they never knew existed.</p>
</section>
<section id="step-4-fine-tuning-the-magic---hyperparameters-and-evaluation" class="level2">
<h2 class="anchored" data-anchor-id="step-4-fine-tuning-the-magic---hyperparameters-and-evaluation">Step 4: Fine-Tuning the Magic - Hyperparameters and Evaluation</h2>
<p>Alright, data enthusiasts, we’ve built our transitive association model, but now it’s time to give it that extra polish. Think of it as tuning a high-performance engine - we need to adjust the nitrous levels just right.</p>
<section id="hyperparameter-tuning-finding-the-sweet-spot" class="level3">
<h3 class="anchored" data-anchor-id="hyperparameter-tuning-finding-the-sweet-spot">Hyperparameter Tuning: Finding the Sweet Spot</h3>
<p>Remember our <code>expand_adjacency_matrix</code> function? It comes with two key hyperparameters:</p>
<ol type="1">
<li><code>max_order</code>: How far down the rabbit hole of connections we’re willing to go</li>
<li><code>weight_factor</code>: Our trust factor for the friend of a friend of a friend</li>
</ol>
<p>These aren’t just arbitrary numbers we pulled out of a magician’s hat. They’re the secret sauce that can make or break our recommendations.</p>
<p>Let’s take a closer look at <code>weight_factor</code>. Set it too high, and you might end up recommending winter parkas to someone shopping for swimwear. Set it too low, and you’re barely scratching the surface of potential connections.</p>
<p>So how do we find the Goldilocks zone? Enter: hyperparameter tuning. It’s like finding the perfect recipe, but instead of slurping soup, we’re crunching numbers.</p>
<div class="sourceCode" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> evaluate_model(adj_matrix, test_data, max_order, weight_factor):</span>
<span id="cb5-2">    expanded_matrix <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> expand_adjacency_matrix(adj_matrix, max_order, weight_factor)</span>
<span id="cb5-3">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> calculate_hit_rate(expanded_matrix, test_data)</span>
<span id="cb5-4"></span>
<span id="cb5-5"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Grid search for best hyperparameters</span></span>
<span id="cb5-6">best_score <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span></span>
<span id="cb5-7">best_params <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {}</span>
<span id="cb5-8"></span>
<span id="cb5-9"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> max_order <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">7</span>):</span>
<span id="cb5-10">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> weight_factor <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> [<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.3</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.7</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.9</span>]:</span>
<span id="cb5-11">        score <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> evaluate_model(train_adj_matrix, test_data, max_order, weight_factor)</span>
<span id="cb5-12">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> score <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span> best_score:</span>
<span id="cb5-13">            best_score <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> score</span>
<span id="cb5-14">            best_params <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'max_order'</span>: max_order, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'weight_factor'</span>: weight_factor}</span>
<span id="cb5-15"></span>
<span id="cb5-16"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Best parameters: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>best_params<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb5-17"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Best score: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>best_score<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span></code></pre></div>
</section>
<section id="model-evaluation-separating-the-wheat-from-the-chaff" class="level3">
<h3 class="anchored" data-anchor-id="model-evaluation-separating-the-wheat-from-the-chaff">Model Evaluation: Separating the Wheat from the Chaff</h3>
<p>Now, how do we know if our model is actually any good? We can’t just take it out for a test drive on the same roads we built it on. That’s where train-test splits come in handy.</p>
<p>Here’s our game plan:</p>
<ol type="1">
<li>Split your data into training and test sets. Think of it as studying for an exam (training) and then taking the final (testing).</li>
<li>Build your adjacency matrix using only the training data.</li>
<li>Use your tuned model to make predictions on the test set.</li>
<li>Compare your transitive model against simpler approaches, like random recommendations or direct associations only.</li>
</ol>
<p>Let’s see how it’s done:</p>
<div class="sourceCode" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> compare_models(train_data, test_data):</span>
<span id="cb6-2">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Build adjacency matrix from train data</span></span>
<span id="cb6-3">    train_adj_matrix <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> build_adjacency_matrix(train_data)</span>
<span id="cb6-4">    </span>
<span id="cb6-5">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Random model (aka "The Dart Board Approach")</span></span>
<span id="cb6-6">    random_score <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> evaluate_model(random_recommendations, test_data)</span>
<span id="cb6-7">    </span>
<span id="cb6-8">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Direct associations model (aka "The One-Track Mind")</span></span>
<span id="cb6-9">    direct_score <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> evaluate_model(train_adj_matrix, test_data)</span>
<span id="cb6-10">    </span>
<span id="cb6-11">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Transitive model with best hyperparameters (aka "The Six Degrees of Kevin Bacon")</span></span>
<span id="cb6-12">    transitive_score <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> evaluate_model(</span>
<span id="cb6-13">        expand_adjacency_matrix(train_adj_matrix, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span>best_params),</span>
<span id="cb6-14">        test_data</span>
<span id="cb6-15">    )</span>
<span id="cb6-16">    </span>
<span id="cb6-17">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Random model score: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>random_score<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb6-18">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Direct associations score: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>direct_score<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb6-19">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Transitive model score: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>transitive_score<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb6-20"></span>
<span id="cb6-21">compare_models(train_data, test_data)</span></code></pre></div>
</section>
</section>
<section id="the-secret-sauce-continuous-improvement" class="level2">
<h2 class="anchored" data-anchor-id="the-secret-sauce-continuous-improvement">The Secret Sauce: Continuous Improvement</h2>
<p>Now, here’s what separates the good recommendation systems from the great ones: continuous improvement. Everything we’ve built so far is just the foundation. The real magic happens when you start iterating and refining.</p>
<ol type="1">
<li><p><strong>Fine-tune your clustering:</strong> As you gather more data, you might discover that certain features are more predictive than others. Don’t hesitate to adjust your approach.</p></li>
<li><p><strong>Adjust association weights:</strong> Consider the context of purchases. Perhaps items bought together in the same transaction should carry more weight than those bought by the same customer on different days.</p></li>
<li><p><strong>Optimize hyperparameters:</strong> Regularly revisit your <code>max_order</code> and <code>weight_factor</code> settings. As your data grows and evolves, so too should your model’s parameters.</p></li>
<li><p><strong>Incorporate user feedback:</strong> If customers consistently ignore certain recommendations, use that information to refine your model.</p></li>
<li><p><strong>A/B test rigorously:</strong> Test different versions of your model against each other. Let the data guide your decisions on which approaches work best for your specific use case.</p></li>
</ol>
<p>Remember, the goal isn’t perfection - it’s continuous improvement. Aim to build a system that consistently outperforms random chance, and then focus on making it a little better every day.</p>
<section id="the-real-world-impact-beyond-the-numbers" class="level3">
<h3 class="anchored" data-anchor-id="the-real-world-impact-beyond-the-numbers">The Real-World Impact: Beyond the Numbers</h3>
<p>Now, I won’t give you specific numbers here because, let’s face it, your mileage may vary. But if you’ve done everything right, your transitive model should be outperforming the others like a sports car in a bicycle race. But here’s the real kicker: this isn’t just about better numbers on a spreadsheet. It’s about creating a recommendation system that feels almost eerily intuitive to your customers.</p>
<p>While we’ve covered significant ground in this post, we’ve only scratched the surface of what’s possible. In our next installment, we’ll explore how cutting-edge AI techniques can take our recommendation system to the next level. We’ll delve into methods that can create truly bespoke shopping experiences, predicting not just what a customer might want now, but what they’ll want next. From leveraging deep learning to harnessing the power of contextual bandits, we’ll explore how to create a recommendation engine that doesn’t just react to customer behavior, but anticipates it. It’s the difference between a skilled salesperson and a personal shopping psychic (with a Ph.D.&nbsp;in data science). In the end, that’s what separates a good recommendation system from a great one. It’s not just about predicting what customers want - it’s about inspiring them, delighting them, and yes, maybe even surprising them a little. So go forth, tune those hyperparameters, split that data, and may your conversion rates be ever in your favor!</p>


</section>
</section>

 ]]></description>
  <category>recommenders</category>
  <category>code</category>
  <guid>https://shwetank-kumar.github.io/posts/cold-start/</guid>
  <pubDate>Sat, 24 Aug 2024 07:00:00 GMT</pubDate>
  <media:content url="https://shwetank-kumar.github.io/posts/cold-start/recommender-system-diagram.svg" medium="image" type="image/svg+xml"/>
</item>
<item>
  <title>🌙 AI Afterhours: Top AI Papers for Aug 15 - Aug 20, 2024</title>
  <dc:creator>Shwetank Kumar</dc:creator>
  <link>https://shwetank-kumar.github.io/posts/summary-2024-08-20/ai_paper_summaries.html</link>
  <description><![CDATA[ 





<p>Welcome to this week’s AI Afterhours! Your weekly digest of most upvoted papers in AI. Below is gist of the results, how they got them, and why you should care.</p>
<p>With that, let’s dive into the most exciting AI research from August 15 to August 20, 2024.</p>
<p><strong>LongVILA: Scaling Long-Context Visual Language Models for Long Videos</strong> is tackling the challenge of processing lengthy video content. By employing a novel sharding strategy and 2D-attention mechanism, LongVILA achieves impressive results, including 99.5% accuracy in a needle-in-a-haystack experiment with 1400 frames. This translates to a context length of 274k tokens - a significant leap forward. The system also boasts a 2.1× to 5.7× speedup compared to traditional methods. Why should you care? This breakthrough could revolutionize video understanding, enabling more sophisticated applications in areas like content moderation, video search, and automated video summarization.</p>
<p><a href="https://arxiv.org/pdf/2408.10188v3">arXiv:2408.10188v3</a> 👍33</p>
<p><strong>DeepSeek-Prover-V1.5</strong> is pushing the boundaries of formal theorem proving in Lean 4. By combining reinforcement learning with Monte-Carlo tree search, the model achieves a 63.5% pass rate on the miniF2F-test benchmark and 25.3% on ProofNet. That’s not just an incremental improvement - it’s outperforming GPT-3.5 and GPT-4 on these tasks. The implications? We’re moving closer to AI systems that can assist in complex mathematical proofs, potentially accelerating discoveries in fields ranging from pure mathematics to theoretical physics.</p>
<p><a href="https://arxiv.org/pdf/2408.08152v1">arXiv:2408.08152v1</a> 👍24</p>
<p><strong>MeshFormer: High-Quality Mesh Generation with 3D-Guided Reconstruction Model</strong> is addressing a key challenge in computer vision: generating high-quality 3D meshes from sparse-view images. The model achieves state-of-the-art results, with F-scores of 0.963 and 0.914 on the GSO and OmniObject3D datasets respectively. What’s exciting is that MeshFormer can be trained efficiently using only 8 GPUs, converging in about two days. This could democratize 3D modeling, allowing even novices to create high-quality 3D assets in seconds - a game-changer for industries like gaming, VR, and digital twin technology.</p>
<p><a href="https://arxiv.org/pdf/2408.10198v1">arXiv:2408.10198v1</a> 👍22</p>
<p><strong>Segment Anything with Multiple Modalities (MM-SAM)</strong> extends the capabilities of the Segment Anything Model to non-RGB sensor modalities. The results are impressive: improvements of up to 12.6% mIoU on the MFNet dataset for cross-modal segmentation, and an average improvement of 6.4% mIoU across seven datasets for multi-modal segmentation. This breakthrough could enhance perception systems’ robustness and accuracy in challenging conditions, with potential applications in autonomous vehicles, robotics, and medical imaging.</p>
<p><a href="https://arxiv.org/pdf/2408.09085v1">arXiv:2408.09085v1</a> 👍13</p>
<p><strong>Heavy Labels Out! Dataset Distillation with Label Space Lightening (HeLlO)</strong> tackles the storage costs associated with large-scale dataset distillation. By leveraging pre-trained foundation models and introducing a novel label-lightening approach, HeLlO achieves comparable performance to state-of-the-art methods while using only 0.003% of the original label storage space. This could make large-scale dataset distillation more feasible for widespread adoption, potentially accelerating AI model training and reducing computational resources.</p>
<p><a href="https://arxiv.org/pdf/2408.08201v1">arXiv:2408.08201v1</a> 👍13</p>
<p><strong>ShortCircuit: AlphaZero-Driven Circuit Design</strong> brings deep learning to logic synthesis, achieving a 98% success rate in generating AND-Inverter Graphs (AIGs) from 8-input truth tables. The resulting AIGs are 18.62% smaller than those generated by state-of-the-art tools, with a comparable running time. This approach could revolutionize circuit design, potentially leading to more efficient hardware and faster electronic systems.</p>
<p><a href="https://arxiv.org/pdf/2408.09858v2">arXiv:2408.09858v2</a> 👍11</p>
<p><strong>NeuFlow v2: High-Efficiency Optical Flow Estimation on Edge Devices</strong> achieves a remarkable 10x-70x speedup compared to state-of-the-art methods, while maintaining comparable accuracy. It can run at over 20 FPS on 512x384 resolution images on a Jetson Orin Nano. This breakthrough could enable real-time optical flow estimation on edge devices, with significant implications for robotics, autonomous vehicles, and augmented reality applications.</p>
<p><a href="https://arxiv.org/pdf/2408.10161v2">arXiv:2408.10161v2</a> 👍10</p>
<p><strong>Generative Photomontage</strong> introduces a novel approach to creating high-quality image composites with seamless seams. The method achieves PSNR scores up to 23.44 and LPIPS losses down to 0.104, outperforming existing image blending methods in terms of realism and fidelity. This could revolutionize image manipulation techniques, with applications in fields like digital art, film production, and advertising.</p>
<p><a href="https://arxiv.org/pdf/2408.07116v2">arXiv:2408.07116v2</a> 👍10</p>
<p><strong>Factorized-Dreamer: Training A High-Quality Video Generator with Limited and Low-Quality Data</strong> addresses the challenge of creating high-quality video generators using publicly available, low-quality datasets. The framework achieves competitive scores on various benchmarks, including a visual quality score of 67.12 on the EvalCrafter benchmark. This could democratize high-quality video generation, enabling smaller teams and researchers to create sophisticated video content without access to large, high-quality datasets.</p>
<p><a href="https://arxiv.org/pdf/2408.10119v1">arXiv:2408.10119v1</a> 👍9</p>
<p>And that’s a wrap! See you next week!</p>



 ]]></description>
  <category>Reinforcement Learning</category>
  <category>dataset_distillation</category>
  <category>Cross-Modal Segmentation</category>
  <category>Image Synthesis</category>
  <category>Logic Synthesis</category>
  <category>Optical Flow Estimation</category>
  <category>Visual Language Models</category>
  <category>Text-to-Video Generation</category>
  <category>3D Mesh Generation</category>
  <guid>https://shwetank-kumar.github.io/posts/summary-2024-08-20/ai_paper_summaries.html</guid>
  <pubDate>Tue, 20 Aug 2024 07:00:00 GMT</pubDate>
</item>
</channel>
</rss>
