[
  {
    "objectID": "posts/inference-scaling/index.html#the-fundamentals-of-token-selection-navigating-vast-probability-spaces",
    "href": "posts/inference-scaling/index.html#the-fundamentals-of-token-selection-navigating-vast-probability-spaces",
    "title": "Less Magic, More Math: Why Inference Scaling is the New Black",
    "section": "The Fundamentals of Token Selection: Navigating Vast Probability Spaces",
    "text": "The Fundamentals of Token Selection: Navigating Vast Probability Spaces\nAt its core, inference in a Large Language Model (LLM) is about predicting the next token in a sequence, given the previous tokens. It’s like playing a cosmic game of “what comes next?” where the stakes are coherent communication. LLMs accomplish this feat by learning the underlying distribution of the training data and storing a compressed version of it in their parameters. However, this nominally simple task hides a universe of complexity that would make even the most seasoned computer scientists break out in a cold sweat.\n\nThe Vastness of Possibility\nImagine you’re an LLM with a vocabulary of |V| tokens, trying to generate a sequence of length T. The naive search space for this task is O(|V|^T). To put this into perspective, if you have a modest vocabulary of 50,000 tokens (which is on the small side for modern LLMs) and you’re generating a sequence of just 20 tokens, you’re looking at 50,000^20 possibilities.\nThat’s a number so large it makes the number of atoms in the observable universe look like pocket change. If each possibility were a grain of sand, you’d have enough to build a beach that stretches from here to Alpha Centauri, with enough left over to fill the Mariana Trench. Twice.\nClearly, brute-forcing our way through this cosmic beach of possibilities isn’t feasible, even with the most powerful supercomputers at our disposal. We need a smarter approach.\n\n\nEnter the Decoding Strategies\nSo given these insurmountable odds, how do LLMs figure out what token to generate next? Enter the world of decoding strategies! Depending on the strategies used and the choice of parameters used to tune them you might get text that is coherent, engaging, surprising or a combination there of. Just remember it will always be sampled from the underlying probability distribution that was learned during the training phase.\nLet’s dive into some of these strategies which we will need to understand inference scaling. There are many more which are well covered in [2] and similar articles.\n\n1. Beam Search: The Chess Grandmaster\nBeam search is like a chess grandmaster, always thinking several moves ahead to find the best overall strategy. It explores multiple possible sequences simultaneously, keeping track of the most promising paths.\nAt each step, beam search maintains a set number (let’s call it k) of the most likely partial sequences, known as hypotheses. This allows the model to consider sequences that may start with lower probability tokens but have higher overall probability, potentially leading to better quality outputs than simple greedy search.\nThe beauty of beam search is that it strikes a balance between exploration and exploitation. It’s not just picking the best immediate option, but considering how that choice might pan out in the long run. The final output is the sequence with the highest overall probability.\n# Beam Search Example\ncheckpoint = \"openai-community/gpt2-medium\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint)\n\ninputs = tokenizer(\"The cat sat on the\", return_tensors=\"pt\")\noutputs = model.generate(**inputs, num_beams=5, max_new_tokens=50)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\nImportantly, traditional beam search with a width k reduces our astronomical search space from O(|V|^T) to O(k|V|T). That’s a significant improvement, though still a hefty computational load for real-time applications!\n\n\n2. Multinomial Sampling: The Creative Writer\nIf beam search is our chess grandmaster, multinomial sampling is the free-spirited writer who throws caution to the wind and lets inspiration guide their pen.\nThis method introduces an element of controlled randomness into the text generation process. Unlike greedy search, which always picks the most probable token, multinomial sampling randomly selects the next token based on the probability distribution provided by the model. It’s like rolling a weighted die, where each face represents a possible next word, and the size of each face corresponds to its probability.\nWhy is this important? It allows for more diverse outputs. Every token with a non-zero probability has a chance of being selected, reducing the risk of bland, repetitive text. It’s how AI can surprise us with creative turns of phrase or unexpected (yet coherent) continuations of a prompt.\n# Multinomial Sampling Example\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ncheckpoint = \"openai-community/gpt2-large\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint)\n\ninputs = tokenizer(\"The cat sat on the\", return_tensors=\"pt\")\noutputs = model.generate(**inputs, do_sample=True, num_beams=1, max_new_tokens=50)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n\n\n3. Beam Search Multinomial Sampling: The Best of Both Worlds\nWhat if we could combine the strategic foresight of beam search with the creative spark of multinomial sampling? Enter beam search multinomial sampling, the hybrid approach that aims to give us the best of both worlds.\nThis method maintains multiple hypotheses like beam search, but instead of always choosing the most probable token for each beam, it samples from the probability distribution. It’s as if our chess player occasionally makes a slightly unorthodox move, not because it’s objectively the best, but because it might lead to an interesting game state.\nThe result? Outputs that are both high-quality and diverse, striking a delicate balance between coherence and creativity.\n# Beam Search Multinomial Sampling Example\nfrom transformers import AutoModelForSeq2SeqLM\n\ncheckpoint = \"google-t5/t5-small\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n\ninputs = tokenizer(\"translate English to German: The cat sat on the mat.\", return_tensors=\"pt\")\noutputs = model.generate(**inputs, num_beams=5, do_sample=True, max_new_tokens=50)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n\n\n4. Diverse Beam Search: The Brainstorming Session\nSometimes, we don’t just want one good output – we want several distinctly different options. That’s where diverse beam search comes in. Think of it as a brainstorming session where participants are explicitly told to come up with ideas that are different from each other.\nDiverse beam search divides the beams into groups and applies a diversity penalty to ensure that the outputs from different groups are distinct. Within each group, standard beam search is applied. This approach is particularly useful when you want to generate multiple alternative outputs that are significantly different from each other, not just minor variations.\n# Diverse Beam Search Example\ncheckpoint = \"google/pegasus-xsum\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n\ninputs = tokenizer(\"Summarize: The cat sat on the mat. The dog slept by the fire. The bird sang in the tree.\", return_tensors=\"pt\")\noutputs = model.generate(**inputs, num_beams=10, num_beam_groups=5, diversity_penalty=1.0, max_new_tokens=30)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n\n\n\nThe Art of Balancing\nEach of these decoding strategies offers different trade-offs between output quality, diversity, & computational cost. The choice of strategy depends on the specific requirements of your task and the desired characteristics of the generated text. For example by playing with just a few parameters you can go from Greedy algorithm with extremely repetitive responses to versatile text giving human like responses.\nUnderstanding these strategies is crucial for anyone working with or developing language models. They’re not just technical details – they’re the brushstrokes that determine the final picture painted by our AI artists and to emulate reflection and thoughtfulness by your friendly neighborhood AI which is what we cover in the next section."
  },
  {
    "objectID": "posts/inference-scaling/index.html#in-ais-deep-reflection-where-probability-flirts-with-search-and-models-judge-other-models-pickup-lines",
    "href": "posts/inference-scaling/index.html#in-ais-deep-reflection-where-probability-flirts-with-search-and-models-judge-other-models-pickup-lines",
    "title": "Less Magic, More Math: Why Inference Scaling is the New Black",
    "section": "In AI’s Deep Reflection: Where Probability Flirts with Search and Models Judge Other Models’ Pickup Lines",
    "text": "In AI’s Deep Reflection: Where Probability Flirts with Search and Models Judge Other Models’ Pickup Lines\nFor years, the AI community has been on a bulk-up routine that would make bodybuilders jealous - just feed the model more parameters and data. Inference scaling changes this up by giving AI models a gym membership and a personal trainer instead. It asks the question - instead of always making our models bigger, how do we make them think harder? Let’s dive into the three main approaches for that with a bit more technical detail this time.\n\n1. Parallel Sampling: The Brute Force Charmer\nParallel sampling is a straightforward yet powerful technique in the world of inference scaling. At its core, it’s about generating multiple independent solutions and selecting the best one.\nHow It Works:\nWhen presented with an input, the AI model doesn’t generate just one answer. Instead, it produces N complete, independent answers. This process is akin to running the model N times in parallel, each time generating a full response to the input.\nThe key to this method’s effectiveness lies in its evaluation step. We employ a sophisticated evaluator, an Outcome Reward Model (ORM). This is an AI model, trained to assess the quality of generated answers based on various criteria. The ORM examines each of the N answers and assigns them a score. This scoring isn’t just based on correctness, but can include factors such as the quality of reasoning, clarity of explanation, and adherence to the task requirements.\nOnce all N answers have been scored, we can use a “best-of-N weighted” selection method to choose the final answer. This method is more nuanced than simply selecting the highest-scoring response. Instead, it considers all answers that arrived at the same conclusion and sums their scores. The conclusion with the highest total score is then selected as the final output.\nMathematical Formulation:\nFor a given input x:\n\nGenerate outputs: y₁, y₂, …, yₙ\nScore each output: s₁ = PRM(y₁), s₂ = PRM(y₂), …, sₙ = PRM(yₙ)\nFinal selection: y* = argmax(s₁, s₂, …, sₙ)\n\nIn practice, the selection process might be more complex, involving grouping similar outputs and summing their scores before selecting the highest-scoring group.\nStrengths:\n\nSimple and scalable. Improving results often involves simply increasing N, effectively leveraging additional computational resources to boost performance - a clear trade-off.\nWorks well for easier questions or tasks where generating a large number of attempts is likely to produce at least one high-quality answer.\nWorks especially when the space of possible good answers is relatively large and diverse.\n\nWeaknesses:\n\nComputationally inefficient for complex problems.\nWhile it will eventually find a good solution if N is large enough, it may use significantly more resources than more targeted approaches.\nThe compute costs can become prohibitive if N is set too high, especially for resource-intensive models.\nEach generation is independent, potentially repeating work or mistakes made in other attempts. It doesn’t learn from or build upon partial successes within a single attempt.\n\nAll-in-all it is a simple but brute-force approach to inference scaling. Its straightforward nature makes it an attractive option, especially when dealing with tasks where the generation of multiple diverse attempts is likely to yield at least one high-quality result. However, its effectiveness needs to be balanced against its potential computational costs, especially for more complex tasks or when scaling to very large N.\n\n\n2. Beam Search: The Chess Grandmaster of Inference\nUnlike parallel sampling, which generates complete independent answers, beam search constructs solutions incrementally, making decisions at each step about which partial solutions are most promising to develop further.\nHow It Works:\n\nInitialization: The process begins by generating N initial predictions for the first step of the solution. These represent different starting points for the answer.\nScoring: A Process Reward Model (PRM) is used to score each of these N initial steps. The PRM evaluates the quality and potential of each partial solution. Note that this is also how it is different from an ORM above.\nPruning: Instead of keeping all N initial steps, beam search retains only the top N/M highest-scoring ones, where M is the beam width. This step focuses the search on the most promising partial solutions.\nExpansion: For each of the retained partial solutions, the model generates M new proposals for the next step. This brings the total number of candidates back to N (N/M * M = N).\nIteration: Steps 2-4 are repeated until either a complete solution is reached or a maximum number of rounds is hit.\n\nThis process allows beam search to maintain a diverse set of partially completed solutions, continually evaluating and refining them as it progresses.\nMathematical Formulation:\nAt each step t:\n\nGenerate candidates: C_t = {c₁, c₂, …, cₙ}\nScore candidates: S_t = {PRM(c₁), PRM(c₂), …, PRM(cₙ)}\nKeep top k: B_t = top_k(C_t, S_t, k=N/M)\nExpand: C_{t+1} = ⋃_{b ∈ B_t} expand(b, M)\n\nwhere:\n\nC_t is the set of candidate partial solutions at step t\nS_t is the set of scores for these candidates\nB_t is the set of top-scoring candidates retained\nexpand(b, M) generates M new candidates from partial solution b\n\nStrengths:\n\nComplex problem-solving where the solution is built incrementally.\nNatural language generation tasks requiring coherent, long-form responses.\nMulti-step planning or decision-making processes.\n\nWeaknesses:\n\nThere is potential for pruning prematurely. By discarding lower-scoring partial solutions early, beam search might miss unconventional but ultimately superior solutions that don’t appear promising in their early stages.\nWhile more efficient than exhaustive search, beam search can still be computationally intensive, especially with large beam widths or for problems requiring many steps.\nIts performance can be highly dependent on the choice of N and M. Optimal values may vary significantly between different types of problems.\n\nBeam search is particularly suited for problems where the solution quality depends on a series of interconnected decisions. Its ability to maintain and explore multiple promising solution paths makes it a go-to approach for complex, multi-step reasoning tasks in AI.\n\n\n3. Revision-Based Approach: The Perfectionist’s Dream\nThis approach is focuses on iterative improvement of an initial response. Unlike parallel sampling or beam search, which generate multiple independent answers or explore multiple paths simultaneously, this approach generates a single answer and then repeatedly refines it.\nHow It Works:\n\nInitial Generation: The model produces an initial answer to the given input.\nIterative Refinement: The model is then presented with its previous answer(s) along with the original input and asked to generate an improved version.\nRepetition: This process of refinement is repeated for a predetermined number of iterations or until some convergence criterion is met.\nSelection: Once all iterations are complete, the best version is selected using either an Outcome Reward Model (ORM) or a majority voting mechanism.\n\nMathematical Formulation:\n1. Initial generation: y₀ = model(x)\n2. for i = 1 to k:\n      yᵢ = model(x, y₀, y₁, ..., yᵢ₋₁)\n3. Final selection:\n      y* = argmax(ORM(y₀), ORM(y₁), ..., ORM(yₖ))\n      or\n      y* = mode(y₀, y₁, ..., yₖ)\nWhere:\n\nx is the input\nyᵢ is the i-th revision\nk is the total number of revisions\nORM is the Outcome Reward Model\n\nKey Components:\n\nRevision Model: This is typically a fine-tuned version of the base language model, specifically trained to improve upon previous answers. The training process involves exposing the model to sequences of increasingly better answers to the same question.\nOutcome Reward Model (ORM): This is a separate model trained to evaluate the quality of complete answers. It’s used to score each revision and select the best one.\nMajority Voting: An alternative to ORM, this method selects the most common answer among all revisions. It’s particularly useful when multiple independent revision sequences are generated.\n\nStrengths:\n\nThis approach allows for gradual refinement of the answer, potentially leading to high-quality results, especially for medium-difficulty questions.\nIt can be more computationally efficient than generating multiple complete answers, as in parallel sampling.\nBy considering previous attempts, the model can build upon its own insights and correct its mistakes.\n\nWeaknesses:\n\nThis approach may get stuck in a local optimum, repeatedly making similar refinements without considering radically different approaches.\nThe quality of the final output can be heavily influenced by the quality of the initial answer.\nTraining a model to effectively revise its own work is a complex task, requiring sophisticated training data and techniques.\n\nRevision-based approach represents a unique strategy in inference scaling, mimicking a facet of the human creative process of drafting and refining ideas. As I have mentioned elsewhere that I don’t think that’s the whole story as far as human creativity is concerned. However, the ability to iteratively improve upon its own output makes for a powerful tool in the AI toolkit, especially for tasks where quality can be enhanced through careful refinement and consideration of previous attempts."
  },
  {
    "objectID": "posts/inference-scaling/index.html#conclusion-the-art-of-thinking-harder-not-just-bigger",
    "href": "posts/inference-scaling/index.html#conclusion-the-art-of-thinking-harder-not-just-bigger",
    "title": "Less Magic, More Math: Why Inference Scaling is the New Black",
    "section": "Conclusion: The Art of Thinking Harder, Not Just Bigger",
    "text": "Conclusion: The Art of Thinking Harder, Not Just Bigger\nThis post has become way longer than originally intended so thanks for staying with it. For more details on performance metrics, and specific use-cases for each technique, I highly recommend checking out reference [3] below.\nAs I wrap up this journey, it’s clear to me that we are amid a paradigm shift (no not AGI). We’re moving from an era of “bigger is better” during training to a more nuanced approach that folds in clever inference algorithms as well in which ORM and PRM models are the unsung heroes. Their quality will set the upper bound on the quality of your inference scaling strategy.\nAnd while these techniques will enable us to use copilots capable of more nuanced approaches to the problem this shift also means what earlier used to be a heavy training related capital expense borne by the model provider will turn more into an operational expense borne by the user."
  },
  {
    "objectID": "posts/inference-scaling/index.html#references",
    "href": "posts/inference-scaling/index.html#references",
    "title": "Less Magic, More Math: Why Inference Scaling is the New Black",
    "section": "References",
    "text": "References\n\nThe illustrated transformer\nText generation strategies\nScaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters"
  },
  {
    "objectID": "posts/cold-start/index.html#the-problem-e-commerce-is-a-jungle-and-your-customers-are-lost",
    "href": "posts/cold-start/index.html#the-problem-e-commerce-is-a-jungle-and-your-customers-are-lost",
    "title": "The Surprisingly Lucrative Journey of Bootstrapping a Brand Recommender System: From Chaos to Cash",
    "section": "The Problem: E-commerce Is a Jungle (And Your Customers Are Lost)",
    "text": "The Problem: E-commerce Is a Jungle (And Your Customers Are Lost)\nImagine you’ve just launched an e-commerce platform. It’s sleek, efficient, and boasts an impressive array of brands. Initially, you’re confident in its success. However, user feedback quickly reveals a common challenge in the e-commerce world:\n\n“I’m overwhelmed by the number of options.”\n“The recommendations don’t seem relevant to my interests.”\n“I can’t find products that match my specific needs.”\n\nThese concerns are not uncommon. In the realm of online retail, the balance between variety and accessibility is crucial. Insufficient options can leave customers feeling limited, while an overabundance can lead to decision fatigue. This phenomenon, often referred to as the “paradox of choice,” can significantly impact user experience and, consequently, your conversion rates.\nThis, my friends, is why a good recommendation system is invaluable. It’s like having a wise, all-knowing friend who gently guides your customers to their next favorite purchase. And today, we’re going to build that friend from scratch."
  },
  {
    "objectID": "posts/cold-start/index.html#step-0-the-oh-crap-we-have-no-data-phase---totally-random-model",
    "href": "posts/cold-start/index.html#step-0-the-oh-crap-we-have-no-data-phase---totally-random-model",
    "title": "The Surprisingly Lucrative Journey of Bootstrapping a Brand Recommender System: From Chaos to Cash",
    "section": "Step 0: The “Oh Crap, We Have No Data” Phase - Totally Random Model",
    "text": "Step 0: The “Oh Crap, We Have No Data” Phase - Totally Random Model\nLet’s start at the beginning (a very good place to start…). You’ve just launched your e-commerce platform. It looks great, it works smoothly, but there’s one big problem: you have no data. Your recommendation system is a blank slate. What do you do?\nTake a deep breath and repeat after me: “Random is better than nothing.” Now, I know what you’re thinking. “But this is just throwing darts blindfolded! How is this helping anyone?” And you’re right, it’s not ideal. But here’s the secret: it’s not about being perfect; it’s about starting the flywheel.\nEvery time a user sees a random recommendation, you’re gathering data. Maybe they ignore it (data point!). Maybe they click on it (data point!). Maybe they buy it (cha-ching and data point!). Every product interaction (or lack thereof) is a breadcrumb that will lead you out of the data desert.\nLet’s whip up a quick Python function to generate random recommendations:\nimport random\n\ndef get_random_recommendations(all_brands, n=5):\n    return random.sample(all_brands, min(n, len(all_brands)))\n\n# Example usage\nall_brands = df['recommending_brand'].unique().tolist()\nprint(get_random_recommendations(all_brands))\nPro tip: While you’re showing random recommendations, make sure you’re logging EVERYTHING. Every view, every click, every purchase. This data will be worth its weight in gold later on. Trust me, future you will thank present you for this foresight. And while you are at it do make sure that the data is high quality. Algorithms are fickle and state of the art on those changes every week – nay day! But poor quality data once logged sets the ceiling on what you can do with it."
  },
  {
    "objectID": "posts/cold-start/index.html#step-1-the-we-have-some-data-but-its-not-about-users-phase---feature-based-clustering-model",
    "href": "posts/cold-start/index.html#step-1-the-we-have-some-data-but-its-not-about-users-phase---feature-based-clustering-model",
    "title": "The Surprisingly Lucrative Journey of Bootstrapping a Brand Recommender System: From Chaos to Cash",
    "section": "Step 1: The “We Have Some Data, But It’s Not About Users” Phase - Feature-Based Clustering Model",
    "text": "Step 1: The “We Have Some Data, But It’s Not About Users” Phase - Feature-Based Clustering Model\nAlright, so you’ve been running your random recommendation engine for a while. You’ve got some sales, you’ve got some brand data, but you still don’t have enough user interaction data to build a proper collaborative filtering system. Don’t worry, we’re going to make lemonade out of these lemons.\nEnter: Feature-Based Clustering.\nNow, gather ’round, because I’m about to share a secret: brands, like people, have personalities. And just like you wouldn’t set up your quiet, bookish friend with your party-animal cousin (trust me, I’ve made that mistake), you shouldn’t be recommending wildly dissimilar brands to your users.\nLet’s create a simple example using K-means clustering. Don’t let the fancy name scare you - it’s just a way of grouping similar things together.\nfrom sklearn.cluster import KMeans\nimport numpy as np\n\n# Example brand features (price, target_age, sportiness)\nbrand_features = {\n    \"Nike\": [80, 25, 9],\n    \"Adidas\": [75, 30, 8],\n    \"Puma\": [60, 28, 7],\n    \"Reebok\": [65, 35, 6],\n    \"Under Armour\": [70, 27, 9],\n    \"New Balance\": [85, 40, 5],\n    \"Asics\": [90, 35, 8],\n    \"Converse\": [55, 22, 3],\n    \"Vans\": [50, 20, 2],\n    \"Skechers\": [45, 45, 4]\n}\n\ndef cluster_brands(brand_features, n_clusters=3):\n    brands = list(brand_features.keys())\n    features = np.array(list(brand_features.values()))\n    \n    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n    clusters = kmeans.fit_predict(features)\n    \n    brand_clusters = {brand: cluster for brand, cluster in zip(brands, clusters)}\n    return brand_clusters\n\nbrand_clusters = cluster_brands(brand_features)\nprint(brand_clusters)\n\ndef get_cluster_recommendations(purchased_brand, brand_clusters, n=5):\n    cluster = brand_clusters[purchased_brand]\n    cluster_brands = [brand for brand, c in brand_clusters.items() if c == cluster]\n    return random.sample(cluster_brands, min(n, len(cluster_brands)))\n\n# Example usage\npurchased_brand = \"Nike\"\nprint(get_cluster_recommendations(purchased_brand, brand_clusters))\nLet me share a real-world example that illustrates the power of this approach. Back in the day I worked with a startup that had a range of products - everything from $5 friendship bracelets that screamed “summer camp chic” to $500 mid-century modern chairs that whispered “I have a trust fund.” Their recommendation system? About as sophisticated as a Magic 8-Ball with a hangover. It was the digital equivalent of that one clueless sales associate who tries to upsell you a tuxedo when you’re shopping for gym shorts.\nThe result? Their conversion rate was lower than my undergrad GPA (and trust me, that’s saying something). They addressed this by implementing a clustering system based on various product attributes. We’re talking price, category, style, color palette - if it could be quantified, they clustered it. And boom! Faster than you can say “artisanal hand-knitted cat sweater,” their click-through rates went through the roof! Their engineering blog was practically giddy with excitement (in that restrained, data-scientist kind of way) about the boost in overall sales.\nNow a good data science algorithm needs to do at least two things (and definitely the second of these two): 1. It needs needs to improve a KPI - in this case conversions, although it can be anything. (e.g. if you are a Ferengi it will invariably be profits as laid out in the Ferengi Rules of Acquisition) 2. Regardless of whether it does 1. it needs to light the way to how you might improve the KPI in the future.\nIn this case, it wasn’t just the algorithm doing the work automatically, but the entire system we built around it. We maintained a degree of randomness in our recommendations to continue generating new data and learning from it. It’s like saying, “We know you’re interested in vintage decor, but have you considered this modern minimalist piece?” This approach not only serves immediate customer interests but also introduces them to new products they might enjoy. The lesson here? Even a relatively simple clustering approach can significantly improve your recommendation engine. It’s not about having the fanciest algorithm on the block; it’s about understanding your customers and not trying to sell snowshoes to someone shopping for flip-flops. So, whether you’re dealing with luxury items and budget options, or niche products and mainstream goods, remember: cluster wisely, but maintain some variety. Your conversion rates (and your customers) will thank you."
  },
  {
    "objectID": "posts/cold-start/index.html#step-2-the-now-were-cooking-with-gas-phase---purchase-based-association-model",
    "href": "posts/cold-start/index.html#step-2-the-now-were-cooking-with-gas-phase---purchase-based-association-model",
    "title": "The Surprisingly Lucrative Journey of Bootstrapping a Brand Recommender System: From Chaos to Cash",
    "section": "Step 2: The “Now We’re Cooking with Gas” Phase - Purchase-Based Association Model",
    "text": "Step 2: The “Now We’re Cooking with Gas” Phase - Purchase-Based Association Model\nAlright, my data-hungry friends, we’ve arrived at the juicy part. You’ve been diligently collecting user interaction data (you have, haven’t you?), and now it’s time to put it to use. We’re going to build a purchase-based association model.\nThis is where the magic really starts to happen. We’re going to create a system that understands that people who buy brand A often buy brand B, even if we don’t know why. It’s like being a really good matchmaker without understanding the intricacies of human psychology.\nLet’s cook up a simple association model:\nfrom collections import defaultdict\n\ndef build_association_model(purchase_data):\n    associations = defaultdict(lambda: defaultdict(int))\n    for purchase in purchase_data:\n        for i, brand1 in enumerate(purchase):\n            for brand2 in purchase[i+1:]:\n                associations[brand1][brand2] += 1\n                associations[brand2][brand1] += 1\n    return associations\n\n# Example purchase data\npurchase_data = [\n    [\"Nike\", \"Adidas\"],\n    [\"Nike\", \"Under Armour\"],\n    [\"Adidas\", \"Puma\"],\n    [\"Puma\", \"Reebok\"],\n    [\"Nike\", \"Converse\"],\n    [\"Vans\", \"Converse\"],\n    [\"New Balance\", \"Asics\"],\n    [\"Skechers\", \"New Balance\"]\n]\n\nassociation_model = build_association_model(purchase_data)\n\ndef get_associated_brands(brand, association_model, n=5):\n    associated = sorted(association_model[brand].items(), key=lambda x: x[1], reverse=True)\n    return [b for b, _ in associated[:n]]\n\n# Example usage\npurchased_brand = \"Nike\"\nprint(get_associated_brands(purchased_brand, association_model))\nNow, let me tell you why this is a game-changer. Consider the case of Amazon, the e-commerce giant. In their early days, they primarily sold books. But as they expanded into other product categories, they faced a massive challenge: how to effectively cross-sell across these diverse categories? Their solution was to implement a sophisticated association model, much like the one we’ve just built (though admittedly, theirs was far more complex). This “item-to-item collaborative filtering” approach, as they called it, allowed them to say, “Customers who bought this item also bought…”\nThe impact was significant. According to a paper published by Amazon’s engineers in 2003 titled “Amazon.com Recommendations: Item-to-Item Collaborative Filtering”, this recommendation system offered substantial advantages over traditional collaborative filtering techniques as it could:\n\nHandle a massive scale of data - tens of millions of customers and millions of catalog items.\nProduce high-quality recommendations in real-time, scaling well to sudden spikes in traffic.\nRecommend across diverse product categories, from books to electronics to clothing.\n\nWhile the paper doesn’t provide specific sales figures, it does mention that Amazon’s recommendation system significantly improved click-through and conversion rates compared to untargeted content such as top sellers."
  },
  {
    "objectID": "posts/cold-start/index.html#step-3-the-six-degrees-of-kevin-bacon-phase---transitive-association-model",
    "href": "posts/cold-start/index.html#step-3-the-six-degrees-of-kevin-bacon-phase---transitive-association-model",
    "title": "The Surprisingly Lucrative Journey of Bootstrapping a Brand Recommender System: From Chaos to Cash",
    "section": "Step 3: The “Six Degrees of Kevin Bacon” Phase - Transitive Association Model",
    "text": "Step 3: The “Six Degrees of Kevin Bacon” Phase - Transitive Association Model\nEver played “Six Degrees of Kevin Bacon”? Well, we’re about to do something similar with our brands, and it’s going to blow your recommendation socks off.\nDirect associations are great, but they’re limited. What if we could create a web of associations, where brand A is connected to brand B, which is connected to brand C, creating an indirect link between A and C? It’s like being at a party where your network explodes as friends introduce you to their friends.\nBy considering second-order and third-order connections, we can uncover hidden relationships in our data, leading to nuanced and unexpected recommendations. Here’s how to do it in &lt; 10 lines of code:\ndef expand_adjacency_matrix(adj_matrix, max_order=5, weight_factor=0.5):\n    expanded_matrix = adj_matrix.copy()\n    current_matrix = normalize_matrix(adj_matrix)\n\n    for order in range(2, max_order + 1):\n        current_matrix = current_matrix.dot(normalize_matrix(adj_matrix))\n        weighted_connections = current_matrix * (weight_factor ** (order - 1))\n        expanded_matrix += weighted_connections\n\n    return normalize_matrix(expanded_matrix)\nLet’s break this down:\n\nWe start with our original adjacency matrix of direct brand connections.\nWe calculate higher-order connections up to a specified maximum (default 5, because six degrees of separation, right?).\nHere’s the clever bit: we apply a weight factor. Each higher-order connection gets reduced weight because your friend’s friend’s friend’s opinion shouldn’t count as much as your direct friend’s.\nWe add all these weighted connections to our original matrix, creating a rich tapestry of brand relationships.\nFinally, we normalize everything to keep our numbers manageable.\n\nThe result? A supercharged adjacency matrix capturing not just direct relationships, but a whole network of indirect connections.\nNow, imagine a niche online bookstore specializing in obscure academic texts. Their inventory is so specific that direct associations are as rare as a first-edition Gutenberg Bible at a yard sale.\nA customer buys “The Mating Habits of 12th Century Mongolian Horses” (yes, I’m having fun with these titles). In a traditional system, we’d be stuck. But with our transitive associations, we might recommend “The Economic Impact of Horse Trading in Medieval Asia”, even if no one had ever purchased these together. Our system could find a chain of associations linking them through other related books.\nThis approach enables academics to joyfully tumble down rabbit holes of related obscure topics, much to the store owner’s delight. It illustrates the power of transitive associations, especially for businesses with niche catalogs or sparse collaborative filtering matrices. We’re transforming a sparse matrix into a dense one, uncovering “hidden” connections that create a discovery engine to surprise and delight customers, potentially leading them down purchasing paths they never knew existed."
  },
  {
    "objectID": "posts/cold-start/index.html#step-4-fine-tuning-the-magic---hyperparameters-and-evaluation",
    "href": "posts/cold-start/index.html#step-4-fine-tuning-the-magic---hyperparameters-and-evaluation",
    "title": "The Surprisingly Lucrative Journey of Bootstrapping a Brand Recommender System: From Chaos to Cash",
    "section": "Step 4: Fine-Tuning the Magic - Hyperparameters and Evaluation",
    "text": "Step 4: Fine-Tuning the Magic - Hyperparameters and Evaluation\nAlright, data enthusiasts, we’ve built our transitive association model, but now it’s time to give it that extra polish. Think of it as tuning a high-performance engine - we need to adjust the nitrous levels just right.\n\nHyperparameter Tuning: Finding the Sweet Spot\nRemember our expand_adjacency_matrix function? It comes with two key hyperparameters:\n\nmax_order: How far down the rabbit hole of connections we’re willing to go\nweight_factor: Our trust factor for the friend of a friend of a friend\n\nThese aren’t just arbitrary numbers we pulled out of a magician’s hat. They’re the secret sauce that can make or break our recommendations.\nLet’s take a closer look at weight_factor. Set it too high, and you might end up recommending winter parkas to someone shopping for swimwear. Set it too low, and you’re barely scratching the surface of potential connections.\nSo how do we find the Goldilocks zone? Enter: hyperparameter tuning. It’s like finding the perfect recipe, but instead of slurping soup, we’re crunching numbers.\ndef evaluate_model(adj_matrix, test_data, max_order, weight_factor):\n    expanded_matrix = expand_adjacency_matrix(adj_matrix, max_order, weight_factor)\n    return calculate_hit_rate(expanded_matrix, test_data)\n\n# Grid search for best hyperparameters\nbest_score = 0\nbest_params = {}\n\nfor max_order in range(2, 7):\n    for weight_factor in [0.1, 0.3, 0.5, 0.7, 0.9]:\n        score = evaluate_model(train_adj_matrix, test_data, max_order, weight_factor)\n        if score &gt; best_score:\n            best_score = score\n            best_params = {'max_order': max_order, 'weight_factor': weight_factor}\n\nprint(f\"Best parameters: {best_params}\")\nprint(f\"Best score: {best_score}\")\n\n\nModel Evaluation: Separating the Wheat from the Chaff\nNow, how do we know if our model is actually any good? We can’t just take it out for a test drive on the same roads we built it on. That’s where train-test splits come in handy.\nHere’s our game plan:\n\nSplit your data into training and test sets. Think of it as studying for an exam (training) and then taking the final (testing).\nBuild your adjacency matrix using only the training data.\nUse your tuned model to make predictions on the test set.\nCompare your transitive model against simpler approaches, like random recommendations or direct associations only.\n\nLet’s see how it’s done:\ndef compare_models(train_data, test_data):\n    # Build adjacency matrix from train data\n    train_adj_matrix = build_adjacency_matrix(train_data)\n    \n    # Random model (aka \"The Dart Board Approach\")\n    random_score = evaluate_model(random_recommendations, test_data)\n    \n    # Direct associations model (aka \"The One-Track Mind\")\n    direct_score = evaluate_model(train_adj_matrix, test_data)\n    \n    # Transitive model with best hyperparameters (aka \"The Six Degrees of Kevin Bacon\")\n    transitive_score = evaluate_model(\n        expand_adjacency_matrix(train_adj_matrix, **best_params),\n        test_data\n    )\n    \n    print(f\"Random model score: {random_score}\")\n    print(f\"Direct associations score: {direct_score}\")\n    print(f\"Transitive model score: {transitive_score}\")\n\ncompare_models(train_data, test_data)"
  },
  {
    "objectID": "posts/cold-start/index.html#the-secret-sauce-continuous-improvement",
    "href": "posts/cold-start/index.html#the-secret-sauce-continuous-improvement",
    "title": "The Surprisingly Lucrative Journey of Bootstrapping a Brand Recommender System: From Chaos to Cash",
    "section": "The Secret Sauce: Continuous Improvement",
    "text": "The Secret Sauce: Continuous Improvement\nNow, here’s what separates the good recommendation systems from the great ones: continuous improvement. Everything we’ve built so far is just the foundation. The real magic happens when you start iterating and refining.\n\nFine-tune your clustering: As you gather more data, you might discover that certain features are more predictive than others. Don’t hesitate to adjust your approach.\nAdjust association weights: Consider the context of purchases. Perhaps items bought together in the same transaction should carry more weight than those bought by the same customer on different days.\nOptimize hyperparameters: Regularly revisit your max_order and weight_factor settings. As your data grows and evolves, so too should your model’s parameters.\nIncorporate user feedback: If customers consistently ignore certain recommendations, use that information to refine your model.\nA/B test rigorously: Test different versions of your model against each other. Let the data guide your decisions on which approaches work best for your specific use case.\n\nRemember, the goal isn’t perfection - it’s continuous improvement. Aim to build a system that consistently outperforms random chance, and then focus on making it a little better every day.\n\nThe Real-World Impact: Beyond the Numbers\nNow, I won’t give you specific numbers here because, let’s face it, your mileage may vary. But if you’ve done everything right, your transitive model should be outperforming the others like a sports car in a bicycle race. But here’s the real kicker: this isn’t just about better numbers on a spreadsheet. It’s about creating a recommendation system that feels almost eerily intuitive to your customers.\nWhile we’ve covered significant ground in this post, we’ve only scratched the surface of what’s possible. In our next installment, we’ll explore how cutting-edge AI techniques can take our recommendation system to the next level. We’ll delve into methods that can create truly bespoke shopping experiences, predicting not just what a customer might want now, but what they’ll want next. From leveraging deep learning to harnessing the power of contextual bandits, we’ll explore how to create a recommendation engine that doesn’t just react to customer behavior, but anticipates it. It’s the difference between a skilled salesperson and a personal shopping psychic (with a Ph.D. in data science). In the end, that’s what separates a good recommendation system from a great one. It’s not just about predicting what customers want - it’s about inspiring them, delighting them, and yes, maybe even surprising them a little. So go forth, tune those hyperparameters, split that data, and may your conversion rates be ever in your favor!"
  },
  {
    "objectID": "favorite-eggheads.html#favorite-topics-and-experts",
    "href": "favorite-eggheads.html#favorite-topics-and-experts",
    "title": "Shwetank Kumar",
    "section": "Favorite topics and experts",
    "text": "Favorite topics and experts\n\nAI tutorials - Andrej Karpathy\nPublic company valuations - Ashwath Damodaran\nFinancial systems - Patrick McKenzie"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "&lt;style type=\"text/css\"&gt;\n/* LOADER */\n.ml-form-embedSubmitLoad {\n  display: inline-block;\n  width: 20px;\n  height: 20px;\n}\n\n.g-recaptcha {\ntransform: scale(1);\n-webkit-transform: scale(1);\ntransform-origin: 0 0;\n-webkit-transform-origin: 0 0;\nheight: 20px;\n}\n\n.sr-only {\n  position: absolute;\n  width: 1px;\n  height: 1px;\n  padding: 0;\n  margin: -1px;\n  overflow: hidden;\n  clip: rect(0,0,0,0);\n  border: 0;\n}\n\n.ml-form-embedSubmitLoad:after {\n  content: \" \";\n  display: block;\n  width: 11px;\n  height: 11px;\n  margin: 1px;\n  border-radius: 50%;\n  border: 4px solid #fff;\nborder-color: #ffffff #ffffff #ffffff transparent;\nanimation: ml-form-embedSubmitLoad 1.2s linear infinite;\n}\n@keyframes ml-form-embedSubmitLoad {\n  0% {\n  transform: rotate(0deg);\n  }\n  100% {\n  transform: rotate(360deg);\n  }\n}\n  #mlb2-18937141.ml-form-embedContainer {\n    box-sizing: border-box;\n    display: table;\n    margin: 0 auto;\n    position: static;\n    width: 100% !important;\n  }\n  #mlb2-18937141.ml-form-embedContainer h4,\n  #mlb2-18937141.ml-form-embedContainer p,\n  #mlb2-18937141.ml-form-embedContainer span,\n  #mlb2-18937141.ml-form-embedContainer button {\n    text-transform: none !important;\n    letter-spacing: normal !important;\n  }\n  #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper {\n    background-color: #f6f6f6;\n    \n    border-width: 0px;\n    border-color: transparent;\n    border-radius: 4px;\n    border-style: solid;\n    box-sizing: border-box;\n    display: inline-block !important;\n    margin: 0;\n    padding: 0;\n    position: relative;\n          }\n  #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper.embedPopup,\n  #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper.embedDefault { width: 400px; }\n  #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper.embedForm { max-width: 400px; width: 100%; }\n  #mlb2-18937141.ml-form-embedContainer .ml-form-align-left { text-align: left; }\n  #mlb2-18937141.ml-form-embedContainer .ml-form-align-center { text-align: center; }\n  #mlb2-18937141.ml-form-embedContainer .ml-form-align-default { display: table-cell !important; vertical-align: middle !important; text-align: center !important; }\n  #mlb2-18937141.ml-form-embedContainer .ml-form-align-right { text-align: right; }\n  #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedHeader img {\n    border-top-left-radius: 4px;\n    border-top-right-radius: 4px;\n    height: auto;\n    margin: 0 auto !important;\n    max-width: 100%;\n    width: undefinedpx;\n  }\n  #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody,\n  #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-successBody {\n    padding: 20px 20px 0 20px;\n  }\n  #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody.ml-form-embedBodyHorizontal {\n    padding-bottom: 0;\n  }\n  #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-embedContent,\n  #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-successBody .ml-form-successContent {\n    text-align: left;\n    margin: 0 0 20px 0;\n  }\n  #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-embedContent h4,\n  #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-successBody .ml-form-successContent h4 {\n    color: #000000;\n    font-family: 'Open Sans', Arial, Helvetica, sans-serif;\n    font-size: 30px;\n    font-weight: 400;\n    margin: 0 0 10px 0;\n    text-align: left;\n    word-break: break-word;\n  }\n  #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-embedContent p,\n  #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-successBody .ml-form-successContent p {\n    color: #000000;\n    font-family: 'Open Sans', Arial, Helvetica, sans-serif;\n    font-size: 14px;\n    font-weight: 400;\n    line-height: 20px;\n    margin: 0 0 10px 0;\n    text-align: left;\n  }\n  #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-embedContent ul,\n  #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-embedContent ol,\n  #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-successBody .ml-form-successContent ul,\n  #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-successBody .ml-form-successContent ol {\n    color: #000000;\n    font-family: 'Open Sans', Arial, Helvetica, sans-serif;\n    font-size: 14px;\n  }\n  #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-embedContent ol ol,\n  #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-successBody .ml-form-successContent ol ol {\n    list-style-type: lower-alpha;\n  }\n  #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-embedContent ol ol ol,\n  #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-successBody .ml-form-successContent ol ol ol {\n    list-style-type: lower-roman;\n  }\n  #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-embedContent p a,\n  #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-successBody .ml-form-successContent p a {\n    color: #000000;\n    text-decoration: underline;\n  }\n\n  #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-block-form .ml-field-group {\n    text-align: left!important;\n  }\n\n  #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-block-form .ml-field-group label {\n    margin-bottom: 5px;\n    color: #333333;\n    font-size: 14px;\n    font-family: 'Open Sans', Arial, Helvetica, sans-serif;\n    font-weight: bold; font-style: normal; text-decoration: none;;\n    display: inline-block;\n    line-height: 20px;\n  }\n  #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-embedContent p:last-child,\n  #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-successBody .ml-form-successContent p:last-child {\n    margin: 0;\n  }\n  #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody form {\n    margin: 0;\n    width: 100%;\n  }\n  #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-formContent,\n  #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-checkboxRow {\n    margin: 0 0 20px 0;\n    width: 100%;\n  }\n  #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-checkboxRow {\n    float: left;\n  }\n  #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-formContent.horozintalForm {\n    margin: 0;\n    padding: 0 0 20px 0;\n    width: 100%;\n    height: auto;\n    float: left;\n  }\n  #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-fieldRow {\n    margin: 0 0 10px 0;\n    width: 100%;\n  }\n  #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-fieldRow.ml-last-item {\n    margin: 0;\n  }\n  #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-fieldRow.ml-formfieldHorizintal {\n    margin: 0;\n  }\n  #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-fieldRow input {\n    background-color: #ffffff !important;\n    color: #333333 !important;\n    border-color: #cccccc;\n    border-radius: 4px !important;\n    border-style: solid !important;\n    border-width: 1px !important;\n    font-family: 'Open Sans', Arial, Helvetica, sans-serif;\n    font-size: 14px !important;\n    height: auto;\n    line-height: 21px !important;\n    margin-bottom: 0;\n    margin-top: 0;\n    margin-left: 0;\n    margin-right: 0;\n    padding: 10px 10px !important;\n    width: 100% !important;\n    box-sizing: border-box !important;\n    max-width: 100% !important;\n  }\n  #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-fieldRow input::-webkit-input-placeholder,\n  #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-horizontalRow input::-webkit-input-placeholder { color: #333333; }\n\n  #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-fieldRow input::-moz-placeholder,\n  #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-horizontalRow input::-moz-placeholder { color: #333333; }\n\n  #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-fieldRow input:-ms-input-placeholder,\n  #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-horizontalRow input:-ms-input-placeholder { color: #333333; }\n\n  #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-fieldRow input:-moz-placeholder,\n  #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-horizontalRow input:-moz-placeholder { color: #333333; }\n\n  #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-fieldRow textarea, #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-horizontalRow textarea {\n    background-color: #ffffff !important;\n    color: #333333 !important;\n    border-color: #cccccc;\n    border-radius: 4px !important;\n    border-style: solid !important;\n    border-width: 1px !important;\n    font-family: 'Open Sans', Arial, Helvetica, sans-serif;\n    font-size: 14px !important;\n    height: auto;\n    line-height: 21px !important;\n    margin-bottom: 0;\n    margin-top: 0;\n    padding: 10px 10px !important;\n    width: 100% !important;\n    box-sizing: border-box !important;\n    max-width: 100% !important;\n  }\n\n  #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-fieldRow .custom-radio .custom-control-label::before, #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-horizontalRow .custom-radio .custom-control-label::before, #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-fieldRow .custom-checkbox .custom-control-label::before, #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-horizontalRow .custom-checkbox .custom-control-label::before, #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-embedPermissions .ml-form-embedPermissionsOptionsCheckbox .label-description::before, #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-interestGroupsRow .ml-form-interestGroupsRowCheckbox .label-description::before, #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-checkboxRow .label-description::before {\n      border-color: #cccccc!important;\n      background-color: #ffffff!important;\n  }\n\n  #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-fieldRow input.custom-control-input[type=\"checkbox\"]{\n    box-sizing: border-box;\n    padding: 0;\n    position: absolute;\n    z-index: -1;\n    opacity: 0;\n    margin-top: 5px;\n    margin-left: -1.5rem;\n    overflow: visible;\n  }\n\n  #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-fieldRow .custom-checkbox .custom-control-label::before, #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-horizontalRow .custom-checkbox .custom-control-label::before, #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-embedPermissions .ml-form-embedPermissionsOptionsCheckbox .label-description::before, #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-interestGroupsRow .ml-form-interestGroupsRowCheckbox .label-description::before, #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-checkboxRow .label-description::before {\n    border-radius: 4px!important;\n  }\n\n\n  #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-checkboxRow input[type=checkbox]:checked~.label-description::after, #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-embedPermissions .ml-form-embedPermissionsOptionsCheckbox input[type=checkbox]:checked~.label-description::after, #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-fieldRow .custom-checkbox .custom-control-input:checked~.custom-control-label::after, #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-horizontalRow .custom-checkbox .custom-control-input:checked~.custom-control-label::after, #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-interestGroupsRow .ml-form-interestGroupsRowCheckbox input[type=checkbox]:checked~.label-description::after {\n    background-image: url(\"data:image/svg+xml,%3csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 8 8'%3e%3cpath fill='%23fff' d='M6.564.75l-3.59 3.612-1.538-1.55L0 4.26 2.974 7.25 8 2.193z'/%3e%3c/svg%3e\");\n  }\n\n  #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-fieldRow .custom-radio .custom-control-input:checked~.custom-control-label::after, #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-fieldRow .custom-radio .custom-control-input:checked~.custom-control-label::after {\n    background-image: url(\"data:image/svg+xml,%3csvg xmlns='http://www.w3.org/2000/svg' viewBox='-4 -4 8 8'%3e%3ccircle r='3' fill='%23fff'/%3e%3c/svg%3e\");\n  }\n\n  #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-fieldRow .custom-radio .custom-control-input:checked~.custom-control-label::before, #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-horizontalRow .custom-radio .custom-control-input:checked~.custom-control-label::before, #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-fieldRow .custom-checkbox .custom-control-input:checked~.custom-control-label::before, #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-horizontalRow .custom-checkbox .custom-control-input:checked~.custom-control-label::before, #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-embedPermissions .ml-form-embedPermissionsOptionsCheckbox input[type=checkbox]:checked~.label-description::before, #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-interestGroupsRow .ml-form-interestGroupsRowCheckbox input[type=checkbox]:checked~.label-description::before, #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-checkboxRow input[type=checkbox]:checked~.label-description::before  {\n      border-color: #000000!important;\n      background-color: #000000!important;\n  }\n\n  #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-fieldRow .custom-radio .custom-control-label::before, #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-horizontalRow .custom-radio .custom-control-label::before, #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-fieldRow .custom-radio .custom-control-label::after, #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-horizontalRow .custom-radio .custom-control-label::after, #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-fieldRow .custom-checkbox .custom-control-label::before, #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-fieldRow .custom-checkbox .custom-control-label::after, #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-horizontalRow .custom-checkbox .custom-control-label::before, #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-horizontalRow .custom-checkbox .custom-control-label::after {\n       top: 2px;\n       box-sizing: border-box;\n  }\n\n  #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-embedPermissions .ml-form-embedPermissionsOptionsCheckbox .label-description::before, #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-embedPermissions .ml-form-embedPermissionsOptionsCheckbox .label-description::after, #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-checkboxRow .label-description::before, #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-checkboxRow .label-description::after {\n       top: 0px!important;\n       box-sizing: border-box!important;\n  }\n\n  #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-checkboxRow .label-description::before, #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-checkboxRow .label-description::after {\n    top: 0px!important;\n       box-sizing: border-box!important;\n  }\n\n   #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-interestGroupsRow .ml-form-interestGroupsRowCheckbox .label-description::after {\n        top: 0px!important;\n        box-sizing: border-box!important;\n        position: absolute;\n        left: -1.5rem;\n        display: block;\n        width: 1rem;\n        height: 1rem;\n        content: \"\";\n   }\n\n  #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-interestGroupsRow .ml-form-interestGroupsRowCheckbox .label-description::before {\n    top: 0px!important;\n    box-sizing: border-box!important;\n  }\n\n  #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .custom-control-label::before {\n      position: absolute;\n      top: 4px;\n      left: -1.5rem;\n      display: block;\n      width: 16px;\n      height: 16px;\n      pointer-events: none;\n      content: \"\";\n      background-color: #ffffff;\n      border: #adb5bd solid 1px;\n      border-radius: 50%;\n  }\n\n  #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .custom-control-label::after {\n      position: absolute;\n      top: 2px!important;\n      left: -1.5rem;\n      display: block;\n      width: 1rem;\n      height: 1rem;\n      content: \"\";\n  }\n\n  #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-embedPermissions .ml-form-embedPermissionsOptionsCheckbox .label-description::before, #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-interestGroupsRow .ml-form-interestGroupsRowCheckbox .label-description::before, #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-checkboxRow .label-description::before {\n      position: absolute;\n      top: 4px;\n      left: -1.5rem;\n      display: block;\n      width: 16px;\n      height: 16px;\n      pointer-events: none;\n      content: \"\";\n      background-color: #ffffff;\n      border: #adb5bd solid 1px;\n      border-radius: 50%;\n  }\n\n  #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-embedPermissions .ml-form-embedPermissionsOptionsCheckbox .label-description::after {\n      position: absolute;\n      top: 0px!important;\n      left: -1.5rem;\n      display: block;\n      width: 1rem;\n      height: 1rem;\n      content: \"\";\n  }\n\n  #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-checkboxRow .label-description::after {\n      position: absolute;\n      top: 0px!important;\n      left: -1.5rem;\n      display: block;\n      width: 1rem;\n      height: 1rem;\n      content: \"\";\n  }\n\n  #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .custom-radio .custom-control-label::after {\n      background: no-repeat 50%/50% 50%;\n  }\n  #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .custom-checkbox .custom-control-label::after, #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-embedPermissions .ml-form-embedPermissionsOptionsCheckbox .label-description::after, #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-interestGroupsRow .ml-form-interestGroupsRowCheckbox .label-description::after, #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-checkboxRow .label-description::after {\n      background: no-repeat 50%/50% 50%;\n  }\n\n  #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-fieldRow .custom-control, #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-horizontalRow .custom-control {\n    position: relative;\n    display: block;\n    min-height: 1.5rem;\n    padding-left: 1.5rem;\n  }\n\n  #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-fieldRow .custom-radio .custom-control-input, #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-horizontalRow .custom-radio .custom-control-input, #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-fieldRow .custom-checkbox .custom-control-input, #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-horizontalRow .custom-checkbox .custom-control-input {\n      position: absolute;\n      z-index: -1;\n      opacity: 0;\n      box-sizing: border-box;\n      padding: 0;\n  }\n\n  #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-fieldRow .custom-radio .custom-control-label, #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-horizontalRow .custom-radio .custom-control-label, #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-fieldRow .custom-checkbox .custom-control-label, #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-horizontalRow .custom-checkbox .custom-control-label {\n      color: #000000;\n      font-size: 12px!important;\n      font-family: 'Open Sans', Arial, Helvetica, sans-serif;\n      line-height: 22px;\n      margin-bottom: 0;\n      position: relative;\n      vertical-align: top;\n      font-style: normal;\n      font-weight: 700;\n  }\n\n  #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-fieldRow .custom-select, #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-horizontalRow .custom-select {\n    background-color: #ffffff !important;\n    color: #333333 !important;\n    border-color: #cccccc;\n    border-radius: 4px !important;\n    border-style: solid !important;\n    border-width: 1px !important;\n    font-family: 'Open Sans', Arial, Helvetica, sans-serif;\n    font-size: 14px !important;\n    line-height: 20px !important;\n    margin-bottom: 0;\n    margin-top: 0;\n    padding: 10px 28px 10px 12px !important;\n    width: 100% !important;\n    box-sizing: border-box !important;\n    max-width: 100% !important;\n    height: auto;\n    display: inline-block;\n    vertical-align: middle;\n    background: url('https://assets.mlcdn.com/ml/images/default/dropdown.svg') no-repeat right .75rem center/8px 10px;\n    -webkit-appearance: none;\n    -moz-appearance: none;\n    appearance: none;\n  }\n\n\n  #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-horizontalRow {\n    height: auto;\n    width: 100%;\n    float: left;\n  }\n  .ml-form-formContent.horozintalForm .ml-form-horizontalRow .ml-input-horizontal { width: 70%; float: left; }\n  .ml-form-formContent.horozintalForm .ml-form-horizontalRow .ml-button-horizontal { width: 30%; float: left; }\n  .ml-form-formContent.horozintalForm .ml-form-horizontalRow .ml-button-horizontal.labelsOn { padding-top: 25px;  }\n  .ml-form-formContent.horozintalForm .ml-form-horizontalRow .horizontal-fields { box-sizing: border-box; float: left; padding-right: 10px;  }\n  #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-horizontalRow input {\n    background-color: #ffffff;\n    color: #333333;\n    border-color: #cccccc;\n    border-radius: 4px;\n    border-style: solid;\n    border-width: 1px;\n    font-family: 'Open Sans', Arial, Helvetica, sans-serif;\n    font-size: 14px;\n    line-height: 20px;\n    margin-bottom: 0;\n    margin-top: 0;\n    padding: 10px 10px;\n    width: 100%;\n    box-sizing: border-box;\n    overflow-y: initial;\n  }\n  #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-horizontalRow button {\n    background-color: #000000 !important;\n    border-color: #000000;\n    border-style: solid;\n    border-width: 1px;\n    border-radius: 4px;\n    box-shadow: none;\n    color: #ffffff !important;\n    cursor: pointer;\n    font-family: 'Open Sans', Arial, Helvetica, sans-serif;\n    font-size: 14px !important;\n    font-weight: 700;\n    line-height: 20px;\n    margin: 0 !important;\n    padding: 10px !important;\n    width: 100%;\n    height: auto;\n  }\n  #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-horizontalRow button:hover {\n    background-color: #333333 !important;\n    border-color: #333333 !important;\n  }\n  #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-checkboxRow input[type=\"checkbox\"] {\n    box-sizing: border-box;\n    padding: 0;\n    position: absolute;\n    z-index: -1;\n    opacity: 0;\n    margin-top: 5px;\n    margin-left: -1.5rem;\n    overflow: visible;\n  }\n  #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-checkboxRow .label-description {\n    color: #000000;\n    display: block;\n    font-family: 'Open Sans', Arial, Helvetica, sans-serif;\n    font-size: 12px;\n    text-align: left;\n    margin-bottom: 0;\n    position: relative;\n    vertical-align: top;\n  }\n  #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-checkboxRow label {\n    font-weight: normal;\n    margin: 0;\n    padding: 0;\n    position: relative;\n    display: block;\n    min-height: 24px;\n    padding-left: 24px;\n\n  }\n  #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-checkboxRow label a {\n    color: #000000;\n    text-decoration: underline;\n  }\n  #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-checkboxRow label p {\n    color: #000000 !important;\n    font-family: 'Open Sans', Arial, Helvetica, sans-serif !important;\n    font-size: 12px !important;\n    font-weight: normal !important;\n    line-height: 18px !important;\n    padding: 0 !important;\n    margin: 0 5px 0 0 !important;\n  }\n  #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-checkboxRow label p:last-child {\n    margin: 0;\n  }\n  #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-embedSubmit {\n    margin: 0 0 20px 0;\n    float: left;\n    width: 100%;\n  }\n  #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-embedSubmit button {\n    background-color: #000000 !important;\n    border: none !important;\n    border-radius: 4px !important;\n    box-shadow: none !important;\n    color: #ffffff !important;\n    cursor: pointer;\n    font-family: 'Open Sans', Arial, Helvetica, sans-serif !important;\n    font-size: 14px !important;\n    font-weight: 700 !important;\n    line-height: 21px !important;\n    height: auto;\n    padding: 10px !important;\n    width: 100% !important;\n    box-sizing: border-box !important;\n  }\n  #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-embedSubmit button.loading {\n    display: none;\n  }\n  #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-embedSubmit button:hover {\n    background-color: #333333 !important;\n  }\n  .ml-subscribe-close {\n    width: 30px;\n    height: 30px;\n    background: url('https://assets.mlcdn.com/ml/images/default/modal_close.png') no-repeat;\n    background-size: 30px;\n    cursor: pointer;\n    margin-top: -10px;\n    margin-right: -10px;\n    position: absolute;\n    top: 0;\n    right: 0;\n  }\n  .ml-error input, .ml-error textarea, .ml-error select {\n    border-color: red!important;\n  }\n\n  .ml-error .custom-checkbox-radio-list {\n    border: 1px solid red !important;\n    border-radius: 4px;\n    padding: 10px;\n  }\n\n  .ml-error .label-description,\n  .ml-error .label-description p,\n  .ml-error .label-description p a,\n  .ml-error label:first-child {\n    color: #ff0000 !important;\n  }\n\n  #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-checkboxRow.ml-error .label-description p,\n  #mlb2-18937141.ml-form-embedContainer .ml-form-embedWrapper .ml-form-embedBody .ml-form-checkboxRow.ml-error .label-description p:first-letter {\n    color: #ff0000 !important;\n  }\n        @media only screen and (max-width: 400px){\n\n    .ml-form-embedWrapper.embedDefault, .ml-form-embedWrapper.embedPopup { width: 100%!important; }\n    .ml-form-formContent.horozintalForm { float: left!important; }\n    .ml-form-formContent.horozintalForm .ml-form-horizontalRow { height: auto!important; width: 100%!important; float: left!important; }\n    .ml-form-formContent.horozintalForm .ml-form-horizontalRow .ml-input-horizontal { width: 100%!important; }\n    .ml-form-formContent.horozintalForm .ml-form-horizontalRow .ml-input-horizontal &gt; div { padding-right: 0px!important; padding-bottom: 10px; }\n    .ml-form-formContent.horozintalForm .ml-button-horizontal { width: 100%!important; }\n    .ml-form-formContent.horozintalForm .ml-button-horizontal.labelsOn { padding-top: 0px!important; }\n\n  }\n&lt;/style&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;div id=\"mlb2-18937141\" class=\"ml-form-embedContainer ml-subscribe-form ml-subscribe-form-18937141\"&gt;\n  &lt;div class=\"ml-form-align-center \"&gt;\n    &lt;div class=\"ml-form-embedWrapper embedForm\"&gt;\n\n\n\n\n      &lt;div class=\"ml-form-embedBody ml-form-embedBodyDefault row-form\"&gt;\n\n        &lt;div class=\"ml-form-embedContent\" style=\" \"&gt;\n          \n            &lt;h4&gt;Newsletter&lt;/h4&gt;\n            \n              &lt;p&gt;Subscribe here to get a summary of the top AI papers in your inbox weekly!&lt;/p&gt;\n            \n          \n        &lt;/div&gt;\n\n        &lt;form class=\"ml-block-form\" action=\"https://assets.mailerlite.com/jsonp/1142981/forms/135284638687953948/subscribe\" data-code=\"\" method=\"post\" target=\"_blank\"&gt;\n          &lt;div class=\"ml-form-formContent\"&gt;\n            \n\n\n              &lt;div class=\"ml-form-fieldRow ml-last-item\"&gt;\n                &lt;div class=\"ml-field-group ml-field-email ml-validate-email ml-validate-required\"&gt;\n\n\n\n\n                  &lt;!-- input --&gt;\n                  &lt;input aria-label=\"email\" aria-required=\"true\" type=\"email\" class=\"form-control\" data-inputmask=\"\" name=\"fields[email]\" placeholder=\"Email\" autocomplete=\"email\"&gt;\n                  &lt;!-- /input --&gt;\n\n                  &lt;!-- textarea --&gt;\n                  \n                  &lt;!-- /textarea --&gt;\n\n                  &lt;!-- select --&gt;\n                  \n                  &lt;!-- /select --&gt;\n\n                  &lt;!-- checkboxes --&gt;\n        \n        &lt;!-- /checkboxes --&gt;\n\n                  &lt;!-- radio --&gt;\n                  \n                  &lt;!-- /radio --&gt;\n\n                  &lt;!-- countries --&gt;\n                  \n                  &lt;!-- /countries --&gt;\n\n\n\n\n\n                &lt;/div&gt;\n              &lt;/div&gt;\n            \n          &lt;/div&gt;\n\n\n\n          &lt;!-- Privacy policy --&gt;\n          \n          &lt;!-- /Privacy policy --&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n          &lt;input type=\"hidden\" name=\"ml-submit\" value=\"1\"&gt;\n\n          &lt;div class=\"ml-form-embedSubmit\"&gt;\n            \n              &lt;button type=\"submit\" class=\"primary\"&gt;Subscribe&lt;/button&gt;\n            \n            &lt;button disabled=\"disabled\" style=\"display: none;\" type=\"button\" class=\"loading\"&gt;\n              &lt;div class=\"ml-form-embedSubmitLoad\"&gt;&lt;/div&gt;\n              &lt;span class=\"sr-only\"&gt;Loading...&lt;/span&gt;\n            &lt;/button&gt;\n          &lt;/div&gt;\n\n\n          &lt;input type=\"hidden\" name=\"anticsrf\" value=\"true\"&gt;\n        &lt;/form&gt;\n      &lt;/div&gt;\n\n      &lt;div class=\"ml-form-successBody row-success\" style=\"display: none\"&gt;\n\n        &lt;div class=\"ml-form-successContent\"&gt;\n          \n            &lt;h4&gt;Thank you!&lt;/h4&gt;\n            \n              &lt;p&gt;You have successfully joined our subscriber list.&lt;/p&gt;\n            \n          \n        &lt;/div&gt;\n\n      &lt;/div&gt;\n    &lt;/div&gt;\n  &lt;/div&gt;\n&lt;/div&gt;\n\n  &lt;script src=\"https://groot.mailerlite.com/js/w/webforms.min.js?v2d8fb22bb5b3677f161552cd9e774127\" type=\"text/javascript\"&gt;&lt;/script&gt;\n    &lt;script&gt;\n        fetch(\"https://assets.mailerlite.com/jsonp/1142981/forms/135284638687953948/takel\")\n    &lt;/script&gt;\n\n\n\n\n\n\n\n\n\n🌙 AI Afterhours: Top AI Papers for Oct 04 - Oct 10, 2024\n\n\n\n\n\n\n\n\n\n\n\nOct 10, 2024\n\n\n5 min\n\n\n\n\n\n\n\n🌙 AI Afterhours: Top AI Papers for Sep 30 - Oct 02, 2024\n\n\n\n\n\n\n\n\n\n\n\nSep 30, 2024\n\n\n6 min\n\n\n\n\n\n\n\nLlama Stack: Meta’s Impressive Stride in the AI Framework Marathon\n\n\n\n\n\n\n\n\n\n\n\nSep 27, 2024\n\n\n3 min\n\n\n\n\n\n\n\nLess Magic, More Math: Why Inference Scaling is the New Black\n\n\n\n\n\n\n\n\n\n\n\nSep 16, 2024\n\n\n22 min\n\n\n\n\n\n\n\n🌙 AI Afterhours: Top AI Papers for Sep 06 - Sep 12, 2024\n\n\n\n\n\n\n\n\n\n\n\nSep 12, 2024\n\n\n5 min\n\n\n\n\n\n\n\nThe Great Data Famine: Why the AI that Ate the Web Is Still Starving\n\n\n\n\n\n\n\n\n\n\n\nSep 6, 2024\n\n\n14 min\n\n\n\n\n\n\n\n🌙 AI Afterhours: Top AI Papers for Aug 30 - Sep 05, 2024\n\n\n\n\n\n\n\n\n\n\n\nAug 30, 2024\n\n\n3 min\n\n\n\n\n\n\n\n🌙 AI Afterhours: Top AI Papers for Aug 21 - Aug 27, 2024\n\n\n\n\n\n\n\n\n\n\n\nAug 27, 2024\n\n\n5 min\n\n\n\n\n\n\n\nThe Surprisingly Lucrative Journey of Bootstrapping a Brand Recommender System: From Chaos to Cash\n\n\n\n\n\n\n\n\n\n\n\nAug 24, 2024\n\n\n16 min\n\n\n\n\n\n\n\n🌙 AI Afterhours: Top AI Papers for Aug 15 - Aug 20, 2024\n\n\n\n\n\n\n\n\n\n\n\nAug 20, 2024\n\n\n4 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#personal-info-in-a-nutshell",
    "href": "index.html#personal-info-in-a-nutshell",
    "title": "Shwetank Kumar",
    "section": "Personal info in a nutshell",
    "text": "Personal info in a nutshell\nI’m a physicist, engineering exec, investor. Here’s my story:\n\nBorn and raised in NCR, India. Delhi native, I live in SF.\nGot hooked on science early, did my B.Tech in EE from IIT, PhD in Applied Physics from Caltech.\nLearned business by working at startups for 10+ years and getting MBA at Wharton.\nI founded a bootstrapped company to help data teams optimize their Snowflake spend.\nI enjoy building stuff for internet and physical systems using data and AI games."
  },
  {
    "objectID": "index.html#some-things-ive-done",
    "href": "index.html#some-things-ive-done",
    "title": "Shwetank Kumar",
    "section": "Some things I’ve done",
    "text": "Some things I’ve done\n\nBuilt AI systems that crunch petabytes of data across image, text, and tabular formats.\nScaled these systems to serve millions of customers simultaneously 1, 2.\nCreated systems that use neural networks to analyze satellite imagery at planet scale.\nLed teams of 120+ brilliant minds in data science, engineering, and analytics."
  },
  {
    "objectID": "index.html#tech-i-love-working-with",
    "href": "index.html#tech-i-love-working-with",
    "title": "Shwetank Kumar",
    "section": "Tech I love working with:",
    "text": "Tech I love working with:\n\nPyTorch, Hugging Face, Langchain, Pydantic\nGCP, Snowflake, Bigquery\nAnything that pushes the boundaries of what’s possible with data"
  },
  {
    "objectID": "index.html#investing",
    "href": "index.html#investing",
    "title": "Shwetank Kumar",
    "section": "Investing:",
    "text": "Investing:\n\nMy current areas of interest are: AI, infrastructure, open source, physical systems / frontier tech.\nSome companies I have invested in - Comet, Startree, Inference, Nimble, Turnstile, Dandelion…\nI am a founding member and part of the steering committee at Invest in Data where I co-invest with a group of ~50 other Data execs.\nWe vet and invest on a quarterly cadence. If you have an exciting AI project or a game-changing startup idea? Let’s talk: shwetank.kumar@gmail.com."
  },
  {
    "objectID": "index.html#where-you-can-find-me",
    "href": "index.html#where-you-can-find-me",
    "title": "Shwetank Kumar",
    "section": "Where you can find me:",
    "text": "Where you can find me:\n\nWriting about AI musings, tech & leadership, and data science on this blog\nSharing insights on LinkedIn\nTinkering with code on GitHub\nSpeaking on panels about the future of AI and data"
  },
  {
    "objectID": "posts/data-scarcity/index.html#the-transformer-plateau-when-bigger-isnt-always-better",
    "href": "posts/data-scarcity/index.html#the-transformer-plateau-when-bigger-isnt-always-better",
    "title": "The Great Data Famine: Why the AI that Ate the Web Is Still Starving",
    "section": "The Transformer Plateau: When Bigger Isn’t Always Better",
    "text": "The Transformer Plateau: When Bigger Isn’t Always Better\nRemember when we thought the solution to every AI challenge was simply to make it bigger? It was like Silicon Valley’s version of “supersize me,” but instead of fries, we were supersizing parameters and datasets. Well, folks, that all-you-can-eat buffet of data and compute has reached its limit.\nSince the “Attention is all you need” paper dropped and gave us the Transformer architecture, we’ve been playing a game of “who can build the biggest model?” It’s been like a digital arms race, with each new model flexing more parameters. And for a while, it worked! We fed these hungry, hungry hippos more data, cranked up the parameter count, and watched the benchmarks climb.\nBut now, we’ve hit a wall. A big, data-shaped wall that’s putting a damper on our “bigger is better” party. We’re now facing a trifecta of challenges that even the most optimistic VC can’t hand-wave away:\n\nData Scarcity: We’re scraping the bottom of the digital barrel, and it’s not pretty. Turns out, the internet isn’t infinite after all.\nPower Consumption: Our models are energy gluttons that could outshine Las Vegas. At this rate, we’ll need a small nuclear reactor for each training run.\nSystem Complexity: We’re playing high-stakes Jenga with GPUs, hoping that one wobbly card doesn’t bring down the entire expensive house of silicon (aka cluster).\n\nUnless we see some major breakthroughs in model architecture or how these Large Language Models (LLMs) learn, we’re stuck. Future AI products will be reheating the same capabilities we have today, just with fancier marketing. It’s like we’ve trained our AI to be a really eloquent parrot, but now we need it to write a novel.\nLets explore why our current AI models are perpetually data-starved, uncover the fundamental limitations of our learning approaches (backprop, SGD …), and maybe even find a path forward that doesn’t involve sacrificing our firstborns to the GPU gods. Welcome to the frontlines of the AI data crisis, in the land of diminishing returns – where the future is bright, but the training datasets are running on empty."
  },
  {
    "objectID": "posts/data-scarcity/index.html#beyond-chinchilla-llama-3.1-and-the-quest-for-more-data",
    "href": "posts/data-scarcity/index.html#beyond-chinchilla-llama-3.1-and-the-quest-for-more-data",
    "title": "The Great Data Famine: Why the AI that Ate the Web Is Still Starving",
    "section": "Beyond Chinchilla: Llama 3.1 and the Quest for More Data",
    "text": "Beyond Chinchilla: Llama 3.1 and the Quest for More Data\nImagine an AI as a toddler with an endless appetite for knowledge. Now, picture that toddler devouring the entire internet and still asking for seconds. That’s our current predicament with Large Language Models (LLMs). These digital gluttons are slow learners with an insatiable hunger for data, and folks, we’re running out of internet to feed them. Who would’ve thought “we’ve run out of internet” would be a legitimate concern in 2024?\n\nExhibit A: Meta’s Llama 3.1 - The Data Devourer\nLet’s dive into a real-world example that’ll make your head spin: Meta’s latest 8B parameter Llama 3.1 model. This digital beast was fed a whopping 15 trillion tokens during training. For those keeping score at home, that’s essentially the entire publicly available internet. The Llama paper [1] claims it’s all public data, but they’re keeping the exact dataset under wraps. The closest we’ve got to peeking behind the curtain is the “Fine Web Dataset” [2] on Hugging Face, tipping the scales at a hefty 44TB.\n\n\nBreaking the Rules: When More is… More?\nNow, if you’ve been following the AI literature like it’s the new Netflix, you might be thinking, “Wait a minute, isn’t that overkill?” And you’d be right - sort of. The Chinchilla paper [3], our previous guidebook for “compute optimal” training, would suggest that an 8B parameter model only needs about 160B tokens. Llama 3.1 ate roughly 100 times that amount!\nBut here’s the kicker: it worked. Meta’s decision to massively overindulge their model led to continued performance improvements. This reveals two mind-bending facts:\n\nMany existing models are actually undernourished by comparison.\nHigh-quality data is the new oil in the AI gold rush.\n\nEven after this data feast, Llama 3.1 hadn’t reached what we’d classically call convergence. It was still improving, like a bottomless pit of potential [4]. This, combined with Microsoft’s Tiny Stories paper [5], is forcing us to rethink the data requirements for training these models.\n\n\nThe Bigger They Are, The Hungrier They Get\nAnd now consider the data requirements for 405B parameter version of Llama 3.1. It should ideally be trained on proportionally more data - we’re talking “several internets” worth. But guess what? It was trained on the same 15T tokens as its smaller sibling. If that dataset was barely enough for the 8B model, it’s like trying to feed a blue whale with a goldfish bowl for the 405B version.\n\n\nA Silver Lining in the AI Cloud\nBefore you start stockpiling hard drives and building your own internet, there’s a glimmer of hope. For enterprises sitting on a goldmine of non-public data (that you’re legally allowed to use, of course), you’re in luck. This is your chance to fine-tune these models for specialized tasks and potentially push their performance beyond what’s publicly possible. And for the efficiency enthusiasts out there, there’s still plenty of room to explore knowledge distillation - teaching smaller models to mimic their bigger, data-guzzling cousins.\n\n\nThe TL;DR Version\n\nLlama 3.1’s training reveals that our “well-trained” models might actually be underfed data-wise.\nWe’ve essentially run out of high-quality public data to train these ever-growing models.\nThe next frontier? Leveraging private data to push these models even further.\n\nWelcome to the era of data scarcity in AI - where the models are hungry, the internet is finite, and every byte counts!"
  },
  {
    "objectID": "posts/data-scarcity/index.html#synthetic-data-ais-infinite-mirror-of-confusion",
    "href": "posts/data-scarcity/index.html#synthetic-data-ais-infinite-mirror-of-confusion",
    "title": "The Great Data Famine: Why the AI that Ate the Web Is Still Starving",
    "section": "Synthetic Data: AI’s Infinite Mirror of Confusion",
    "text": "Synthetic Data: AI’s Infinite Mirror of Confusion\nWhen faced with data scarcity, researchers and engineers came up with a brilliant idea: Why not leverage AI models to generate their own training data? This concept, far from being a desperate measure, is actually a legitimate and innovative approach to addressing the data hunger of large language models which offers many advantages like unparalleled scalability, solution for privacy preservation in training data, and customization for specialized tasks. However, it also comes with its own set of challenges.\n\nModel Collapse: When AI Goes on a Bland Diet\nThis self-cannibalization of data leads to what’s ominously known as ‘model collapse’. Don’t let the fancy term scare you - it’s simply what happens when your AI goes on a diet of nothing but its own increasingly bland word salad.\nHere’s how it works: the model (or its bigger cousin) generates tokens based on probability distributions so it favors tokens closer to the mean (the “average” outputs) providing fewer examples of tokens out in the wings of the distribution. After a few cycles of generating and training on synthetic data, you lose all the diverse content from the original dataset. Result? Your models over generations lose the brilliance and versatility that would come from diversity and start generating singular, monochromatic data which doesn’t capture the real world anymore.\nIts like making a photocopy of a photocopy - each generation gets blurrier and weirder until you end up with something that looks like it came from a glitchy parallel universe where creativity went to die.\n\n\nBias on Steroids\nThe opposite side of the same coin is that any small biases in the initial dataset get amplified out of propostion. So now the slight bias in your dataset is dialed to 11 on a scale of 10 and it thinks the entire world population consists of cat-loving, pizza-eating coders who never see the sun. Diversity? That’s just a myth, like inbox zero or bug-free code.\n\n\nQuality Control Nightmare\nValidating synthetic data is like fact-checking a politician’s promises - a Sisyphusean task (i.e. can’t be done, easily anyway). It’s a guessing game where the grand prize is “maybe your AI won’t embarrass itself in public.” And good luck keeping it current - by the time you’ve generated your synthetic data, the real world has moved on, leaving your AI stuck in last season’s trends like a digital fashion faux pas.\n\n\nCybersecurity Swiss Cheese\nJust when you thought it couldn’t get worse, enter the hackers. Synthetic data is like a new chew toy for cybercriminals. They’re gleefully exploring all the ways they can mess with your data generation process, turning your AI into their personal puppet show.\n\n\nSilver Linings for Synthetic Data\nBut wait! Don’t despair just yet. One person’s data dilemma is another’s research opportunity. Here are some tantalizing questions for the brave AI researchers of tomorrow:\n\nCan we develop smarter sampling strategies to generate synthetic data from the neglected “wings” of the probability distribution?\nWhat’s the perfect cocktail of real and synthetic data? Is there a golden ratio, or does it depend on the task?\nHow can we build Fort Knox-level security around our synthetic data generation process?\n\n\n\nThe TL;DR Version\n\nSynthetic data is a promising solution to data scarcity, offering scalability, privacy, and customization.\nHowever, it comes with significant challenges: model collapse, bias amplification, quality control issues, and security risks.\nThe future of synthetic data lies in developing better generation strategies, finding optimal real-synthetic data ratios, and creating robust security frameworks.\n\nUntil we crack these problems, synthetic data is the AI world’s equivalent of combating climate change by painting everything green. It looks fantastic in PowerPoint presentations, but step outside, and you’ll find a world of plastic trees where your AI thinks photosynthesis is just the latest Instagram filter."
  },
  {
    "objectID": "posts/data-scarcity/index.html#deep-networks-are-slow-learners",
    "href": "posts/data-scarcity/index.html#deep-networks-are-slow-learners",
    "title": "The Great Data Famine: Why the AI that Ate the Web Is Still Starving",
    "section": "Deep Networks are Slow Learners",
    "text": "Deep Networks are Slow Learners\nThe underlying problem that is a root cause of all our AI woes is that neural networks and other architectures using a combination of back propagation and stochastic gradient descent (let’s call these SGD & Progeny Pvt. Ltd. to include other algorithms like Adam, Ada etc.) are slow learners - absorbing knowledge at the speed of continental drift. They’re the Pangaea of machine learning, slowly but inexorably shuffling bits of information around until, eons later, you might just have a functional model. Let’s look at how these work.\n\nThe SGD Conundrum: Navigating IKEA Blindfolded\nStochastic Gradient Descent (SGD) and its variants are the cornerstone of modern machine learning optimization techniques. However, their effectiveness is limited by their inherent randomness. It’s basically like trying to find your way out of an IKEA blindfolded by randomly stumbling around, making small adjustments to your trajectory based on very limited local information. You might eventually find the exit, but you’ll bump into a lot of BILLY bookcases along the way.\n\n\nEscape from Local Minima: The Comfortable Rut\nNeural networks often get trapped in suboptimal solutions, or “local minima,” during training. To overcome this, we need to expose our models to diverse, high-quality data repeatedly. However, the scale at which this needs to happen is staggering - often requiring millions of iterations. Of course, there are heuristics that can help you along in the process, but coming up with the right set of parameters for training and escaping each minima requires a lot of experimentation with hyperparameters.\n\n\nThe Hyperparameter Labyrinth: Cracking the Infinite Safe\nHyperparameter tuning in machine learning is akin to trying to crack a safe with an infinite number of dials. Each parameter - learning rate, batch size, network architecture, etc. - can dramatically affect model performance, yet their interactions are often unpredictable and non-linear. It’s not uncommon for researchers to spend more time tuning hyperparameters than actually training models. This process is often more art than science, relying heavily on intuition, experience, and, frankly, a fair bit of luck.\nAutomated hyperparameter optimization techniques exist, and while helpful, often require significant computational resources and can still miss optimal configurations due to the vast search space. Moreover, hyperparameters that work well for one dataset or task might fail spectacularly on another, making it challenging to develop generalizable best practices.\nThe complexity of hyperparameter tuning also raises questions about the robustness and interpretability of our models. If slight tweaks to these parameters can lead to drastically different results, how can we trust the stability and reliability of our AI systems?\n\n\nThe Curse of Memorization: AI’s “The Office” Obsession\nThese models are turning into the Rain Man of useless information. Great if you need to count toothpicks, not so great for actual intelligence. As these models grow larger, they’re not getting smarter – they’re just getting better at regurgitating what they’ve seen before. It’s like that friend who can recite every line from “The Office” but can’t hold a conversation about anything else.\n\n\nThe Silver Lining: Future Directions\nData scarcity, which is a direct result of these fundamental limitations, is pushing us towards an inflection point. We can’t just keep making models bigger and feeding them more data. We need to fundamentally rethink how these models learn and generalize. It’s like we’ve been trying to build a skyscraper by just piling up more and more bricks. Now we need to stop and think about architecture, efficiency, and maybe invest in an elevator or two.\nSome promising directions include:\n\nAlgorithms that rely on second-order moments, though they require more memory to store gradients.\nCombining these techniques with simplifying assumptions about the nature of the matrix [7], allowing us to store sparser versions.\nCoupling these approaches with newer execution engines in GPUs for sparse matrices.\nExploring hardware-software co-design as a powerful research direction.\n\n\n\nThe TL;DR Version\n\nDeep networks learn slowly due to limitations in optimization techniques like SGD.\nKey challenges include navigating complex loss landscapes, escaping local minima, tuning hyperparameters, and avoiding mere memorization.\nWe need to rethink our approach to model architecture and learning processes.\nPromising directions include advanced optimization algorithms, sparse matrix techniques, and hardware-software co-design.\n\nRemember, in the world of AI, we’re not just teaching machines to learn – we’re learning how to teach. And right now, we’re realizing we might need to go back to teacher school ourselves."
  },
  {
    "objectID": "posts/data-scarcity/index.html#conclusion-the-tip-of-the-ai-iceberg",
    "href": "posts/data-scarcity/index.html#conclusion-the-tip-of-the-ai-iceberg",
    "title": "The Great Data Famine: Why the AI that Ate the Web Is Still Starving",
    "section": "Conclusion: The Tip of the AI Iceberg",
    "text": "Conclusion: The Tip of the AI Iceberg\nAs we’ve explored in this article, the challenges facing AI development are numerous and complex. We’ve only scratched the surface of the data scarcity issue, and there are still two major hurdles we haven’t yet discussed: power consumption and system complexity.\nThe energy requirements for training and running these increasingly large AI models are staggering. As we push the boundaries of model size and complexity, we’re also pushing the limits of our computational infrastructure. The power consumption of these models isn’t just a technical issue—it’s an environmental and economic concern that the AI community will need to address.\nSystem complexity is another critical challenge. As our AI systems grow more sophisticated, managing and optimizing them becomes increasingly difficult. We’re rapidly approaching a point where the complexity of these systems may outpace our ability to understand and control them effectively.\nDown the road I intend to delve deeper into these issues, exploring the implications of AI’s growing energy appetite and the challenges posed by increasingly complex systems. In the meanwhile if there are things in this space that you would like to learn about - DMs are always open!\n\nThe Llama 3 Herd of Models\nFineweb dataset\nTraining Compute-Optimal Large Language Models\n@Karpathy\nTinyStories: How Small Can Language Models Be and Still Speak Coherent English?\nIs Model Collapse Inevitable? Breaking the Curse of Recursion by Accumulating Real and Synthetic Data\nShampoo: Preconditioned Stochastic Tensor Optimization"
  },
  {
    "objectID": "posts/llama-stack/index.html#what-is-llama-stack",
    "href": "posts/llama-stack/index.html#what-is-llama-stack",
    "title": "Llama Stack: Meta’s Impressive Stride in the AI Framework Marathon",
    "section": "What is Llama Stack?",
    "text": "What is Llama Stack?\nLlama Stack is Meta’s comprehensive framework for developing generative AI applications. It’s not just a stack; it’s a skyscraper of possibilities reaching into the cloud(s). This multi-layered, API-standardized, fine-tuning-friendly behemoth streamlines the entire AI development process, from model training to production deployment."
  },
  {
    "objectID": "posts/llama-stack/index.html#key-components-of-llama-stack",
    "href": "posts/llama-stack/index.html#key-components-of-llama-stack",
    "title": "Llama Stack: Meta’s Impressive Stride in the AI Framework Marathon",
    "section": "Key Components of Llama Stack",
    "text": "Key Components of Llama Stack\n\nInference API: Predict the future (results may vary)\nSafety API: Because with great power comes great responsibility\nMemory API: Bend data to your will like a silicon sorcerer\nAgentic System API: Orchestrate AI agents with ease\nTelemetry API: Achieve AI enlightenment through deep insights"
  },
  {
    "objectID": "posts/llama-stack/index.html#what-youll-learn-in-the-video",
    "href": "posts/llama-stack/index.html#what-youll-learn-in-the-video",
    "title": "Llama Stack: Meta’s Impressive Stride in the AI Framework Marathon",
    "section": "What You’ll Learn in the Video",
    "text": "What You’ll Learn in the Video\n\nHow to deploy Llama Stack effectively, avoiding common pitfalls like a seasoned AI navigator\nThe art of testing your AI creation using nothing but a browser - no magic wand required!\nExploring mind-bending use cases with an IPython Notebook, turning code into AI gold\nDeploying AI solutions faster than you can say “Is this magic?”"
  },
  {
    "objectID": "posts/llama-stack/index.html#getting-started-with-llama-stack",
    "href": "posts/llama-stack/index.html#getting-started-with-llama-stack",
    "title": "Llama Stack: Meta’s Impressive Stride in the AI Framework Marathon",
    "section": "Getting Started with Llama Stack",
    "text": "Getting Started with Llama Stack\nThe video guide will walk you through:\n\nInstalling the Llama CLI: Your Swiss Army knife for all things Llama Stack\nDownloading Models: Choose from a variety of Llama models\nBuilding and Configuring Distributions: Customize your AI stack\nRunning Your Llama Stack Server: Bring your AI creation to life\nTesting with Clients: Witness the power of your newly created AI system"
  },
  {
    "objectID": "posts/llama-stack/index.html#embrace-the-llama-be-the-llama",
    "href": "posts/llama-stack/index.html#embrace-the-llama-be-the-llama",
    "title": "Llama Stack: Meta’s Impressive Stride in the AI Framework Marathon",
    "section": "Embrace the Llama, be the Llama",
    "text": "Embrace the Llama, be the Llama\nLlama Stack represents a significant step forward in making AI development more accessible and efficient. For businesses looking to leverage AI, it’s a powerful ally. For developers, it’s an opportunity to elevate your skills and streamline your workflow.\nAfter all, in the world of tech, sometimes you need to spit in the face of convention to make progress. And what better way to do that than with a framework named after a creature known for its spitting prowess? 🦙💻\nBy the end of our video, you’ll be slinging APIs, fine-tuning models, and deploying AI solutions with the casual ease of a seasoned pro. Your colleagues will whisper your name in awe, and your LinkedIn profile might just spontaneously combust from the sheer radiance of your newly acquired skills. So, are you ready to evolve from a developer into a digital deity? Watch the video above, and let’s turn that Llama into a unicorn!"
  }
]