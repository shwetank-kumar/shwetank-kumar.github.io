[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "The Surprisingly Lucrative Journey of Bootstrapping a Brand Recommender System: From Chaos to Cash\n\n\n\n\n\n\nrecommenders\n\n\ncode\n\n\n\n\n\n\n\n\n\nAug 24, 2024\n\n\nShwetank Kumar\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "favorite-eggheads.html",
    "href": "favorite-eggheads.html",
    "title": "Shwetank Kumar",
    "section": "",
    "text": "AI tutorials - Andrej Karpathy\nPublic company valuations - Ashwath Damodaran\nFinancial systems - Patrick McKenzie"
  },
  {
    "objectID": "favorite-eggheads.html#favorite-topics-and-experts",
    "href": "favorite-eggheads.html#favorite-topics-and-experts",
    "title": "Shwetank Kumar",
    "section": "",
    "text": "AI tutorials - Andrej Karpathy\nPublic company valuations - Ashwath Damodaran\nFinancial systems - Patrick McKenzie"
  },
  {
    "objectID": "posts/cold-start/index.html",
    "href": "posts/cold-start/index.html",
    "title": "The Surprisingly Lucrative Journey of Bootstrapping a Brand Recommender System: From Chaos to Cash",
    "section": "",
    "text": "Welcome, data enthusiasts and e-commerce innovators. Today, we’re diving into the fascinating world of recommendation systems - specifically, how to build a brand recommender from the ground up. We’ll explore the journey from raw data to a sophisticated system that can significantly boost your bottom line. This post will guide you through the process of creating a recommendation engine that not only predicts customer preferences but also enhances the overall shopping experience. So, let’s embark on this data-driven adventure and unlock the potential of personalized recommendations in e-commerce."
  },
  {
    "objectID": "posts/cold-start/index.html#the-problem-e-commerce-is-a-jungle-and-your-customers-are-lost",
    "href": "posts/cold-start/index.html#the-problem-e-commerce-is-a-jungle-and-your-customers-are-lost",
    "title": "The Surprisingly Lucrative Journey of Bootstrapping a Brand Recommender System: From Chaos to Cash",
    "section": "The Problem: E-commerce Is a Jungle (And Your Customers Are Lost)",
    "text": "The Problem: E-commerce Is a Jungle (And Your Customers Are Lost)\nImagine you’ve just launched an e-commerce platform. It’s sleek, efficient, and boasts an impressive array of brands. Initially, you’re confident in its success. However, user feedback quickly reveals a common challenge in the e-commerce world: “I’m overwhelmed by the number of options.” “The recommendations don’t seem relevant to my interests.” “I can’t find products that match my specific needs.” These concerns are not uncommon. In the realm of online retail, the balance between variety and accessibility is crucial. Insufficient options can leave customers feeling limited, while an overabundance can lead to decision fatigue. This phenomenon, often referred to as the “paradox of choice,” can significantly impact user experience and, consequently, your conversion rates.\nThis, my friends, is why a good recommendation system is invaluable. It’s like having a wise, all-knowing friend who gently guides your customers to their next favorite purchase. And today, we’re going to build that friend from scratch."
  },
  {
    "objectID": "posts/cold-start/index.html#step-0-the-oh-crap-we-have-no-data-phase---totally-random-model",
    "href": "posts/cold-start/index.html#step-0-the-oh-crap-we-have-no-data-phase---totally-random-model",
    "title": "The Surprisingly Lucrative Journey of Bootstrapping a Brand Recommender System: From Chaos to Cash",
    "section": "Step 0: The “Oh Crap, We Have No Data” Phase - Totally Random Model",
    "text": "Step 0: The “Oh Crap, We Have No Data” Phase - Totally Random Model\nLet’s start at the very beginning (a very good place to start…). You’ve just launched your platform, and your data cupboard is as bare as Old Mother Hubbard’s. What do you do?\nWell, first, take a deep breath and repeat after me: “Random is better than nothing.”\nYou see, when you have no data, your best friend is Mr. Random. It’s not ideal, but it’s a start. And in the world of startups, starting is half the battle. Now, I know what you’re thinking. “But this is just throwing darts blindfolded! How is this helping anyone?” And you’re right, it’s not ideal. But here’s the secret: it’s not about being perfect; it’s about starting the flywheel.\nEvery time a user sees a random recommendation, you’re gathering data. Maybe they ignore it (data point!). Maybe they click on it (data point!). Maybe they buy it (cha-ching and data point!). Every interaction is a breadcrumb that will lead you out of the data desert.\nLet’s whip up a quick Python function to generate random recommendations:\nimport random\n\ndef get_random_recommendations(all_brands, n=5):\n    return random.sample(all_brands, min(n, len(all_brands)))\n\n# Example usage\nall_brands = df['recommending_brand'].unique().tolist()\nprint(get_random_recommendations(all_brands))\nPro tip: While you’re showing random recommendations, make sure you’re logging EVERYTHING. Every view, every click, every purchase. This data will be worth its weight in gold later on. Trust me, future you will thank present you for this foresight. And while you are at it do make sure that the data is high quality. Algorithms are fickle and state of the art on those changes every week – nay day! But poor quality data once logged sets the ceiling on what you can do with it."
  },
  {
    "objectID": "posts/cold-start/index.html#step-1-the-we-have-some-data-but-its-not-about-users-phase---feature-based-clustering-model",
    "href": "posts/cold-start/index.html#step-1-the-we-have-some-data-but-its-not-about-users-phase---feature-based-clustering-model",
    "title": "The Surprisingly Lucrative Journey of Bootstrapping a Brand Recommender System: From Chaos to Cash",
    "section": "Step 1: The “We Have Some Data, But It’s Not About Users” Phase - Feature-Based Clustering Model",
    "text": "Step 1: The “We Have Some Data, But It’s Not About Users” Phase - Feature-Based Clustering Model\nAlright, so you’ve been running your random recommendation engine for a while. You’ve got some sales, you’ve got some brand data, but you still don’t have enough user interaction data to build a proper collaborative filtering system. Don’t worry, we’re going to make lemonade out of these lemons.\nEnter: Feature-Based Clustering.\nNow, gather ’round, because I’m about to share a secret that took me embarrassingly long to figure out: brands, like people, have personalities. And just like you wouldn’t set up your quiet, bookish friend with your party-animal cousin (trust me, I’ve made that mistake), you shouldn’t be recommending wildly dissimilar brands to your users.\nLet’s create a simple example using K-means clustering. Don’t let the fancy name scare you - it’s just a way of grouping similar things together.\nfrom sklearn.cluster import KMeans\nimport numpy as np\n\n# Example brand features (price, target_age, sportiness)\nbrand_features = {\n    \"Nike\": [80, 25, 9],\n    \"Adidas\": [75, 30, 8],\n    \"Puma\": [60, 28, 7],\n    \"Reebok\": [65, 35, 6],\n    \"Under Armour\": [70, 27, 9],\n    \"New Balance\": [85, 40, 5],\n    \"Asics\": [90, 35, 8],\n    \"Converse\": [55, 22, 3],\n    \"Vans\": [50, 20, 2],\n    \"Skechers\": [45, 45, 4]\n}\n\ndef cluster_brands(brand_features, n_clusters=3):\n    brands = list(brand_features.keys())\n    features = np.array(list(brand_features.values()))\n    \n    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n    clusters = kmeans.fit_predict(features)\n    \n    brand_clusters = {brand: cluster for brand, cluster in zip(brands, clusters)}\n    return brand_clusters\n\nbrand_clusters = cluster_brands(brand_features)\nprint(brand_clusters)\n\ndef get_cluster_recommendations(purchased_brand, brand_clusters, n=5):\n    cluster = brand_clusters[purchased_brand]\n    cluster_brands = [brand for brand, c in brand_clusters.items() if c == cluster]\n    return random.sample(cluster_brands, min(n, len(cluster_brands)))\n\n# Example usage\npurchased_brand = \"Nike\"\nprint(get_cluster_recommendations(purchased_brand, brand_clusters))\nNow, let me tell you a story that’ll make your data scientist senses tingle. Back in the day I was working on a recommendation system with a startup. (I will change the product details and the exact numbers to protect the innocent. Also I don’t fancy a chat with their legal team, thank you very much!). So picture this: They had a range of products - everything from $5 friendship bracelets that screamed “summer camp chic” to $500 mid-century modern chairs that whispered “I have a trust fund.” Their recommendation system? About as sophisticated as a Magic 8-Ball with a hangover. It was the digital equivalent of that one clueless sales associate who tries to upsell you a tuxedo when you’re shopping for gym shorts.\nThe result? Their bounce rate was higher than a kangaroo on a trampoline, and their conversion rate was lower than my undergrad GPA (and trust me, that’s saying something). They addressed this by implementing a clustering system that would make even the most seasoned high school clique organizer jealous. We’re talking price, category, style, color palette - if it could be quantified, they clustered it. And boom! Faster than you can say “artisanal hand-knitted cat sweater,” their improvements were more impressive than that time I managed to fold a fitted sheet correctly on the first try. Their click-through rates shot up like a rocket. Their engineering blog was practically giddy with excitement (in that restrained, data-scientist kind of way) about the boost in overall sales. Now a good data science algorithm needs to do at least two things (and definitely the second of these two): 1. It needs needs to improve a KPI - in this case conversions, although it can be anything. (e.g. if you are a Ferengi it will invariably be profits as laid out in the Ferengi Rules of Acquisition) 2. Regardless of whether it does 1. it needs to light the way to how you might improve the KPI in the future.\nIn this case it wasnt so much the algorithm that did it automatically but the rest of the system that we built around it. We kept a dash of randomness so we could continue to generate data and learn from it. It’s like w they were saying, “Hey, we know you love vintage teacups, but have you considered this handmade steampunk monocle?” They weren’t just preaching to the choir; they were introducing the choir to new hymns they might enjoy, possibly while wearing said monocle. The moral of the story? Even a relatively simple clustering approach can turn your recommendation engine from a bull in a china shop to a graceful ballerina in a Fabergé egg factory. It’s not about having the fanciest algorithm on the block; it’s about understanding your customers and not trying to sell snowshoes to someone shopping for flip-flops. So, whether you’re dealing with designer loafers and budget sneakers, or vintage furniture and friendship bracelets, remember: cluster wisely, but keep it atleast mildly chaotic. Your conversion rates (and your customers) will thank you."
  },
  {
    "objectID": "posts/cold-start/index.html#step-2-the-now-were-cooking-with-gas-phase---purchase-based-association-model",
    "href": "posts/cold-start/index.html#step-2-the-now-were-cooking-with-gas-phase---purchase-based-association-model",
    "title": "The Surprisingly Lucrative Journey of Bootstrapping a Brand Recommender System: From Chaos to Cash",
    "section": "Step 2: The “Now We’re Cooking with Gas” Phase - Purchase-Based Association Model",
    "text": "Step 2: The “Now We’re Cooking with Gas” Phase - Purchase-Based Association Model\nAlright, my data-hungry friends, we’ve arrived at the juicy part. You’ve been diligently collecting user interaction data (you have, haven’t you?), and now it’s time to put it to use. We’re going to build a purchase-based association model.\nThis is where the magic really starts to happen. We’re going to create a system that understands that people who buy brand A often buy brand B, even if we don’t know why. It’s like being a really good matchmaker without understanding the intricacies of human psychology.\nLet’s cook up a simple association model:\nfrom collections import defaultdict\n\ndef build_association_model(purchase_data):\n    associations = defaultdict(lambda: defaultdict(int))\n    for purchase in purchase_data:\n        for i, brand1 in enumerate(purchase):\n            for brand2 in purchase[i+1:]:\n                associations[brand1][brand2] += 1\n                associations[brand2][brand1] += 1\n    return associations\n\n# Example purchase data\npurchase_data = [\n    [\"Nike\", \"Adidas\"],\n    [\"Nike\", \"Under Armour\"],\n    [\"Adidas\", \"Puma\"],\n    [\"Puma\", \"Reebok\"],\n    [\"Nike\", \"Converse\"],\n    [\"Vans\", \"Converse\"],\n    [\"New Balance\", \"Asics\"],\n    [\"Skechers\", \"New Balance\"]\n]\n\nassociation_model = build_association_model(purchase_data)\n\ndef get_associated_brands(brand, association_model, n=5):\n    associated = sorted(association_model[brand].items(), key=lambda x: x[1], reverse=True)\n    return [b for b, _ in associated[:n]]\n\n# Example usage\npurchased_brand = \"Nike\"\nprint(get_associated_brands(purchased_brand, association_model))\nNow, let me tell you why this is a game-changer. Consider the case of Amazon, the e-commerce giant. In their early days, they primarily sold books. But as they expanded into other product categories, they faced a massive challenge: how to effectively cross-sell across these diverse categories? Their solution was to implement a sophisticated association model, much like the one we’ve just built (though admittedly, theirs was far more complex). This “item-to-item collaborative filtering” approach, as they called it, allowed them to say, “Customers who bought this item also bought…”\nThe impact was significant. According to a paper published by Amazon’s engineers in 2003 titled “Amazon.com Recommendations: Item-to-Item Collaborative Filtering”, this recommendation system offered substantial advantages over traditional collaborative filtering techniques:\nIt could handle a massive scale of data - tens of millions of customers and millions of catalog items. It produced high-quality recommendations in real-time, scaling well to sudden spikes in traffic. It could recommend across diverse product categories, from books to electronics to clothing.\nWhile the paper doesn’t provide specific sales figures, it does mention that Amazon’s recommendation system significantly improved click-through and conversion rates compared to untargeted content such as top sellers.\nBut here’s where it gets really interesting. We’re not just creating direct associations. Oh no, we’re going to take this to the next level with transitive associations. By considering not just direct connections between items, but also second-order and third-order connections, we can uncover hidden relationships in our data. This could allow us to make even more nuanced and unexpected recommendations, potentially leading to discoveries that customers might not have made on their own."
  },
  {
    "objectID": "posts/cold-start/index.html#step-3-the-six-degrees-of-kevin-bacon-phase---transitive-association-model",
    "href": "posts/cold-start/index.html#step-3-the-six-degrees-of-kevin-bacon-phase---transitive-association-model",
    "title": "The Surprisingly Lucrative Journey of Bootstrapping a Brand Recommender System: From Chaos to Cash",
    "section": "Step 3: The “Six Degrees of Kevin Bacon” Phase - Transitive Association Model",
    "text": "Step 3: The “Six Degrees of Kevin Bacon” Phase - Transitive Association Model\nAlright, pop culture reference time. Have you ever played “Six Degrees of Kevin Bacon”? The game where you try to connect any actor to Kevin Bacon through no more than six movie connections? Well, we’re going to do something similar with our brands, and it’s going to blow your recommendation socks off. You see, direct associations are great, but they’re limited. What if we could create a web of associations, where brand A is connected to brand B, which is connected to brand C, creating an indirect connection between A and C? It’s like being at a party and having your friend introduce you to their friend, who then introduces you to their friend. Suddenly, your network has exploded. This is where transitive associations come in. And boy, do we have a neat trick up our sleeves. Behold, the magic of matrix expansion: pythonCopydef expand_adjacency_matrix(adj_matrix, max_order=5, weight_factor=0.5): ““” Expands the adjacency matrix to include higher-order connections with normalization.\n:param adj_matrix: pandas DataFrame representing the adjacency matrix\n:param max_order: int, the maximum order of connections to consider\n:param weight_factor: float, the factor by which to reduce weight for each higher order\n:return: pandas DataFrame representing the expanded adjacency matrix\n\"\"\"\nexpanded_matrix = adj_matrix.copy()\ncurrent_matrix = normalize_matrix(adj_matrix)\n\nfor order in range(2, max_order + 1):\n    # Calculate higher-order connections\n    current_matrix = current_matrix.dot(normalize_matrix(adj_matrix))\n    \n    # Apply weight factor\n    weighted_connections = current_matrix * (weight_factor ** (order - 1))\n    \n    # Update expanded matrix, adding the weighted connections\n    expanded_matrix += weighted_connections\n\n# Final normalization to ensure all values are between 0 and 1\nexpanded_matrix = normalize_matrix(expanded_matrix)\n\nreturn expanded_matrix\nNow, let me break this down for you, because this is where the magic really happens:\nWe start with our original adjacency matrix, which represents direct connections between brands. We then calculate higher-order connections. It’s like saying, “If A is connected to B, and B is connected to C, then A has a second-order connection to C.” We do this up to a specified maximum order (default is 5, because, you know, six degrees of separation and all that). Here’s the clever bit: we apply a weight factor. Each higher-order connection gets a reduced weight because, let’s face it, your friend’s friend’s friend’s opinion probably shouldn’t count as much as your direct friend’s. We add all these weighted connections to our original matrix, creating a rich tapestry of brand relationships. Finally, we normalize everything to keep our numbers manageable.\nThe result? A supercharged adjacency matrix that captures not just direct relationships between brands, but a whole network of indirect connections.\nImagine we’re working with a niche online bookstore that specializes in obscure academic texts. The challenge? Their inventory is so specific that direct associations are as rare as a first-edition Gutenberg Bible at a yard sale. Let’s say a customer buys a book on “The Mating Habits of 12th Century Mongolian Horses” (yes, I’m having fun with these titles). In a traditional recommendation system, we’d be stuck. After all, how many other people are likely to buy that exact book? But with our transitive associations in play, the magic could happen. We might be able to recommend “The Economic Impact of Horse Trading in Medieval Asia” to our horse-book buyer, even if no one had ever purchased these two books together. Why? Because our system could potentially find a chain of associations linking them through other related books. You can imagine that with this additional unlock enables academics to joyfully go tumbling down rabbit holes of related obscure topics, much to the delight of the store owner. This scenario, while fictional, illustrates the potential power of transitive associations in a recommendation system, especially for businesses with niche or diverse product catalogs or only a sparse matrix for collaborative filtering. Through this transitive assumption about the world you go from sparse to a dense matrix uncovering “hidden” connections that could create a discovery engine to surprise and delights customers, potentially leading them down purchasing paths they never knew existed."
  },
  {
    "objectID": "posts/cold-start/index.html#step-4-fine-tuning-the-magic---hyperparameters-and-evaluation",
    "href": "posts/cold-start/index.html#step-4-fine-tuning-the-magic---hyperparameters-and-evaluation",
    "title": "The Surprisingly Lucrative Journey of Bootstrapping a Brand Recommender System: From Chaos to Cash",
    "section": "Step 4: Fine-Tuning the Magic - Hyperparameters and Evaluation",
    "text": "Step 4: Fine-Tuning the Magic - Hyperparameters and Evaluation\nAlright, data enthusiasts, we’ve built our transitive association model, but now it’s time to give it that extra polish. Think of it as tuning a high-performance engine - we need to adjust the nitrous levels just right to make this baby purr.\n\nHyperparameter Tuning: Finding the Sweet Spot\nRemember our expand_adjacency_matrix function? It comes with two key hyperparameters:\n\nmax_order: How far down the rabbit hole of connections we’re willing to go\nweight_factor: Our trust factor for the friend of a friend of a friend\n\nThese aren’t just arbitrary numbers we pulled out of a magician’s hat. They’re the secret sauce that can make or break our recommendations.\nLet’s take a closer look at weight_factor. Set it too high, and you might end up recommending winter parkas to someone shopping for swimwear. Set it too low, and you’re barely scratching the surface of potential connections.\nSo how do we find the Goldilocks zone? Enter: hyperparameter tuning. It’s like finding the perfect recipe, but instead of tasting soup, we’re crunching numbers.\ndef evaluate_model(adj_matrix, test_data, max_order, weight_factor):\n    expanded_matrix = expand_adjacency_matrix(adj_matrix, max_order, weight_factor)\n    return calculate_hit_rate(expanded_matrix, test_data)\n\n# Grid search for best hyperparameters\nbest_score = 0\nbest_params = {}\n\nfor max_order in range(2, 7):\n    for weight_factor in [0.1, 0.3, 0.5, 0.7, 0.9]:\n        score = evaluate_model(train_adj_matrix, test_data, max_order, weight_factor)\n        if score &gt; best_score:\n            best_score = score\n            best_params = {'max_order': max_order, 'weight_factor': weight_factor}\n\nprint(f\"Best parameters: {best_params}\")\nprint(f\"Best score: {best_score}\")\n\n\nModel Evaluation: Separating the Wheat from the Chaff\nNow, how do we know if our model is actually any good? We can’t just take it out for a test drive on the same roads we built it on. That’s where train-test splits come in handy.\nHere’s our game plan:\n\nSplit your data into training and test sets. Think of it as studying for an exam (training) and then taking the final (testing).\nBuild your adjacency matrix using only the training data.\nUse your tuned model to make predictions on the test set.\nCompare your transitive model against simpler approaches, like random recommendations or direct associations only.\n\nLet’s see how it’s done:\ndef compare_models(train_data, test_data):\n    # Build adjacency matrix from train data\n    train_adj_matrix = build_adjacency_matrix(train_data)\n    \n    # Random model (aka \"The Dart Board Approach\")\n    random_score = evaluate_model(random_recommendations, test_data)\n    \n    # Direct associations model (aka \"The One-Track Mind\")\n    direct_score = evaluate_model(train_adj_matrix, test_data)\n    \n    # Transitive model with best hyperparameters (aka \"The Six Degrees of Kevin Bacon\")\n    transitive_score = evaluate_model(\n        expand_adjacency_matrix(train_adj_matrix, **best_params),\n        test_data\n    )\n    \n    print(f\"Random model score: {random_score}\")\n    print(f\"Direct associations score: {direct_score}\")\n    print(f\"Transitive model score: {transitive_score}\")\n\ncompare_models(train_data, test_data)\n\n\nThe Real-World Impact: Beyond the Numbers\nNow, I won’t give you specific numbers here because, let’s face it, your mileage may vary. But if you’ve done everything right, your transitive model should be outperforming the others like a sports car in a bicycle race.\nBut here’s the real kicker: this isn’t just about better numbers on a spreadsheet. It’s about creating a recommendation system that feels almost eerily intuitive to your customers. It’s about suggesting that vintage leather jacket to someone who just bought cowboy boots, not because they’re directly related, but because your system understands the subtle connections between western wear, vintage fashion, and leather goods.\nIn the end, that’s what separates a good recommendation system from a great one. It’s not just about predicting what customers want - it’s about inspiring them, surprising them, and yes, maybe even delighting them a little.\nSo go forth, tune those hyperparameters, split that data, and may your conversion rates be ever in your favor!"
  },
  {
    "objectID": "posts/cold-start/index.html#the-secret-sauce-continuous-improvement",
    "href": "posts/cold-start/index.html#the-secret-sauce-continuous-improvement",
    "title": "The Surprisingly Lucrative Journey of Bootstrapping a Brand Recommender System: From Chaos to Cash",
    "section": "The Secret Sauce: Continuous Improvement",
    "text": "The Secret Sauce: Continuous Improvement\nNow, here’s what separates the good recommendation systems from the great ones: continuous improvement. Everything we’ve built so far is just the foundation. The real magic happens when you start iterating and refining.\n\nFine-tune your clustering: As you gather more data, you might discover that certain features are more predictive than others. Don’t hesitate to adjust your approach.\nAdjust association weights: Consider the context of purchases. Perhaps items bought together in the same transaction should carry more weight than those bought by the same customer on different days.\nOptimize hyperparameters: Regularly revisit your max_order and weight_factor settings. As your data grows and evolves, so too should your model’s parameters.\nIncorporate user feedback: If customers consistently ignore certain recommendations, use that information to refine your model.\nA/B test rigorously: Test different versions of your model against each other. Let the data guide your decisions on which approaches work best for your specific use case.\n\nRemember, the goal isn’t perfection - it’s continuous improvement. Aim to build a system that consistently outperforms random chance, and then focus on making it a little better every day."
  },
  {
    "objectID": "posts/cold-start/index.html#looking-ahead-the-future-of-personalization",
    "href": "posts/cold-start/index.html#looking-ahead-the-future-of-personalization",
    "title": "The Surprisingly Lucrative Journey of Bootstrapping a Brand Recommender System: From Chaos to Cash",
    "section": "Looking Ahead: The Future of Personalization",
    "text": "Looking Ahead: The Future of Personalization\nWhile we’ve covered significant ground in this post, we’ve only scratched the surface of what’s possible in the world of recommendation systems. As we look to the future, the potential for even more sophisticated and personalized recommendations is truly exciting.\nImagine a recommendation system that doesn’t just suggest individual products, but understands and predicts entire purchase sequences. Or one that can adapt in real-time to changing user preferences and market trends. These aren’t just pipe dreams - they’re the next frontier in e-commerce personalization.\nIn our next installment, we’ll explore how cutting-edge AI techniques can take our recommendation system to the next level. We’ll delve into methods that can create truly bespoke shopping experiences, predicting not just what a customer might want now, but what they’ll want next.\nFrom leveraging deep learning to harnessing the power of contextual bandits, we’ll explore how to create a recommendation engine that doesn’t just react to customer behavior, but anticipates it. It’s the difference between a skilled salesperson and a personal shopping psychic (with a Ph.D. in data science).\nSo stay tuned, keep experimenting with your models, and get ready to take your recommendation game to the next level. The future of e-commerce personalization is bright, and armed with the right algorithms, you’ll be well-positioned to shine. See you in the next post!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Shwetank Kumar",
    "section": "",
    "text": "I’m a physicist, engineering exec, investor. Here’s my story:\n\nBorn and raised in NCR, India. Delhi native, I live in SF.\nGot hooked on science early, did my B.Tech in EE from IIT, PhD in Applied Physics from Caltech.\nLearned business by working at startups for 10+ years and getting MBA at Wharton.\nI founded a bootstrapped company to help data teams optimize their Snowflake spend.\nI enjoy building stuff for internet and physical systems using data and AI games."
  },
  {
    "objectID": "index.html#personal-info-in-a-nutshell",
    "href": "index.html#personal-info-in-a-nutshell",
    "title": "Shwetank Kumar",
    "section": "",
    "text": "I’m a physicist, engineering exec, investor. Here’s my story:\n\nBorn and raised in NCR, India. Delhi native, I live in SF.\nGot hooked on science early, did my B.Tech in EE from IIT, PhD in Applied Physics from Caltech.\nLearned business by working at startups for 10+ years and getting MBA at Wharton.\nI founded a bootstrapped company to help data teams optimize their Snowflake spend.\nI enjoy building stuff for internet and physical systems using data and AI games."
  },
  {
    "objectID": "index.html#some-things-ive-done",
    "href": "index.html#some-things-ive-done",
    "title": "Shwetank Kumar",
    "section": "Some things I’ve done",
    "text": "Some things I’ve done\n\nBuilt AI systems that crunch petabytes of data across image, text, and tabular formats.\nScaled these systems to serve millions of customers simultaneously 1, 2.\nCreated systems that use neural networks to analyze satellite imagery at planet scale.\nLed teams of 120+ brilliant minds in data science, engineering, and analytics."
  },
  {
    "objectID": "index.html#tech-i-love-working-with",
    "href": "index.html#tech-i-love-working-with",
    "title": "Shwetank Kumar",
    "section": "Tech I love working with:",
    "text": "Tech I love working with:\n\nPyTorch, Hugging Face, Langchain, Pydantic\nGCP, Snowflake, Bigquery\nAnything that pushes the boundaries of what’s possible with data"
  },
  {
    "objectID": "index.html#investing",
    "href": "index.html#investing",
    "title": "Shwetank Kumar",
    "section": "Investing:",
    "text": "Investing:\n\nMy current areas of interest are: AI, infrastructure, open source, physical systems / frontier tech.\nSome companies I have invested in - Comet, Startree, Inference, Nimble, Turnstile, Dandelion…\nI am a founding member and part of the steering committee at Invest in Data where I co-invest with a group of ~50 other Data execs.\nWe vet and invest on a quarterly cadence. If you have an exciting AI project or a game-changing startup idea? Let’s talk: shwetank.kumar@gmail.com."
  },
  {
    "objectID": "index.html#where-you-can-find-me",
    "href": "index.html#where-you-can-find-me",
    "title": "Shwetank Kumar",
    "section": "Where you can find me:",
    "text": "Where you can find me:\n\nWriting about AI musings, tech & leadership, and data science on this blog\nSharing insights on LinkedIn\nTinkering with code on GitHub\nSpeaking on panels about the future of AI and data"
  }
]